{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Private_2위 _ Public -0.43308 _ 1D-CNN.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C4998E9lRQgp","executionInfo":{"status":"ok","timestamp":1623222398744,"user_tz":-540,"elapsed":14937,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"657f0f32-d754-4156-e2c9-aff6fd3c0459"},"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive/DATA')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YYpf3sAqRIjv","executionInfo":{"status":"ok","timestamp":1623222402037,"user_tz":-540,"elapsed":714,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy import signal\n","from tqdm import tqdm\n","from numpy.fft import fft, fftshift\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import random\n","import warnings\n","warnings.filterwarnings(action='ignore')\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Ilop2aVRIjz"},"source":["#### Data loading"]},{"cell_type":"code","metadata":{"id":"nUTH0kRaRIjz","executionInfo":{"status":"ok","timestamp":1623222414763,"user_tz":-540,"elapsed":9761,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["train=pd.read_csv('./train_features.csv')\n","train_labels=pd.read_csv('./train_labels.csv')\n","test=pd.read_csv('./test_features.csv')\n","\n","submission=pd.read_csv('./sample_submission.csv')\n","\n","pd.options.display.max_columns=50"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yZVNqPqWRIjz"},"source":["#### Feature engneering"]},{"cell_type":"markdown","metadata":{"id":"iEaSGImURIj0"},"source":["#####  가속도, 자이로, (자이로-가속도) 센서값을 에너지로 표현"]},{"cell_type":"code","metadata":{"id":"GPa5xSmNRIj0","executionInfo":{"status":"ok","timestamp":1623222428105,"user_tz":-540,"elapsed":707,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["# 왜 세제곱근일까? \n","train['acc_Energy']=(train['acc_x']**2+train['acc_y']**2+train['acc_z']**2)**(1/3)\n","test['acc_Energy']=(test['acc_x']**2+test['acc_y']**2+test['acc_z']**2)**(1/3)\n","\n","train['gy_Energy']=(train['gy_x']**2+train['gy_y']**2+train['gy_z']**2)**(1/3)\n","test['gy_Energy']=(test['gy_x']**2+test['gy_y']**2+test['gy_z']**2)**(1/3)\n","\n","train['gy_acc_Energy']=((train['gy_x']-train['acc_x'])**2+(train['gy_y']-train['acc_y'])**2+(train['gy_z']-train['acc_z'])**2)**(1/3)\n","test['gy_acc_Energy']=((test['gy_x']-test['acc_x'])**2+(test['gy_y']-test['acc_y'])**2+(test['gy_z']-test['acc_z'])**2)**(1/3)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N685v4FCRIj1"},"source":["###### id별 데이터는 0.02초마다 측정된 값들이기 때문에 이전 시간 대비 변화량 적용"]},{"cell_type":"code","metadata":{"id":"tUKG7gYjRIj1","executionInfo":{"status":"ok","timestamp":1623222430452,"user_tz":-540,"elapsed":277,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["dt=0.02 \n","def jerk_signal(signal): \n","        return np.array([(signal[i+1]-signal[i])/dt for i in range(len(signal)-1)])"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CDxON2S4RIj1","executionInfo":{"status":"ok","timestamp":1623222475855,"user_tz":-540,"elapsed":43108,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"54e0c781-39e6-454f-d6f8-f8ffeda190fc"},"source":["train_dt=[]\n","# tqdm => 진행표시바 나오게 함\n","# np.unique => 고유한 원소의 집합 \n","for i in tqdm(train['id'].unique()): # id 0~3124 \n","    # 한 아이디에 600rows가 있음\n","    temp=train.loc[train['id']==i]\n","    # acc xyz, gy xyz, 위에서 생성한 energy 컬럼 3개\n","    for v in train.columns[2:]:\n","        # 컬럼의 값들만 jerk_signal 함수에 넣은 뒤 반환\n","        values=jerk_signal(temp[v].values)\n","        \n","        # values의 0번째 위치에 0 삽입\n","        values=np.insert(values,0,0)\n","        # v+'_dt' 라는 컬럼에 values값 삽입\n","        temp.loc[:,v+'_dt']=values\n","    train_dt.append(temp)\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["100%|██████████| 3125/3125 [00:43<00:00, 72.67it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":437},"id":"GwCLXMiLep0D","executionInfo":{"status":"ok","timestamp":1623222484070,"user_tz":-540,"elapsed":306,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"f9c9c9a9-3f37-45f1-b6a1-b7008f3239c9"},"source":["train_dt[0]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>time</th>\n","      <th>acc_x</th>\n","      <th>acc_y</th>\n","      <th>acc_z</th>\n","      <th>gy_x</th>\n","      <th>gy_y</th>\n","      <th>gy_z</th>\n","      <th>acc_Energy</th>\n","      <th>gy_Energy</th>\n","      <th>gy_acc_Energy</th>\n","      <th>acc_x_dt</th>\n","      <th>acc_y_dt</th>\n","      <th>acc_z_dt</th>\n","      <th>gy_x_dt</th>\n","      <th>gy_y_dt</th>\n","      <th>gy_z_dt</th>\n","      <th>acc_Energy_dt</th>\n","      <th>gy_Energy_dt</th>\n","      <th>gy_acc_Energy_dt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.206087</td>\n","      <td>-0.179371</td>\n","      <td>-0.148447</td>\n","      <td>-0.591608</td>\n","      <td>-30.549010</td>\n","      <td>-31.676112</td>\n","      <td>1.146962</td>\n","      <td>12.465436</td>\n","      <td>12.427938</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1.287696</td>\n","      <td>-0.198974</td>\n","      <td>-0.182444</td>\n","      <td>0.303100</td>\n","      <td>-39.139103</td>\n","      <td>-24.927216</td>\n","      <td>1.200703</td>\n","      <td>12.913284</td>\n","      <td>12.865692</td>\n","      <td>4.080495</td>\n","      <td>-0.980114</td>\n","      <td>-1.699854</td>\n","      <td>44.735403</td>\n","      <td>-429.504677</td>\n","      <td>337.444793</td>\n","      <td>2.687024</td>\n","      <td>22.392358</td>\n","      <td>21.887693</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1.304609</td>\n","      <td>-0.195114</td>\n","      <td>-0.253382</td>\n","      <td>-3.617278</td>\n","      <td>-44.122565</td>\n","      <td>-25.019629</td>\n","      <td>1.217403</td>\n","      <td>13.725729</td>\n","      <td>13.692643</td>\n","      <td>0.845632</td>\n","      <td>0.192961</td>\n","      <td>-3.546937</td>\n","      <td>-196.018888</td>\n","      <td>-249.173073</td>\n","      <td>-4.620631</td>\n","      <td>0.835012</td>\n","      <td>40.622253</td>\n","      <td>41.347563</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1.293095</td>\n","      <td>-0.230366</td>\n","      <td>-0.215210</td>\n","      <td>2.712986</td>\n","      <td>-53.597843</td>\n","      <td>-27.454013</td>\n","      <td>1.209981</td>\n","      <td>15.374021</td>\n","      <td>15.314907</td>\n","      <td>-0.575711</td>\n","      <td>-1.762585</td>\n","      <td>1.908626</td>\n","      <td>316.513181</td>\n","      <td>-473.763910</td>\n","      <td>-121.719195</td>\n","      <td>-0.371100</td>\n","      <td>82.414636</td>\n","      <td>81.113199</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1.300887</td>\n","      <td>-0.187757</td>\n","      <td>-0.222523</td>\n","      <td>4.286707</td>\n","      <td>-57.906561</td>\n","      <td>-27.961234</td>\n","      <td>1.211254</td>\n","      <td>16.074363</td>\n","      <td>16.017964</td>\n","      <td>0.389598</td>\n","      <td>2.130453</td>\n","      <td>-0.365665</td>\n","      <td>78.686055</td>\n","      <td>-215.435892</td>\n","      <td>-25.361098</td>\n","      <td>0.063656</td>\n","      <td>35.017060</td>\n","      <td>35.152822</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>595</th>\n","      <td>0</td>\n","      <td>595</td>\n","      <td>0.985242</td>\n","      <td>-0.326122</td>\n","      <td>-0.354528</td>\n","      <td>-14.903280</td>\n","      <td>20.172339</td>\n","      <td>22.973018</td>\n","      <td>1.063469</td>\n","      <td>10.497476</td>\n","      <td>10.675966</td>\n","      <td>1.177958</td>\n","      <td>-4.242820</td>\n","      <td>-0.324471</td>\n","      <td>-220.368219</td>\n","      <td>-286.214412</td>\n","      <td>340.765651</td>\n","      <td>1.494244</td>\n","      <td>17.990525</td>\n","      <td>19.714449</td>\n","    </tr>\n","    <tr>\n","      <th>596</th>\n","      <td>0</td>\n","      <td>596</td>\n","      <td>1.052837</td>\n","      <td>-0.220710</td>\n","      <td>-0.413472</td>\n","      <td>-10.857025</td>\n","      <td>19.786856</td>\n","      <td>23.174597</td>\n","      <td>1.099211</td>\n","      <td>10.152517</td>\n","      <td>10.318246</td>\n","      <td>3.379771</td>\n","      <td>5.270607</td>\n","      <td>-2.947208</td>\n","      <td>202.312749</td>\n","      <td>-19.274131</td>\n","      <td>10.078975</td>\n","      <td>1.787111</td>\n","      <td>-17.247953</td>\n","      <td>-17.885963</td>\n","    </tr>\n","    <tr>\n","      <th>597</th>\n","      <td>0</td>\n","      <td>597</td>\n","      <td>1.025643</td>\n","      <td>-0.227845</td>\n","      <td>-0.354516</td>\n","      <td>-2.334243</td>\n","      <td>25.768654</td>\n","      <td>18.932070</td>\n","      <td>1.071307</td>\n","      <td>10.092134</td>\n","      <td>10.193175</td>\n","      <td>-1.359731</td>\n","      <td>-0.356764</td>\n","      <td>2.947780</td>\n","      <td>426.139108</td>\n","      <td>299.089890</td>\n","      <td>-212.126333</td>\n","      <td>-1.395196</td>\n","      <td>-3.019192</td>\n","      <td>-6.253557</td>\n","    </tr>\n","    <tr>\n","      <th>598</th>\n","      <td>0</td>\n","      <td>598</td>\n","      <td>1.031553</td>\n","      <td>-0.387862</td>\n","      <td>-0.277857</td>\n","      <td>-9.710746</td>\n","      <td>28.697694</td>\n","      <td>20.631577</td>\n","      <td>1.089077</td>\n","      <td>11.034378</td>\n","      <td>11.183082</td>\n","      <td>0.295512</td>\n","      <td>-8.000850</td>\n","      <td>3.832979</td>\n","      <td>-368.825182</td>\n","      <td>146.451989</td>\n","      <td>84.975330</td>\n","      <td>0.888507</td>\n","      <td>47.112213</td>\n","      <td>49.495340</td>\n","    </tr>\n","    <tr>\n","      <th>599</th>\n","      <td>0</td>\n","      <td>599</td>\n","      <td>1.138159</td>\n","      <td>-0.426846</td>\n","      <td>-0.430263</td>\n","      <td>-15.891015</td>\n","      <td>21.675950</td>\n","      <td>32.123007</td>\n","      <td>1.184697</td>\n","      <td>12.060479</td>\n","      <td>12.249947</td>\n","      <td>5.330310</td>\n","      <td>-1.949185</td>\n","      <td>-7.620324</td>\n","      <td>-309.013460</td>\n","      <td>-351.087173</td>\n","      <td>574.571503</td>\n","      <td>4.780981</td>\n","      <td>51.305053</td>\n","      <td>53.343259</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>600 rows × 20 columns</p>\n","</div>"],"text/plain":["     id  time     acc_x     acc_y     acc_z       gy_x       gy_y       gy_z  \\\n","0     0     0  1.206087 -0.179371 -0.148447  -0.591608 -30.549010 -31.676112   \n","1     0     1  1.287696 -0.198974 -0.182444   0.303100 -39.139103 -24.927216   \n","2     0     2  1.304609 -0.195114 -0.253382  -3.617278 -44.122565 -25.019629   \n","3     0     3  1.293095 -0.230366 -0.215210   2.712986 -53.597843 -27.454013   \n","4     0     4  1.300887 -0.187757 -0.222523   4.286707 -57.906561 -27.961234   \n","..   ..   ...       ...       ...       ...        ...        ...        ...   \n","595   0   595  0.985242 -0.326122 -0.354528 -14.903280  20.172339  22.973018   \n","596   0   596  1.052837 -0.220710 -0.413472 -10.857025  19.786856  23.174597   \n","597   0   597  1.025643 -0.227845 -0.354516  -2.334243  25.768654  18.932070   \n","598   0   598  1.031553 -0.387862 -0.277857  -9.710746  28.697694  20.631577   \n","599   0   599  1.138159 -0.426846 -0.430263 -15.891015  21.675950  32.123007   \n","\n","     acc_Energy  gy_Energy  gy_acc_Energy  acc_x_dt  acc_y_dt  acc_z_dt  \\\n","0      1.146962  12.465436      12.427938  0.000000  0.000000  0.000000   \n","1      1.200703  12.913284      12.865692  4.080495 -0.980114 -1.699854   \n","2      1.217403  13.725729      13.692643  0.845632  0.192961 -3.546937   \n","3      1.209981  15.374021      15.314907 -0.575711 -1.762585  1.908626   \n","4      1.211254  16.074363      16.017964  0.389598  2.130453 -0.365665   \n","..          ...        ...            ...       ...       ...       ...   \n","595    1.063469  10.497476      10.675966  1.177958 -4.242820 -0.324471   \n","596    1.099211  10.152517      10.318246  3.379771  5.270607 -2.947208   \n","597    1.071307  10.092134      10.193175 -1.359731 -0.356764  2.947780   \n","598    1.089077  11.034378      11.183082  0.295512 -8.000850  3.832979   \n","599    1.184697  12.060479      12.249947  5.330310 -1.949185 -7.620324   \n","\n","        gy_x_dt     gy_y_dt     gy_z_dt  acc_Energy_dt  gy_Energy_dt  \\\n","0      0.000000    0.000000    0.000000       0.000000      0.000000   \n","1     44.735403 -429.504677  337.444793       2.687024     22.392358   \n","2   -196.018888 -249.173073   -4.620631       0.835012     40.622253   \n","3    316.513181 -473.763910 -121.719195      -0.371100     82.414636   \n","4     78.686055 -215.435892  -25.361098       0.063656     35.017060   \n","..          ...         ...         ...            ...           ...   \n","595 -220.368219 -286.214412  340.765651       1.494244     17.990525   \n","596  202.312749  -19.274131   10.078975       1.787111    -17.247953   \n","597  426.139108  299.089890 -212.126333      -1.395196     -3.019192   \n","598 -368.825182  146.451989   84.975330       0.888507     47.112213   \n","599 -309.013460 -351.087173  574.571503       4.780981     51.305053   \n","\n","     gy_acc_Energy_dt  \n","0            0.000000  \n","1           21.887693  \n","2           41.347563  \n","3           81.113199  \n","4           35.152822  \n","..                ...  \n","595         19.714449  \n","596        -17.885963  \n","597         -6.253557  \n","598         49.495340  \n","599         53.343259  \n","\n","[600 rows x 20 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nMaANKeqRIj3","executionInfo":{"status":"ok","timestamp":1623222497062,"user_tz":-540,"elapsed":10098,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"e7d60db9-e0e5-4955-b759-3a671b6f7c58"},"source":["test_dt=[]\n","# train과 같은 방식으로 진행\n","for i in tqdm(test['id'].unique()):\n","    temp=test.loc[test['id']==i]\n","    for v in train.columns[2:]:\n","        values=jerk_signal(temp[v].values)\n","        values=np.insert(values,0,0)\n","        temp.loc[:,v+'_dt']=values\n","    test_dt.append(temp)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["100%|██████████| 782/782 [00:09<00:00, 78.84it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"_-ucCBgYRIj3"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WF6mze99RIj4"},"source":["##### 가속도, 자이로 센서값들을 푸리에 변환"]},{"cell_type":"code","metadata":{"id":"0uYSfDmFRIj4","executionInfo":{"status":"ok","timestamp":1623222502921,"user_tz":-540,"elapsed":269,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["from scipy import fftpack\n","from numpy.fft import *\n","\n","def fourier_transform_one_signal(t_signal):\n","    # fft => Fast Fourier transforms\n","    # 매개변수로 받은 t_signal을 푸리에 변환 시킨다\n","    complex_f_signal= fftpack.fft(t_signal)\n","     \n","    # 주파수 해석의 목적은 각 주파수 별로 크기 (magnitude)가 어느 정도인지 알아내는 것\n","    # 따라서 절대값 함수 abs()를 이용해서 크기를 연산할 수 있음\n","    amplitude_f_signal=np.abs(complex_f_signal)\n","    return amplitude_f_signal"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"2VhFjH7tRIj4","executionInfo":{"status":"ok","timestamp":1623222507143,"user_tz":-540,"elapsed":742,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["train=pd.concat(train_dt)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"2F2nkl_YmdyM","executionInfo":{"status":"ok","timestamp":1623205791918,"user_tz":-540,"elapsed":323,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["# train"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qTOVn5vDRIj4","executionInfo":{"status":"ok","timestamp":1623222523158,"user_tz":-540,"elapsed":10670,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"86539743-ac51-4e2b-e746-b19e5b2833ed"},"source":["fft=[]\n","for i in tqdm(train['id'].unique()):\n","    temp=train.loc[train['id']==i]\n","    # acc값과 gy 값만 사용\n","    for i in train.columns[2:8]:\n","        # 위에서 선언한 함수 사용해서 푸리에 변환\n","        temp[i]=fourier_transform_one_signal(temp[i].values)\n","    fft.append(temp)\n","train=pd.concat(fft)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["100%|██████████| 3125/3125 [00:10<00:00, 311.53it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-BRkSLvvm9ej","executionInfo":{"status":"ok","timestamp":1623205890220,"user_tz":-540,"elapsed":272,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["# train"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQ6FK8qpRIj5","executionInfo":{"status":"ok","timestamp":1623222530677,"user_tz":-540,"elapsed":269,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["test=pd.concat(test_dt)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PqGodzjzRIj5","executionInfo":{"status":"ok","timestamp":1623222534061,"user_tz":-540,"elapsed":1806,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"ca59e6a9-c77e-4ac8-aafb-5cd0b94994f2"},"source":["fft_t=[]\n","for i in tqdm(test['id'].unique()):\n","    temp=test.loc[test['id']==i]\n","    for i in test.columns[2:8]:\n","        temp[i]=fourier_transform_one_signal(temp[i].values)\n","    fft_t.append(temp)\n","test=pd.concat(fft_t)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["100%|██████████| 782/782 [00:01<00:00, 478.94it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"EpQHDrVRRIj5"},"source":["##### Standard scaling 적용\n"]},{"cell_type":"code","metadata":{"id":"Wu1gsSdcRIj6","executionInfo":{"status":"ok","timestamp":1623222542776,"user_tz":-540,"elapsed":285,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["col=train.columns\n","train_s=train.copy()\n","test_s=test.copy()"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":437},"id":"XgMEJI0-9LJh","executionInfo":{"status":"ok","timestamp":1623222543901,"user_tz":-540,"elapsed":6,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"bf8ee75b-60fd-4357-ed36-54cfd85a712a"},"source":["train_s"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>time</th>\n","      <th>acc_x</th>\n","      <th>acc_y</th>\n","      <th>acc_z</th>\n","      <th>gy_x</th>\n","      <th>gy_y</th>\n","      <th>gy_z</th>\n","      <th>acc_Energy</th>\n","      <th>gy_Energy</th>\n","      <th>gy_acc_Energy</th>\n","      <th>acc_x_dt</th>\n","      <th>acc_y_dt</th>\n","      <th>acc_z_dt</th>\n","      <th>gy_x_dt</th>\n","      <th>gy_y_dt</th>\n","      <th>gy_z_dt</th>\n","      <th>acc_Energy_dt</th>\n","      <th>gy_Energy_dt</th>\n","      <th>gy_acc_Energy_dt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>558.797337</td>\n","      <td>131.082711</td>\n","      <td>222.252919</td>\n","      <td>1119.161589</td>\n","      <td>2015.703683</td>\n","      <td>709.264425</td>\n","      <td>1.146962</td>\n","      <td>12.465436</td>\n","      <td>12.427938</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3.233175</td>\n","      <td>15.689279</td>\n","      <td>12.229014</td>\n","      <td>221.599635</td>\n","      <td>361.903330</td>\n","      <td>477.080942</td>\n","      <td>1.200703</td>\n","      <td>12.913284</td>\n","      <td>12.865692</td>\n","      <td>4.080495</td>\n","      <td>-0.980114</td>\n","      <td>-1.699854</td>\n","      <td>44.735403</td>\n","      <td>-429.504677</td>\n","      <td>337.444793</td>\n","      <td>2.687024</td>\n","      <td>22.392358</td>\n","      <td>21.887693</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>4.832535</td>\n","      <td>8.199566</td>\n","      <td>3.901211</td>\n","      <td>357.200415</td>\n","      <td>430.568986</td>\n","      <td>452.096143</td>\n","      <td>1.217403</td>\n","      <td>13.725729</td>\n","      <td>13.692643</td>\n","      <td>0.845632</td>\n","      <td>0.192961</td>\n","      <td>-3.546937</td>\n","      <td>-196.018888</td>\n","      <td>-249.173073</td>\n","      <td>-4.620631</td>\n","      <td>0.835012</td>\n","      <td>40.622253</td>\n","      <td>41.347563</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>5.675383</td>\n","      <td>5.330015</td>\n","      <td>2.527445</td>\n","      <td>340.433376</td>\n","      <td>787.558320</td>\n","      <td>467.307109</td>\n","      <td>1.209981</td>\n","      <td>15.374021</td>\n","      <td>15.314907</td>\n","      <td>-0.575711</td>\n","      <td>-1.762585</td>\n","      <td>1.908626</td>\n","      <td>316.513181</td>\n","      <td>-473.763910</td>\n","      <td>-121.719195</td>\n","      <td>-0.371100</td>\n","      <td>82.414636</td>\n","      <td>81.113199</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>7.415275</td>\n","      <td>7.980024</td>\n","      <td>6.566908</td>\n","      <td>128.188871</td>\n","      <td>1372.095224</td>\n","      <td>715.824074</td>\n","      <td>1.211254</td>\n","      <td>16.074363</td>\n","      <td>16.017964</td>\n","      <td>0.389598</td>\n","      <td>2.130453</td>\n","      <td>-0.365665</td>\n","      <td>78.686055</td>\n","      <td>-215.435892</td>\n","      <td>-25.361098</td>\n","      <td>0.063656</td>\n","      <td>35.017060</td>\n","      <td>35.152822</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1874995</th>\n","      <td>3124</td>\n","      <td>595</td>\n","      <td>11.743654</td>\n","      <td>3.796333</td>\n","      <td>12.513870</td>\n","      <td>715.873677</td>\n","      <td>1124.494889</td>\n","      <td>645.627066</td>\n","      <td>1.009050</td>\n","      <td>25.963234</td>\n","      <td>25.897316</td>\n","      <td>1.484646</td>\n","      <td>0.303666</td>\n","      <td>0.800069</td>\n","      <td>-150.644663</td>\n","      <td>-34.630282</td>\n","      <td>-8.380088</td>\n","      <td>-0.679712</td>\n","      <td>8.387109</td>\n","      <td>8.432977</td>\n","    </tr>\n","    <tr>\n","      <th>1874996</th>\n","      <td>3124</td>\n","      <td>596</td>\n","      <td>211.498089</td>\n","      <td>82.888508</td>\n","      <td>86.807874</td>\n","      <td>5515.261695</td>\n","      <td>28917.564390</td>\n","      <td>20218.747027</td>\n","      <td>1.002827</td>\n","      <td>25.784692</td>\n","      <td>25.722482</td>\n","      <td>1.474659</td>\n","      <td>-0.005442</td>\n","      <td>1.775771</td>\n","      <td>-39.061611</td>\n","      <td>110.842743</td>\n","      <td>-16.732496</td>\n","      <td>-0.311171</td>\n","      <td>-8.927089</td>\n","      <td>-8.741727</td>\n","    </tr>\n","    <tr>\n","      <th>1874997</th>\n","      <td>3124</td>\n","      <td>597</td>\n","      <td>12.175349</td>\n","      <td>6.200258</td>\n","      <td>2.084554</td>\n","      <td>343.695161</td>\n","      <td>464.375112</td>\n","      <td>78.097163</td>\n","      <td>1.006239</td>\n","      <td>25.628060</td>\n","      <td>25.572145</td>\n","      <td>0.915321</td>\n","      <td>-0.407957</td>\n","      <td>1.744566</td>\n","      <td>113.799702</td>\n","      <td>151.036858</td>\n","      <td>-137.001896</td>\n","      <td>0.170620</td>\n","      <td>-7.831611</td>\n","      <td>-7.516832</td>\n","    </tr>\n","    <tr>\n","      <th>1874998</th>\n","      <td>3124</td>\n","      <td>598</td>\n","      <td>19.116783</td>\n","      <td>3.830800</td>\n","      <td>6.938661</td>\n","      <td>791.376179</td>\n","      <td>2724.373764</td>\n","      <td>1131.590078</td>\n","      <td>1.001038</td>\n","      <td>25.626266</td>\n","      <td>25.573288</td>\n","      <td>1.709833</td>\n","      <td>-0.796984</td>\n","      <td>0.479107</td>\n","      <td>211.827245</td>\n","      <td>-18.171144</td>\n","      <td>-44.717652</td>\n","      <td>-0.260074</td>\n","      <td>-0.089713</td>\n","      <td>0.057150</td>\n","    </tr>\n","    <tr>\n","      <th>1874999</th>\n","      <td>3124</td>\n","      <td>599</td>\n","      <td>22.306532</td>\n","      <td>4.721920</td>\n","      <td>15.463388</td>\n","      <td>357.639418</td>\n","      <td>2058.364675</td>\n","      <td>977.868201</td>\n","      <td>0.990773</td>\n","      <td>25.645131</td>\n","      <td>25.595348</td>\n","      <td>2.609116</td>\n","      <td>-0.883512</td>\n","      <td>0.534668</td>\n","      <td>285.946217</td>\n","      <td>-52.118894</td>\n","      <td>-20.837588</td>\n","      <td>-0.513215</td>\n","      <td>0.943240</td>\n","      <td>1.102985</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1875000 rows × 20 columns</p>\n","</div>"],"text/plain":["           id  time       acc_x       acc_y       acc_z         gy_x  \\\n","0           0     0  558.797337  131.082711  222.252919  1119.161589   \n","1           0     1    3.233175   15.689279   12.229014   221.599635   \n","2           0     2    4.832535    8.199566    3.901211   357.200415   \n","3           0     3    5.675383    5.330015    2.527445   340.433376   \n","4           0     4    7.415275    7.980024    6.566908   128.188871   \n","...       ...   ...         ...         ...         ...          ...   \n","1874995  3124   595   11.743654    3.796333   12.513870   715.873677   \n","1874996  3124   596  211.498089   82.888508   86.807874  5515.261695   \n","1874997  3124   597   12.175349    6.200258    2.084554   343.695161   \n","1874998  3124   598   19.116783    3.830800    6.938661   791.376179   \n","1874999  3124   599   22.306532    4.721920   15.463388   357.639418   \n","\n","                 gy_y          gy_z  acc_Energy  gy_Energy  gy_acc_Energy  \\\n","0         2015.703683    709.264425    1.146962  12.465436      12.427938   \n","1          361.903330    477.080942    1.200703  12.913284      12.865692   \n","2          430.568986    452.096143    1.217403  13.725729      13.692643   \n","3          787.558320    467.307109    1.209981  15.374021      15.314907   \n","4         1372.095224    715.824074    1.211254  16.074363      16.017964   \n","...               ...           ...         ...        ...            ...   \n","1874995   1124.494889    645.627066    1.009050  25.963234      25.897316   \n","1874996  28917.564390  20218.747027    1.002827  25.784692      25.722482   \n","1874997    464.375112     78.097163    1.006239  25.628060      25.572145   \n","1874998   2724.373764   1131.590078    1.001038  25.626266      25.573288   \n","1874999   2058.364675    977.868201    0.990773  25.645131      25.595348   \n","\n","         acc_x_dt  acc_y_dt  acc_z_dt     gy_x_dt     gy_y_dt     gy_z_dt  \\\n","0        0.000000  0.000000  0.000000    0.000000    0.000000    0.000000   \n","1        4.080495 -0.980114 -1.699854   44.735403 -429.504677  337.444793   \n","2        0.845632  0.192961 -3.546937 -196.018888 -249.173073   -4.620631   \n","3       -0.575711 -1.762585  1.908626  316.513181 -473.763910 -121.719195   \n","4        0.389598  2.130453 -0.365665   78.686055 -215.435892  -25.361098   \n","...           ...       ...       ...         ...         ...         ...   \n","1874995  1.484646  0.303666  0.800069 -150.644663  -34.630282   -8.380088   \n","1874996  1.474659 -0.005442  1.775771  -39.061611  110.842743  -16.732496   \n","1874997  0.915321 -0.407957  1.744566  113.799702  151.036858 -137.001896   \n","1874998  1.709833 -0.796984  0.479107  211.827245  -18.171144  -44.717652   \n","1874999  2.609116 -0.883512  0.534668  285.946217  -52.118894  -20.837588   \n","\n","         acc_Energy_dt  gy_Energy_dt  gy_acc_Energy_dt  \n","0             0.000000      0.000000          0.000000  \n","1             2.687024     22.392358         21.887693  \n","2             0.835012     40.622253         41.347563  \n","3            -0.371100     82.414636         81.113199  \n","4             0.063656     35.017060         35.152822  \n","...                ...           ...               ...  \n","1874995      -0.679712      8.387109          8.432977  \n","1874996      -0.311171     -8.927089         -8.741727  \n","1874997       0.170620     -7.831611         -7.516832  \n","1874998      -0.260074     -0.089713          0.057150  \n","1874999      -0.513215      0.943240          1.102985  \n","\n","[1875000 rows x 20 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"P7gj7zitRIj6","executionInfo":{"status":"ok","timestamp":1623222552452,"user_tz":-540,"elapsed":5974,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["from sklearn.preprocessing import StandardScaler,MinMaxScaler\n","\n","# Standard => 기본 스케일. 평균과 표준편차 사용\n","# 평균을 제거하고 데이터를 단위 분산으로 조정\n","# 이상치가 있다면 평균과 표준편차에 영향을 미쳐 변환된 데이터의 확산은 매우 달라지게 된다\n","# 따라서 이상치가 있는 경우 균형 잡힌 척도를 보장할 수 없다\n","scaler = StandardScaler()\n","\n","\n","# 변환만을 생각한다면 fit() , transform()을 함께 사용하지 않고 \n","# transform()만 사용하면 될텐데 두개 메소드를 함께 사용하는 이유가 있다\n","\n","# 학습데이터 세트에서 변환을 위한 기반 설정(예를 들어 학습 데이터 세트의 최대값/최소값등)을 \n","# 먼저 fit()을 통해서 설정한 뒤에 이를 기반으로 학습 데이터의 transform()을 수행하되 \n","# 학습 데이터에서 설정된 변환을 위한 기반 설정을 그대로 테스트 데이터에도 적용하기 위해서이다\n","# 참고 https://subinium.github.io/MLwithPython-3-3/\n","train_s.iloc[:,2:]= scaler.fit_transform(train_s.iloc[:,2:])\n","train_sc = pd.DataFrame(data = train_s,columns =col)\n","\n","test_s.iloc[:,2:]= scaler.transform(test_s.iloc[:,2:])\n","test_sc = pd.DataFrame(data = test_s,columns =col)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":437},"id":"Or0MTXauRIj6","executionInfo":{"status":"ok","timestamp":1623222554652,"user_tz":-540,"elapsed":409,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"d83ae158-a424-4d09-b31a-293280914b10"},"source":["train_sc"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>time</th>\n","      <th>acc_x</th>\n","      <th>acc_y</th>\n","      <th>acc_z</th>\n","      <th>gy_x</th>\n","      <th>gy_y</th>\n","      <th>gy_z</th>\n","      <th>acc_Energy</th>\n","      <th>gy_Energy</th>\n","      <th>gy_acc_Energy</th>\n","      <th>acc_x_dt</th>\n","      <th>acc_y_dt</th>\n","      <th>acc_z_dt</th>\n","      <th>gy_x_dt</th>\n","      <th>gy_y_dt</th>\n","      <th>gy_z_dt</th>\n","      <th>acc_Energy_dt</th>\n","      <th>gy_Energy_dt</th>\n","      <th>gy_acc_Energy_dt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>27.356382</td>\n","      <td>8.807207</td>\n","      <td>19.465910</td>\n","      <td>0.376992</td>\n","      <td>0.869226</td>\n","      <td>0.150423</td>\n","      <td>0.495681</td>\n","      <td>-0.272719</td>\n","      <td>-0.276391</td>\n","      <td>0.000027</td>\n","      <td>0.000298</td>\n","      <td>-0.000433</td>\n","      <td>0.000347</td>\n","      <td>0.000373</td>\n","      <td>0.000273</td>\n","      <td>0.000101</td>\n","      <td>0.001505</td>\n","      <td>0.001501</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>-0.054866</td>\n","      <td>0.833464</td>\n","      <td>0.820412</td>\n","      <td>-0.282128</td>\n","      <td>-0.093560</td>\n","      <td>0.011266</td>\n","      <td>0.742974</td>\n","      <td>-0.236152</td>\n","      <td>-0.240632</td>\n","      <td>0.416836</td>\n","      <td>-0.118821</td>\n","      <td>-0.255054</td>\n","      <td>0.032738</td>\n","      <td>-0.349095</td>\n","      <td>0.377085</td>\n","      <td>0.564992</td>\n","      <td>0.166566</td>\n","      <td>0.162871</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0.024046</td>\n","      <td>0.315921</td>\n","      <td>0.081086</td>\n","      <td>-0.182551</td>\n","      <td>-0.053585</td>\n","      <td>-0.003708</td>\n","      <td>0.819822</td>\n","      <td>-0.169815</td>\n","      <td>-0.173080</td>\n","      <td>0.086405</td>\n","      <td>0.023750</td>\n","      <td>-0.531727</td>\n","      <td>-0.141582</td>\n","      <td>-0.202368</td>\n","      <td>-0.004887</td>\n","      <td>0.175645</td>\n","      <td>0.300944</td>\n","      <td>0.306341</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0.065632</td>\n","      <td>0.117634</td>\n","      <td>-0.040874</td>\n","      <td>-0.194863</td>\n","      <td>0.154242</td>\n","      <td>0.005408</td>\n","      <td>0.785669</td>\n","      <td>-0.035229</td>\n","      <td>-0.040560</td>\n","      <td>-0.058780</td>\n","      <td>-0.213920</td>\n","      <td>0.285459</td>\n","      <td>0.229520</td>\n","      <td>-0.385106</td>\n","      <td>-0.135647</td>\n","      <td>-0.077915</td>\n","      <td>0.609008</td>\n","      <td>0.599518</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0.151477</td>\n","      <td>0.300751</td>\n","      <td>0.317742</td>\n","      <td>-0.350724</td>\n","      <td>0.494539</td>\n","      <td>0.154354</td>\n","      <td>0.791528</td>\n","      <td>0.021954</td>\n","      <td>0.016872</td>\n","      <td>0.039823</td>\n","      <td>0.259227</td>\n","      <td>-0.055206</td>\n","      <td>0.057320</td>\n","      <td>-0.174917</td>\n","      <td>-0.028047</td>\n","      <td>0.013483</td>\n","      <td>0.259626</td>\n","      <td>0.260669</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1874995</th>\n","      <td>3124</td>\n","      <td>595</td>\n","      <td>0.365037</td>\n","      <td>0.011656</td>\n","      <td>0.845701</td>\n","      <td>0.080839</td>\n","      <td>0.350395</td>\n","      <td>0.112282</td>\n","      <td>-0.138940</td>\n","      <td>0.829394</td>\n","      <td>0.823900</td>\n","      <td>0.151679</td>\n","      <td>0.037205</td>\n","      <td>0.119409</td>\n","      <td>-0.108728</td>\n","      <td>-0.027804</td>\n","      <td>-0.009085</td>\n","      <td>-0.142794</td>\n","      <td>0.063329</td>\n","      <td>0.063674</td>\n","    </tr>\n","    <tr>\n","      <th>1874996</th>\n","      <td>3124</td>\n","      <td>596</td>\n","      <td>10.220817</td>\n","      <td>5.476964</td>\n","      <td>7.441373</td>\n","      <td>3.605246</td>\n","      <td>16.530576</td>\n","      <td>11.843241</td>\n","      <td>-0.167578</td>\n","      <td>0.814816</td>\n","      <td>0.809618</td>\n","      <td>0.150658</td>\n","      <td>-0.000363</td>\n","      <td>0.265559</td>\n","      <td>-0.027936</td>\n","      <td>0.090560</td>\n","      <td>-0.018412</td>\n","      <td>-0.065316</td>\n","      <td>-0.064300</td>\n","      <td>-0.062949</td>\n","    </tr>\n","    <tr>\n","      <th>1874997</th>\n","      <td>3124</td>\n","      <td>597</td>\n","      <td>0.386337</td>\n","      <td>0.177768</td>\n","      <td>-0.080193</td>\n","      <td>-0.192468</td>\n","      <td>-0.033904</td>\n","      <td>-0.227861</td>\n","      <td>-0.151875</td>\n","      <td>0.802027</td>\n","      <td>0.797338</td>\n","      <td>0.093524</td>\n","      <td>-0.049283</td>\n","      <td>0.260884</td>\n","      <td>0.082744</td>\n","      <td>0.123264</td>\n","      <td>-0.152712</td>\n","      <td>0.035970</td>\n","      <td>-0.056225</td>\n","      <td>-0.053918</td>\n","    </tr>\n","    <tr>\n","      <th>1874998</th>\n","      <td>3124</td>\n","      <td>598</td>\n","      <td>0.728823</td>\n","      <td>0.014037</td>\n","      <td>0.350745</td>\n","      <td>0.136284</td>\n","      <td>1.281790</td>\n","      <td>0.403540</td>\n","      <td>-0.175811</td>\n","      <td>0.801880</td>\n","      <td>0.797431</td>\n","      <td>0.174681</td>\n","      <td>-0.096564</td>\n","      <td>0.071332</td>\n","      <td>0.153722</td>\n","      <td>-0.014412</td>\n","      <td>-0.049662</td>\n","      <td>-0.054574</td>\n","      <td>0.000843</td>\n","      <td>0.001922</td>\n","    </tr>\n","    <tr>\n","      <th>1874999</th>\n","      <td>3124</td>\n","      <td>599</td>\n","      <td>0.886204</td>\n","      <td>0.075614</td>\n","      <td>1.107553</td>\n","      <td>-0.182228</td>\n","      <td>0.894062</td>\n","      <td>0.311408</td>\n","      <td>-0.223043</td>\n","      <td>0.803421</td>\n","      <td>0.799233</td>\n","      <td>0.266539</td>\n","      <td>-0.107081</td>\n","      <td>0.079654</td>\n","      <td>0.207388</td>\n","      <td>-0.042034</td>\n","      <td>-0.022996</td>\n","      <td>-0.107792</td>\n","      <td>0.008458</td>\n","      <td>0.009633</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1875000 rows × 20 columns</p>\n","</div>"],"text/plain":["           id  time      acc_x     acc_y      acc_z      gy_x       gy_y  \\\n","0           0     0  27.356382  8.807207  19.465910  0.376992   0.869226   \n","1           0     1  -0.054866  0.833464   0.820412 -0.282128  -0.093560   \n","2           0     2   0.024046  0.315921   0.081086 -0.182551  -0.053585   \n","3           0     3   0.065632  0.117634  -0.040874 -0.194863   0.154242   \n","4           0     4   0.151477  0.300751   0.317742 -0.350724   0.494539   \n","...       ...   ...        ...       ...        ...       ...        ...   \n","1874995  3124   595   0.365037  0.011656   0.845701  0.080839   0.350395   \n","1874996  3124   596  10.220817  5.476964   7.441373  3.605246  16.530576   \n","1874997  3124   597   0.386337  0.177768  -0.080193 -0.192468  -0.033904   \n","1874998  3124   598   0.728823  0.014037   0.350745  0.136284   1.281790   \n","1874999  3124   599   0.886204  0.075614   1.107553 -0.182228   0.894062   \n","\n","              gy_z  acc_Energy  gy_Energy  gy_acc_Energy  acc_x_dt  acc_y_dt  \\\n","0         0.150423    0.495681  -0.272719      -0.276391  0.000027  0.000298   \n","1         0.011266    0.742974  -0.236152      -0.240632  0.416836 -0.118821   \n","2        -0.003708    0.819822  -0.169815      -0.173080  0.086405  0.023750   \n","3         0.005408    0.785669  -0.035229      -0.040560 -0.058780 -0.213920   \n","4         0.154354    0.791528   0.021954       0.016872  0.039823  0.259227   \n","...            ...         ...        ...            ...       ...       ...   \n","1874995   0.112282   -0.138940   0.829394       0.823900  0.151679  0.037205   \n","1874996  11.843241   -0.167578   0.814816       0.809618  0.150658 -0.000363   \n","1874997  -0.227861   -0.151875   0.802027       0.797338  0.093524 -0.049283   \n","1874998   0.403540   -0.175811   0.801880       0.797431  0.174681 -0.096564   \n","1874999   0.311408   -0.223043   0.803421       0.799233  0.266539 -0.107081   \n","\n","         acc_z_dt   gy_x_dt   gy_y_dt   gy_z_dt  acc_Energy_dt  gy_Energy_dt  \\\n","0       -0.000433  0.000347  0.000373  0.000273       0.000101      0.001505   \n","1       -0.255054  0.032738 -0.349095  0.377085       0.564992      0.166566   \n","2       -0.531727 -0.141582 -0.202368 -0.004887       0.175645      0.300944   \n","3        0.285459  0.229520 -0.385106 -0.135647      -0.077915      0.609008   \n","4       -0.055206  0.057320 -0.174917 -0.028047       0.013483      0.259626   \n","...           ...       ...       ...       ...            ...           ...   \n","1874995  0.119409 -0.108728 -0.027804 -0.009085      -0.142794      0.063329   \n","1874996  0.265559 -0.027936  0.090560 -0.018412      -0.065316     -0.064300   \n","1874997  0.260884  0.082744  0.123264 -0.152712       0.035970     -0.056225   \n","1874998  0.071332  0.153722 -0.014412 -0.049662      -0.054574      0.000843   \n","1874999  0.079654  0.207388 -0.042034 -0.022996      -0.107792      0.008458   \n","\n","         gy_acc_Energy_dt  \n","0                0.001501  \n","1                0.162871  \n","2                0.306341  \n","3                0.599518  \n","4                0.260669  \n","...                   ...  \n","1874995          0.063674  \n","1874996         -0.062949  \n","1874997         -0.053918  \n","1874998          0.001922  \n","1874999          0.009633  \n","\n","[1875000 rows x 20 columns]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"D02aM-hTRIj6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GZOmEXVGRIj7"},"source":["##### 모델링\n","\n","+ CNN, LSTM, CNN+LSTM 등 여러 구조 적용해보다가 CNN에서 Flatten 없이 Global average pooling 한 구조가 가장 성능이 좋아 채택했습니다.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ceOmiD7kCK6h","executionInfo":{"status":"ok","timestamp":1623222569965,"user_tz":-540,"elapsed":3456,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"503c99b3-673f-4d47-b57e-78bfdba387c8"},"source":["pip install tensorflow_addons"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Collecting tensorflow_addons\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/4b/e893d194e626c24b3df2253066aa418f46a432fdb68250cde14bf9bb0700/tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679kB)\n","\u001b[K     |████████████████████████████████| 686kB 7.3MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.13.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtx1cXMxCjr3","executionInfo":{"status":"ok","timestamp":1623222576318,"user_tz":-540,"elapsed":3677,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"98e4e150-4974-460e-f749-e2ce974c7a86"},"source":["pip install np_utils"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Collecting np_utils\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/18/5704a782fd72727a9e63198fcc76fadb86975f45bcdf579c10f668329508/np_utils-0.5.12.1.tar.gz (61kB)\n","\r\u001b[K     |█████▍                          | 10kB 24.4MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 20kB 30.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30kB 20.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 40kB 16.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 51kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 5.4MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from np_utils) (1.19.5)\n","Requirement already satisfied: future>=0.16 in /usr/local/lib/python3.7/dist-packages (from np_utils) (0.16.0)\n","Building wheels for collected packages: np-utils\n","  Building wheel for np-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for np-utils: filename=np_utils-0.5.12.1-cp37-none-any.whl size=57133 sha256=80ccf32f231aa9b66f45372fc7a25aae3a789ced2db191768cf5c7b5e53c245a\n","  Stored in directory: /root/.cache/pip/wheels/92/4b/81/206efd0d01330a96f3aebe5021d2d5f0b264b7ade827c306ef\n","Successfully built np-utils\n","Installing collected packages: np-utils\n","Successfully installed np-utils-0.5.12.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KWhdQQZ8RIj7","executionInfo":{"status":"ok","timestamp":1623222579762,"user_tz":-540,"elapsed":678,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["import tensorflow as tf\n","import tensorflow.keras as keras\n","import tensorflow_addons as tfa\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM,Bidirectional,Dropout\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","from keras.utils.np_utils import to_categorical\n","from keras import backend as K \n","from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\n","from sklearn.model_selection import KFold,StratifiedKFold\n","from numpy.random import seed\n","import keras"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M-qSe_IORIj7","executionInfo":{"status":"ok","timestamp":1623222582698,"user_tz":-540,"elapsed":304,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"88e47a49-503e-4bce-f6ed-0b4b27cdb639"},"source":["X=np.array(train_sc.iloc[:,2:]).reshape(3125, 600, -1)\n","# 아이디 별로 600개의 데이터가 있고, 3125개의 아이디가 있다(0~3124)\n","X.shape"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3125, 600, 18)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aB5_USOdRIj8","executionInfo":{"status":"ok","timestamp":1623222584230,"user_tz":-540,"elapsed":409,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"0141cc4e-634e-455f-aee6-1ed607506cd2"},"source":["test_x=np.array(test_sc.iloc[:,2:]).reshape(782, 600, -1)\n","# reshape()의 ‘-1’이 의미하는 바는, 변경된 배열의 ‘-1’ \n","# 위치의 차원은 “원래 배열의 길이와 남은 차원으로 부터 추정”\n","# => 차원을 따로 지정하지않고 추정하겠다라는 의미인듯\n","test_x.shape"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(782, 600, 18)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"GFCsPybfFh-I","executionInfo":{"status":"ok","timestamp":1623222585494,"user_tz":-540,"elapsed":277,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"de384484-e049-4250-f7eb-4d1415524850"},"source":["train_labels"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>label_desc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>37</td>\n","      <td>Shoulder Press (dumbbell)</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>26</td>\n","      <td>Non-Exercise</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>Biceps Curl (band)</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>26</td>\n","      <td>Non-Exercise</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>26</td>\n","      <td>Non-Exercise</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3120</th>\n","      <td>3120</td>\n","      <td>26</td>\n","      <td>Non-Exercise</td>\n","    </tr>\n","    <tr>\n","      <th>3121</th>\n","      <td>3121</td>\n","      <td>26</td>\n","      <td>Non-Exercise</td>\n","    </tr>\n","    <tr>\n","      <th>3122</th>\n","      <td>3122</td>\n","      <td>15</td>\n","      <td>Dynamic Stretch (at your own pace)</td>\n","    </tr>\n","    <tr>\n","      <th>3123</th>\n","      <td>3123</td>\n","      <td>26</td>\n","      <td>Non-Exercise</td>\n","    </tr>\n","    <tr>\n","      <th>3124</th>\n","      <td>3124</td>\n","      <td>2</td>\n","      <td>Bicep Curl</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3125 rows × 3 columns</p>\n","</div>"],"text/plain":["        id  label                          label_desc\n","0        0     37           Shoulder Press (dumbbell)\n","1        1     26                        Non-Exercise\n","2        2      3                  Biceps Curl (band)\n","3        3     26                        Non-Exercise\n","4        4     26                        Non-Exercise\n","...    ...    ...                                 ...\n","3120  3120     26                        Non-Exercise\n","3121  3121     26                        Non-Exercise\n","3122  3122     15  Dynamic Stretch (at your own pace)\n","3123  3123     26                        Non-Exercise\n","3124  3124      2                          Bicep Curl\n","\n","[3125 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W8ynpblPRIj8","executionInfo":{"status":"ok","timestamp":1623222587495,"user_tz":-540,"elapsed":269,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"951c9c8a-21aa-4948-9144-dac8378fe644"},"source":["y = train_labels['label'].values\n","y = tf.keras.utils.to_categorical(train_labels['label']) \n","print(y)\n","print(\"y.shape => \",y.shape)\n","y[0]\n","\n","# y[0] 출력해보면 알겠지만 label 번째 인덱스 값만 1이다(0부터 시작했을 경우)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 1. ... 0. 0. 0.]]\n","y.shape =>  (3125, 61)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"iL9yHVlnRIj8"},"source":["##### 모델 구조 "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vjIuDxRTINUq","executionInfo":{"status":"ok","timestamp":1623222590767,"user_tz":-540,"elapsed":269,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"5c6668e4-40d7-44a2-c439-826c7441491b"},"source":["# seed를 지정하면 계속 똑같은 값만 나온다\n","seed(1)\n","np.random.rand(10)"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([4.17022005e-01, 7.20324493e-01, 1.14374817e-04, 3.02332573e-01,\n","       1.46755891e-01, 9.23385948e-02, 1.86260211e-01, 3.45560727e-01,\n","       3.96767474e-01, 5.38816734e-01])"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"pkNLUw8ZRIj8","executionInfo":{"status":"ok","timestamp":1623222592782,"user_tz":-540,"elapsed":319,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["def cnn_model(input_shape, classes):\n","    # random.seed를 설정해주면 같은 랜덤셋이 나온다했는데..\n","    # 왜 자꾸 seed를 지정해주는지는 모르겠다 \n","    seed(2021)\n","    tf.random.set_seed(2021)\n","    \n","    # keras input은 데이터의 입력 모양을 모델에 알려주는 역할\n","    input_layer = keras.layers.Input(input_shape)\n","    # Conv1D => 1D 컨볼루션 레이어\n","    # Layers 부분 너무 어렵다 ㅋ 이해 불가\n","    conv1 = keras.layers.Conv1D(filters=128, kernel_size=9, padding='same')(input_layer)\n","    conv1 = keras.layers.BatchNormalization()(conv1)\n","    conv1 = keras.layers.Activation(activation='relu')(conv1)\n","    conv1 = keras.layers.Dropout(rate=0.3)(conv1)\n","\n","    conv2 = keras.layers.Conv1D(filters=256, kernel_size=6, padding='same')(conv1)\n","    conv2 = keras.layers.BatchNormalization()(conv2)\n","    conv2 = keras.layers.Activation('relu')(conv2)\n","    conv2 = keras.layers.Dropout(rate=0.4)(conv2)\n","    \n","    conv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\n","    conv3 = keras.layers.BatchNormalization()(conv3)\n","    conv3 = keras.layers.Activation('relu')(conv3)\n","    conv3 = keras.layers.Dropout(rate=0.5)(conv3)\n","    \n","    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n","    \n","    output_layer = keras.layers.Dense(classes, activation='softmax')(gap)\n","    \n","    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n","    \n","    model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(), \n","        metrics=['accuracy'])\n","    \n","    return model"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GUOQ-tWcRIj9"},"source":["##### 10-fold StratifiedKFold"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"bAlIw_XMXllo","executionInfo":{"status":"ok","timestamp":1623222595312,"user_tz":-540,"elapsed":272,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"44802591-01d8-4465-fa1e-bbfc2f7c1817"},"source":["os.getcwd()"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/DATA'"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"colab":{"base_uri":"https://localhost:8080/"},"id":"-3NqgM5BRIj9","executionInfo":{"status":"ok","timestamp":1623224498683,"user_tz":-540,"elapsed":1899489,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"5e767d8e-b62f-435d-cb24-611248aa1f9b"},"source":["# 과적합을 방지하기 위한 교차검증\n","# StratifiedKFold => target에 속성값 개수를 동일하게 가져감으로써 기존의 kfold와 같은 데이터 몰림현상 방지\n","skf = StratifiedKFold(n_splits = 10, random_state = 2021, shuffle = True)\n","\n","# ReduceLROnPlateau => keras의 콜백함수\n","# 학습률이 개선되지 않을 때 학습률을 동적으로 조정하여 개선하는 효과를 기대할 수 있음\n","# patience => epoch 4 동안 개선되지 않으면 callback 호출\n","# verbos => 1일 경우, EarlyStopping이 적용될 때, 화면에 적용되었다고 나타냄, 0일 경우, 화면에 나타냄 없이 종료\n","# factor => callback 호출 시 학습률을 0.5로 줄인다(1/2)\n","reLR = ReduceLROnPlateau(patience = 4,verbose = 1,factor = 0.5) \n","\n","# EarlyStopping => 더 이상 개선의 여지가 없을 때 학습을 종료시키는 콜백함수\n","# monitor = 'val_loss' => 검증 손실을 기준으로 callback이 호출\n","# patience => epoch 8 동안 개선되지 않으면 callback 호출\n","# mode => 관찰 항목이 val_loss인 경우 감소되는 것이 멈출 때 종료되야 하므로 'min'으로 설정\n","es =EarlyStopping(monitor='val_loss', patience=8, mode='min')\n","\n","accuracy = []\n","losss=[]\n","models=[]\n","\n","for i, (train, validation) in enumerate(skf.split(X, y.argmax(1))) :\n","    # ModelCheckpoint => 모델을 저장할 때 사용하는 콜백 함수\n","    # filepath => 모델을 저장할 경로\n","    # save_best_only => True 인 경우, monitor 되고 있는 값을 기준으로 가장 좋은 값으로 모델이 저장\n","    #                => False인 경우, 매 epoch마다 모델이 filepath{epoch}으로 저장 ex) model0,model1,model2..\n","    # verbose, monitor, mode => 위랑 일맥상통\n","    # save_weights_only => True인 경우, 모델의 weights만 저장\n","    #                   -> False인 경우, 모델 레이어 및 weights 모두 저장\n","    mc = ModelCheckpoint(f'./model_kf/cv_study{i + 1}.h5',save_best_only=True, verbose=0, monitor = 'val_loss', mode = 'min', save_weights_only=True)\n","    print(\"-\" * 20 +\"Fold_\"+str(i+1)+ \"-\" * 20)\n","    model = cnn_model((600,18),61)\n","    history = model.fit(X[train], y[train], epochs = 100, validation_data= (X[validation], y[validation]), \n","                        verbose=1,batch_size=64,callbacks=[es,mc,reLR])\n","    model.load_weights(f'./model_kf/cv_study{i + 1}.h5')\n","    \n","    # model.evaluate => 테스트 데이터를 통해 학습한 모델에 대한 정확도를 평가\n","    # 첫번째 인자 = 테스트 데이터에 해당\n","    # 두번째 인자 = 지도 학습에서 레이블 테스트 데이터에 해당\n","    # batch_size = 배치 크기\n","    k_accuracy = '%.4f' % (model.evaluate(X[validation], y[validation])[1])\n","    k_loss = '%.4f' % (model.evaluate(X[validation], y[validation])[0])\n","    \n","    accuracy.append(k_accuracy)\n","    losss.append(k_loss)\n","    models.append(model)\n","\n","print('\\nK-fold cross validation Auc: {}'.format(accuracy))\n","print('\\nK-fold cross validation loss: {}'.format(losss))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["--------------------Fold_1--------------------\n","Epoch 1/100\n","44/44 [==============================] - 47s 63ms/step - loss: 3.3854 - accuracy: 0.3490 - val_loss: 3.1186 - val_accuracy: 0.3099\n","Epoch 2/100\n","44/44 [==============================] - 2s 44ms/step - loss: 1.9730 - accuracy: 0.5509 - val_loss: 1.9230 - val_accuracy: 0.5335\n","Epoch 3/100\n","44/44 [==============================] - 2s 44ms/step - loss: 1.7848 - accuracy: 0.5647 - val_loss: 2.0415 - val_accuracy: 0.5495\n","Epoch 4/100\n","44/44 [==============================] - 2s 43ms/step - loss: 1.5552 - accuracy: 0.6141 - val_loss: 1.9216 - val_accuracy: 0.5623\n","Epoch 5/100\n","44/44 [==============================] - 2s 44ms/step - loss: 1.3832 - accuracy: 0.6464 - val_loss: 1.5131 - val_accuracy: 0.6070\n","Epoch 6/100\n","44/44 [==============================] - 2s 43ms/step - loss: 1.2681 - accuracy: 0.6767 - val_loss: 1.4045 - val_accuracy: 0.6230\n","Epoch 7/100\n","44/44 [==============================] - 2s 44ms/step - loss: 1.1642 - accuracy: 0.6891 - val_loss: 1.2562 - val_accuracy: 0.6550\n","Epoch 8/100\n","44/44 [==============================] - 2s 44ms/step - loss: 1.0599 - accuracy: 0.7163 - val_loss: 1.1951 - val_accuracy: 0.6741\n","Epoch 9/100\n","44/44 [==============================] - 2s 44ms/step - loss: 0.9699 - accuracy: 0.7388 - val_loss: 1.1102 - val_accuracy: 0.7093\n","Epoch 10/100\n","44/44 [==============================] - 2s 44ms/step - loss: 0.9049 - accuracy: 0.7565 - val_loss: 0.9849 - val_accuracy: 0.7572\n","Epoch 11/100\n","44/44 [==============================] - 2s 44ms/step - loss: 0.8766 - accuracy: 0.7559 - val_loss: 0.9499 - val_accuracy: 0.7700\n","Epoch 12/100\n","44/44 [==============================] - 2s 44ms/step - loss: 0.7876 - accuracy: 0.7879 - val_loss: 1.1078 - val_accuracy: 0.7316\n","Epoch 13/100\n","44/44 [==============================] - 2s 44ms/step - loss: 0.7494 - accuracy: 0.7811 - val_loss: 0.9569 - val_accuracy: 0.7764\n","Epoch 14/100\n","44/44 [==============================] - 2s 44ms/step - loss: 0.7087 - accuracy: 0.8038 - val_loss: 0.8573 - val_accuracy: 0.7700\n","Epoch 15/100\n","44/44 [==============================] - 2s 44ms/step - loss: 0.6789 - accuracy: 0.8012 - val_loss: 0.7829 - val_accuracy: 0.8083\n","Epoch 16/100\n","44/44 [==============================] - 2s 44ms/step - loss: 0.6177 - accuracy: 0.8345 - val_loss: 0.7674 - val_accuracy: 0.8211\n","Epoch 17/100\n","44/44 [==============================] - 2s 44ms/step - loss: 0.5845 - accuracy: 0.8394 - val_loss: 0.7481 - val_accuracy: 0.7891\n","Epoch 18/100\n","44/44 [==============================] - 2s 44ms/step - loss: 0.5729 - accuracy: 0.8267 - val_loss: 0.7253 - val_accuracy: 0.8083\n","Epoch 19/100\n","44/44 [==============================] - 2s 44ms/step - loss: 0.5377 - accuracy: 0.8489 - val_loss: 0.7077 - val_accuracy: 0.8019\n","Epoch 20/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.5015 - accuracy: 0.8563 - val_loss: 0.6760 - val_accuracy: 0.8051\n","Epoch 21/100\n","44/44 [==============================] - 2s 44ms/step - loss: 0.5068 - accuracy: 0.8555 - val_loss: 0.7566 - val_accuracy: 0.8051\n","Epoch 22/100\n","44/44 [==============================] - 2s 44ms/step - loss: 0.4769 - accuracy: 0.8577 - val_loss: 0.6515 - val_accuracy: 0.7987\n","Epoch 23/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.4489 - accuracy: 0.8746 - val_loss: 0.6796 - val_accuracy: 0.7987\n","Epoch 24/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.4305 - accuracy: 0.8793 - val_loss: 0.6961 - val_accuracy: 0.7987\n","Epoch 25/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4528 - accuracy: 0.8754 - val_loss: 0.6685 - val_accuracy: 0.8051\n","Epoch 26/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.4032 - accuracy: 0.8821 - val_loss: 0.7307 - val_accuracy: 0.7923\n","\n","Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 27/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.3778 - accuracy: 0.8980 - val_loss: 0.6048 - val_accuracy: 0.8275\n","Epoch 28/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.3579 - accuracy: 0.9013 - val_loss: 0.6107 - val_accuracy: 0.8307\n","Epoch 29/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.3495 - accuracy: 0.8982 - val_loss: 0.6188 - val_accuracy: 0.8211\n","Epoch 30/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.3655 - accuracy: 0.9054 - val_loss: 0.5933 - val_accuracy: 0.8211\n","Epoch 31/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.3481 - accuracy: 0.9042 - val_loss: 0.6045 - val_accuracy: 0.8307\n","Epoch 32/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.3322 - accuracy: 0.9075 - val_loss: 0.5875 - val_accuracy: 0.8307\n","Epoch 33/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.3065 - accuracy: 0.9190 - val_loss: 0.6043 - val_accuracy: 0.8179\n","Epoch 34/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.3193 - accuracy: 0.9137 - val_loss: 0.5768 - val_accuracy: 0.8403\n","Epoch 35/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.3115 - accuracy: 0.9125 - val_loss: 0.6351 - val_accuracy: 0.8211\n","Epoch 36/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.3204 - accuracy: 0.9088 - val_loss: 0.5910 - val_accuracy: 0.8243\n","Epoch 37/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.3028 - accuracy: 0.9173 - val_loss: 0.5907 - val_accuracy: 0.8307\n","Epoch 38/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2932 - accuracy: 0.9178 - val_loss: 0.5969 - val_accuracy: 0.8179\n","\n","Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 39/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2802 - accuracy: 0.9281 - val_loss: 0.5581 - val_accuracy: 0.8403\n","Epoch 40/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2814 - accuracy: 0.9204 - val_loss: 0.5525 - val_accuracy: 0.8562\n","Epoch 41/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2614 - accuracy: 0.9357 - val_loss: 0.5576 - val_accuracy: 0.8403\n","Epoch 42/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2753 - accuracy: 0.9329 - val_loss: 0.5546 - val_accuracy: 0.8530\n","Epoch 43/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2764 - accuracy: 0.9274 - val_loss: 0.5700 - val_accuracy: 0.8403\n","Epoch 44/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2530 - accuracy: 0.9426 - val_loss: 0.5573 - val_accuracy: 0.8435\n","\n","Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 45/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2422 - accuracy: 0.9412 - val_loss: 0.5623 - val_accuracy: 0.8466\n","Epoch 46/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2500 - accuracy: 0.9416 - val_loss: 0.5551 - val_accuracy: 0.8435\n","Epoch 47/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2555 - accuracy: 0.9390 - val_loss: 0.5462 - val_accuracy: 0.8498\n","Epoch 48/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2540 - accuracy: 0.9353 - val_loss: 0.5487 - val_accuracy: 0.8466\n","Epoch 49/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2461 - accuracy: 0.9332 - val_loss: 0.5500 - val_accuracy: 0.8403\n","Epoch 50/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2497 - accuracy: 0.9342 - val_loss: 0.5482 - val_accuracy: 0.8339\n","Epoch 51/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2358 - accuracy: 0.9435 - val_loss: 0.5483 - val_accuracy: 0.8498\n","\n","Epoch 00051: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","Epoch 52/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2757 - accuracy: 0.9264 - val_loss: 0.5416 - val_accuracy: 0.8466\n","Epoch 53/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2341 - accuracy: 0.9457 - val_loss: 0.5391 - val_accuracy: 0.8498\n","Epoch 54/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2359 - accuracy: 0.9446 - val_loss: 0.5425 - val_accuracy: 0.8466\n","Epoch 55/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2564 - accuracy: 0.9400 - val_loss: 0.5413 - val_accuracy: 0.8435\n","Epoch 56/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2564 - accuracy: 0.9357 - val_loss: 0.5377 - val_accuracy: 0.8530\n","Epoch 57/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2384 - accuracy: 0.9398 - val_loss: 0.5421 - val_accuracy: 0.8466\n","Epoch 58/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2436 - accuracy: 0.9417 - val_loss: 0.5431 - val_accuracy: 0.8530\n","Epoch 59/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2210 - accuracy: 0.9487 - val_loss: 0.5390 - val_accuracy: 0.8435\n","Epoch 60/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2502 - accuracy: 0.9346 - val_loss: 0.5405 - val_accuracy: 0.8562\n","\n","Epoch 00060: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","Epoch 61/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2481 - accuracy: 0.9402 - val_loss: 0.5390 - val_accuracy: 0.8435\n","Epoch 62/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2379 - accuracy: 0.9477 - val_loss: 0.5387 - val_accuracy: 0.8498\n","Epoch 63/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2291 - accuracy: 0.9441 - val_loss: 0.5379 - val_accuracy: 0.8435\n","Epoch 64/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2340 - accuracy: 0.9422 - val_loss: 0.5385 - val_accuracy: 0.8530\n","\n","Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n","10/10 [==============================] - 0s 19ms/step - loss: 0.5377 - accuracy: 0.8530\n","10/10 [==============================] - 0s 8ms/step - loss: 0.5377 - accuracy: 0.8530\n","--------------------Fold_2--------------------\n","Epoch 1/100\n","44/44 [==============================] - 4s 56ms/step - loss: 3.4087 - accuracy: 0.3354 - val_loss: 3.2255 - val_accuracy: 0.3099\n","Epoch 2/100\n","44/44 [==============================] - 2s 46ms/step - loss: 2.0137 - accuracy: 0.5430 - val_loss: 1.8953 - val_accuracy: 0.5431\n","Epoch 3/100\n","44/44 [==============================] - 2s 46ms/step - loss: 1.7352 - accuracy: 0.5813 - val_loss: 1.9697 - val_accuracy: 0.5431\n","Epoch 4/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.5308 - accuracy: 0.6183 - val_loss: 1.6242 - val_accuracy: 0.5751\n","Epoch 5/100\n","44/44 [==============================] - 2s 46ms/step - loss: 1.4247 - accuracy: 0.6293 - val_loss: 1.5189 - val_accuracy: 0.6006\n","Epoch 6/100\n","44/44 [==============================] - 2s 46ms/step - loss: 1.2451 - accuracy: 0.6790 - val_loss: 1.4240 - val_accuracy: 0.6166\n","Epoch 7/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.1711 - accuracy: 0.6860 - val_loss: 1.3118 - val_accuracy: 0.6486\n","Epoch 8/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.1032 - accuracy: 0.7053 - val_loss: 1.2204 - val_accuracy: 0.6741\n","Epoch 9/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.0041 - accuracy: 0.7232 - val_loss: 1.0473 - val_accuracy: 0.7188\n","Epoch 10/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.9420 - accuracy: 0.7445 - val_loss: 0.9411 - val_accuracy: 0.7827\n","Epoch 11/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.8385 - accuracy: 0.7718 - val_loss: 0.8954 - val_accuracy: 0.7508\n","Epoch 12/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.8003 - accuracy: 0.7736 - val_loss: 0.8805 - val_accuracy: 0.7540\n","Epoch 13/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.7522 - accuracy: 0.7943 - val_loss: 0.7989 - val_accuracy: 0.7508\n","Epoch 14/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.7256 - accuracy: 0.7921 - val_loss: 0.7737 - val_accuracy: 0.7923\n","Epoch 15/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6682 - accuracy: 0.8114 - val_loss: 0.7364 - val_accuracy: 0.7827\n","Epoch 16/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6274 - accuracy: 0.8184 - val_loss: 0.8454 - val_accuracy: 0.7923\n","Epoch 17/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.6108 - accuracy: 0.8328 - val_loss: 0.6872 - val_accuracy: 0.7891\n","Epoch 18/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.5734 - accuracy: 0.8373 - val_loss: 0.6849 - val_accuracy: 0.8179\n","Epoch 19/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5298 - accuracy: 0.8582 - val_loss: 0.6215 - val_accuracy: 0.8211\n","Epoch 20/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.5006 - accuracy: 0.8490 - val_loss: 0.6306 - val_accuracy: 0.8147\n","Epoch 21/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4861 - accuracy: 0.8641 - val_loss: 0.6267 - val_accuracy: 0.8339\n","Epoch 22/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4675 - accuracy: 0.8744 - val_loss: 0.6370 - val_accuracy: 0.8243\n","Epoch 23/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4761 - accuracy: 0.8637 - val_loss: 0.6137 - val_accuracy: 0.8147\n","Epoch 24/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4288 - accuracy: 0.8816 - val_loss: 0.6321 - val_accuracy: 0.7923\n","Epoch 25/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4289 - accuracy: 0.8704 - val_loss: 0.6184 - val_accuracy: 0.8147\n","Epoch 26/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4097 - accuracy: 0.8842 - val_loss: 0.6312 - val_accuracy: 0.8083\n","Epoch 27/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4123 - accuracy: 0.8869 - val_loss: 0.5748 - val_accuracy: 0.8019\n","Epoch 28/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4011 - accuracy: 0.8868 - val_loss: 0.5548 - val_accuracy: 0.8339\n","Epoch 29/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3786 - accuracy: 0.8870 - val_loss: 0.5521 - val_accuracy: 0.8307\n","Epoch 30/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3801 - accuracy: 0.8919 - val_loss: 0.5911 - val_accuracy: 0.8307\n","Epoch 31/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.3597 - accuracy: 0.9034 - val_loss: 0.5609 - val_accuracy: 0.8275\n","Epoch 32/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3411 - accuracy: 0.9009 - val_loss: 0.6695 - val_accuracy: 0.7827\n","Epoch 33/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3481 - accuracy: 0.8964 - val_loss: 0.5674 - val_accuracy: 0.8179\n","\n","Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 34/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2958 - accuracy: 0.9202 - val_loss: 0.5591 - val_accuracy: 0.8307\n","Epoch 35/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2925 - accuracy: 0.9272 - val_loss: 0.5030 - val_accuracy: 0.8371\n","Epoch 36/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2995 - accuracy: 0.9197 - val_loss: 0.5115 - val_accuracy: 0.8403\n","Epoch 37/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2800 - accuracy: 0.9272 - val_loss: 0.5229 - val_accuracy: 0.8339\n","Epoch 38/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.3001 - accuracy: 0.9153 - val_loss: 0.5407 - val_accuracy: 0.8403\n","Epoch 39/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2712 - accuracy: 0.9318 - val_loss: 0.5015 - val_accuracy: 0.8435\n","Epoch 40/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2783 - accuracy: 0.9289 - val_loss: 0.5270 - val_accuracy: 0.8339\n","Epoch 41/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2823 - accuracy: 0.9200 - val_loss: 0.5114 - val_accuracy: 0.8371\n","Epoch 42/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2944 - accuracy: 0.9203 - val_loss: 0.5045 - val_accuracy: 0.8371\n","Epoch 43/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2545 - accuracy: 0.9403 - val_loss: 0.5103 - val_accuracy: 0.8371\n","\n","Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 44/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2681 - accuracy: 0.9301 - val_loss: 0.5057 - val_accuracy: 0.8339\n","Epoch 45/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2591 - accuracy: 0.9345 - val_loss: 0.5057 - val_accuracy: 0.8403\n","Epoch 46/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2469 - accuracy: 0.9350 - val_loss: 0.4976 - val_accuracy: 0.8371\n","Epoch 47/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2487 - accuracy: 0.9385 - val_loss: 0.4974 - val_accuracy: 0.8498\n","Epoch 48/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2304 - accuracy: 0.9445 - val_loss: 0.4775 - val_accuracy: 0.8466\n","Epoch 49/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2597 - accuracy: 0.9323 - val_loss: 0.4869 - val_accuracy: 0.8530\n","Epoch 50/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2202 - accuracy: 0.9483 - val_loss: 0.5101 - val_accuracy: 0.8371\n","Epoch 51/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2285 - accuracy: 0.9462 - val_loss: 0.4977 - val_accuracy: 0.8339\n","Epoch 52/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2222 - accuracy: 0.9428 - val_loss: 0.4897 - val_accuracy: 0.8466\n","\n","Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 53/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2165 - accuracy: 0.9483 - val_loss: 0.4879 - val_accuracy: 0.8466\n","Epoch 54/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2023 - accuracy: 0.9530 - val_loss: 0.4945 - val_accuracy: 0.8403\n","Epoch 55/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2158 - accuracy: 0.9467 - val_loss: 0.4769 - val_accuracy: 0.8466\n","Epoch 56/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2094 - accuracy: 0.9514 - val_loss: 0.4882 - val_accuracy: 0.8498\n","Epoch 57/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2004 - accuracy: 0.9474 - val_loss: 0.4841 - val_accuracy: 0.8435\n","Epoch 58/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2117 - accuracy: 0.9458 - val_loss: 0.4805 - val_accuracy: 0.8435\n","Epoch 59/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2172 - accuracy: 0.9483 - val_loss: 0.4823 - val_accuracy: 0.8498\n","\n","Epoch 00059: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","Epoch 60/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2035 - accuracy: 0.9465 - val_loss: 0.4791 - val_accuracy: 0.8498\n","Epoch 61/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2041 - accuracy: 0.9488 - val_loss: 0.4784 - val_accuracy: 0.8498\n","Epoch 62/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2062 - accuracy: 0.9470 - val_loss: 0.4844 - val_accuracy: 0.8435\n","Epoch 63/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1904 - accuracy: 0.9596 - val_loss: 0.4769 - val_accuracy: 0.8466\n","\n","Epoch 00063: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","Epoch 64/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2044 - accuracy: 0.9545 - val_loss: 0.4818 - val_accuracy: 0.8466\n","Epoch 65/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1996 - accuracy: 0.9514 - val_loss: 0.4793 - val_accuracy: 0.8530\n","Epoch 66/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2110 - accuracy: 0.9474 - val_loss: 0.4839 - val_accuracy: 0.8530\n","Epoch 67/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2150 - accuracy: 0.9485 - val_loss: 0.4809 - val_accuracy: 0.8466\n","\n","Epoch 00067: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n","Epoch 68/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1896 - accuracy: 0.9519 - val_loss: 0.4807 - val_accuracy: 0.8498\n","Epoch 69/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2062 - accuracy: 0.9495 - val_loss: 0.4771 - val_accuracy: 0.8466\n","Epoch 70/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2017 - accuracy: 0.9491 - val_loss: 0.4763 - val_accuracy: 0.8498\n","Epoch 71/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1994 - accuracy: 0.9543 - val_loss: 0.4767 - val_accuracy: 0.8530\n","Epoch 72/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2017 - accuracy: 0.9518 - val_loss: 0.4778 - val_accuracy: 0.8530\n","Epoch 73/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1998 - accuracy: 0.9535 - val_loss: 0.4797 - val_accuracy: 0.8530\n","Epoch 74/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1983 - accuracy: 0.9582 - val_loss: 0.4774 - val_accuracy: 0.8498\n","\n","Epoch 00074: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n","Epoch 75/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2060 - accuracy: 0.9516 - val_loss: 0.4784 - val_accuracy: 0.8498\n","Epoch 76/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2033 - accuracy: 0.9497 - val_loss: 0.4779 - val_accuracy: 0.8498\n","Epoch 77/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1970 - accuracy: 0.9520 - val_loss: 0.4780 - val_accuracy: 0.8498\n","Epoch 78/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2099 - accuracy: 0.9494 - val_loss: 0.4765 - val_accuracy: 0.8498\n","\n","Epoch 00078: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n","10/10 [==============================] - 0s 10ms/step - loss: 0.4763 - accuracy: 0.8498\n","10/10 [==============================] - 0s 10ms/step - loss: 0.4763 - accuracy: 0.8498\n","--------------------Fold_3--------------------\n","Epoch 1/100\n","44/44 [==============================] - 4s 56ms/step - loss: 3.3707 - accuracy: 0.3413 - val_loss: 3.1712 - val_accuracy: 0.3323\n","Epoch 2/100\n","44/44 [==============================] - 2s 46ms/step - loss: 1.9792 - accuracy: 0.5490 - val_loss: 1.8588 - val_accuracy: 0.5527\n","Epoch 3/100\n","44/44 [==============================] - 2s 46ms/step - loss: 1.7005 - accuracy: 0.5957 - val_loss: 1.7961 - val_accuracy: 0.5527\n","Epoch 4/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.5332 - accuracy: 0.6167 - val_loss: 1.5767 - val_accuracy: 0.5751\n","Epoch 5/100\n","44/44 [==============================] - 2s 46ms/step - loss: 1.3838 - accuracy: 0.6445 - val_loss: 1.5897 - val_accuracy: 0.6070\n","Epoch 6/100\n","44/44 [==============================] - 2s 46ms/step - loss: 1.2321 - accuracy: 0.6813 - val_loss: 1.3247 - val_accuracy: 0.6358\n","Epoch 7/100\n","44/44 [==============================] - 2s 46ms/step - loss: 1.1609 - accuracy: 0.6955 - val_loss: 1.1625 - val_accuracy: 0.6901\n","Epoch 8/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.0539 - accuracy: 0.7216 - val_loss: 1.0786 - val_accuracy: 0.7252\n","Epoch 9/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.0404 - accuracy: 0.7126 - val_loss: 1.0476 - val_accuracy: 0.7380\n","Epoch 10/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.8957 - accuracy: 0.7581 - val_loss: 0.9319 - val_accuracy: 0.7508\n","Epoch 11/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.8666 - accuracy: 0.7487 - val_loss: 0.9032 - val_accuracy: 0.7796\n","Epoch 12/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.7923 - accuracy: 0.7912 - val_loss: 0.8737 - val_accuracy: 0.7827\n","Epoch 13/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.7455 - accuracy: 0.7852 - val_loss: 0.7612 - val_accuracy: 0.7987\n","Epoch 14/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.7207 - accuracy: 0.7977 - val_loss: 0.7680 - val_accuracy: 0.7732\n","Epoch 15/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6335 - accuracy: 0.8108 - val_loss: 0.7727 - val_accuracy: 0.7796\n","Epoch 16/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6014 - accuracy: 0.8297 - val_loss: 0.7356 - val_accuracy: 0.7796\n","Epoch 17/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.6130 - accuracy: 0.8205 - val_loss: 0.7385 - val_accuracy: 0.7923\n","Epoch 18/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.5624 - accuracy: 0.8339 - val_loss: 0.6862 - val_accuracy: 0.8179\n","Epoch 19/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5362 - accuracy: 0.8566 - val_loss: 0.6114 - val_accuracy: 0.8339\n","Epoch 20/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.5027 - accuracy: 0.8540 - val_loss: 0.6406 - val_accuracy: 0.8211\n","Epoch 21/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.5012 - accuracy: 0.8578 - val_loss: 0.6609 - val_accuracy: 0.8243\n","Epoch 22/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4637 - accuracy: 0.8736 - val_loss: 0.6213 - val_accuracy: 0.8243\n","Epoch 23/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4760 - accuracy: 0.8607 - val_loss: 0.5740 - val_accuracy: 0.8307\n","Epoch 24/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4610 - accuracy: 0.8650 - val_loss: 0.5888 - val_accuracy: 0.8371\n","Epoch 25/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4228 - accuracy: 0.8712 - val_loss: 0.5786 - val_accuracy: 0.8211\n","Epoch 26/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3995 - accuracy: 0.8816 - val_loss: 0.5342 - val_accuracy: 0.8562\n","Epoch 27/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4308 - accuracy: 0.8697 - val_loss: 0.5074 - val_accuracy: 0.8435\n","Epoch 28/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3940 - accuracy: 0.8869 - val_loss: 0.5534 - val_accuracy: 0.8466\n","Epoch 29/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3773 - accuracy: 0.8918 - val_loss: 0.5467 - val_accuracy: 0.8403\n","Epoch 30/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3764 - accuracy: 0.8876 - val_loss: 0.6969 - val_accuracy: 0.7859\n","Epoch 31/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3789 - accuracy: 0.8871 - val_loss: 0.5004 - val_accuracy: 0.8307\n","Epoch 32/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3452 - accuracy: 0.9044 - val_loss: 0.5300 - val_accuracy: 0.8498\n","Epoch 33/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3480 - accuracy: 0.8981 - val_loss: 0.5783 - val_accuracy: 0.8083\n","Epoch 34/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3441 - accuracy: 0.8959 - val_loss: 0.6565 - val_accuracy: 0.7987\n","Epoch 35/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3636 - accuracy: 0.8977 - val_loss: 0.5222 - val_accuracy: 0.8562\n","\n","Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 36/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2896 - accuracy: 0.9231 - val_loss: 0.4962 - val_accuracy: 0.8307\n","Epoch 37/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2747 - accuracy: 0.9292 - val_loss: 0.4808 - val_accuracy: 0.8435\n","Epoch 38/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2765 - accuracy: 0.9195 - val_loss: 0.4688 - val_accuracy: 0.8466\n","Epoch 39/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2669 - accuracy: 0.9223 - val_loss: 0.4574 - val_accuracy: 0.8466\n","Epoch 40/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2725 - accuracy: 0.9188 - val_loss: 0.4667 - val_accuracy: 0.8562\n","Epoch 41/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2757 - accuracy: 0.9266 - val_loss: 0.4592 - val_accuracy: 0.8562\n","Epoch 42/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2813 - accuracy: 0.9175 - val_loss: 0.4965 - val_accuracy: 0.8307\n","Epoch 43/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2716 - accuracy: 0.9328 - val_loss: 0.4930 - val_accuracy: 0.8371\n","\n","Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 44/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2433 - accuracy: 0.9336 - val_loss: 0.4462 - val_accuracy: 0.8530\n","Epoch 45/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2370 - accuracy: 0.9401 - val_loss: 0.4522 - val_accuracy: 0.8594\n","Epoch 46/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2421 - accuracy: 0.9318 - val_loss: 0.4446 - val_accuracy: 0.8562\n","Epoch 47/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2276 - accuracy: 0.9401 - val_loss: 0.4385 - val_accuracy: 0.8562\n","Epoch 48/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2399 - accuracy: 0.9376 - val_loss: 0.4363 - val_accuracy: 0.8626\n","Epoch 49/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2287 - accuracy: 0.9434 - val_loss: 0.4316 - val_accuracy: 0.8690\n","Epoch 50/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2296 - accuracy: 0.9351 - val_loss: 0.4341 - val_accuracy: 0.8626\n","Epoch 51/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2241 - accuracy: 0.9384 - val_loss: 0.4489 - val_accuracy: 0.8498\n","Epoch 52/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2578 - accuracy: 0.9282 - val_loss: 0.4311 - val_accuracy: 0.8626\n","Epoch 53/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2272 - accuracy: 0.9398 - val_loss: 0.4367 - val_accuracy: 0.8498\n","Epoch 54/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2432 - accuracy: 0.9348 - val_loss: 0.4505 - val_accuracy: 0.8498\n","Epoch 55/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2190 - accuracy: 0.9464 - val_loss: 0.4319 - val_accuracy: 0.8658\n","Epoch 56/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2236 - accuracy: 0.9392 - val_loss: 0.4313 - val_accuracy: 0.8594\n","\n","Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 57/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2210 - accuracy: 0.9389 - val_loss: 0.4294 - val_accuracy: 0.8658\n","Epoch 58/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2097 - accuracy: 0.9456 - val_loss: 0.4450 - val_accuracy: 0.8562\n","Epoch 59/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2097 - accuracy: 0.9443 - val_loss: 0.4291 - val_accuracy: 0.8530\n","Epoch 60/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2181 - accuracy: 0.9440 - val_loss: 0.4273 - val_accuracy: 0.8658\n","Epoch 61/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1913 - accuracy: 0.9483 - val_loss: 0.4273 - val_accuracy: 0.8562\n","Epoch 62/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2131 - accuracy: 0.9417 - val_loss: 0.4268 - val_accuracy: 0.8658\n","Epoch 63/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2132 - accuracy: 0.9492 - val_loss: 0.4272 - val_accuracy: 0.8594\n","Epoch 64/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1800 - accuracy: 0.9532 - val_loss: 0.4247 - val_accuracy: 0.8722\n","Epoch 65/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2086 - accuracy: 0.9511 - val_loss: 0.4285 - val_accuracy: 0.8722\n","Epoch 66/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2137 - accuracy: 0.9509 - val_loss: 0.4302 - val_accuracy: 0.8626\n","Epoch 67/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1956 - accuracy: 0.9452 - val_loss: 0.4353 - val_accuracy: 0.8594\n","Epoch 68/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1925 - accuracy: 0.9552 - val_loss: 0.4295 - val_accuracy: 0.8594\n","\n","Epoch 00068: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","Epoch 69/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1823 - accuracy: 0.9520 - val_loss: 0.4251 - val_accuracy: 0.8722\n","Epoch 70/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1963 - accuracy: 0.9484 - val_loss: 0.4231 - val_accuracy: 0.8658\n","Epoch 71/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1828 - accuracy: 0.9549 - val_loss: 0.4250 - val_accuracy: 0.8658\n","Epoch 72/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1899 - accuracy: 0.9502 - val_loss: 0.4206 - val_accuracy: 0.8658\n","Epoch 73/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1925 - accuracy: 0.9553 - val_loss: 0.4210 - val_accuracy: 0.8658\n","Epoch 74/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1912 - accuracy: 0.9578 - val_loss: 0.4202 - val_accuracy: 0.8722\n","Epoch 75/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1949 - accuracy: 0.9464 - val_loss: 0.4191 - val_accuracy: 0.8690\n","Epoch 76/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1831 - accuracy: 0.9494 - val_loss: 0.4201 - val_accuracy: 0.8658\n","Epoch 77/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1885 - accuracy: 0.9524 - val_loss: 0.4209 - val_accuracy: 0.8658\n","Epoch 78/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2000 - accuracy: 0.9447 - val_loss: 0.4219 - val_accuracy: 0.8626\n","Epoch 79/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1744 - accuracy: 0.9624 - val_loss: 0.4195 - val_accuracy: 0.8626\n","\n","Epoch 00079: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","Epoch 80/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1883 - accuracy: 0.9548 - val_loss: 0.4216 - val_accuracy: 0.8722\n","Epoch 81/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1941 - accuracy: 0.9517 - val_loss: 0.4208 - val_accuracy: 0.8690\n","Epoch 82/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.1873 - accuracy: 0.9550 - val_loss: 0.4207 - val_accuracy: 0.8658\n","Epoch 83/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1864 - accuracy: 0.9520 - val_loss: 0.4215 - val_accuracy: 0.8594\n","\n","Epoch 00083: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n","10/10 [==============================] - 0s 8ms/step - loss: 0.4191 - accuracy: 0.8690\n","10/10 [==============================] - 0s 8ms/step - loss: 0.4191 - accuracy: 0.8690\n","--------------------Fold_4--------------------\n","Epoch 1/100\n","44/44 [==============================] - 4s 57ms/step - loss: 3.3705 - accuracy: 0.3537 - val_loss: 3.0903 - val_accuracy: 0.3610\n","Epoch 2/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.9606 - accuracy: 0.5554 - val_loss: 1.9824 - val_accuracy: 0.5431\n","Epoch 3/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.7311 - accuracy: 0.5809 - val_loss: 1.9143 - val_accuracy: 0.5367\n","Epoch 4/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.5520 - accuracy: 0.6212 - val_loss: 1.9509 - val_accuracy: 0.5240\n","Epoch 5/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.3649 - accuracy: 0.6499 - val_loss: 1.6628 - val_accuracy: 0.5783\n","Epoch 6/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.2730 - accuracy: 0.6592 - val_loss: 1.4657 - val_accuracy: 0.6294\n","Epoch 7/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.1638 - accuracy: 0.6978 - val_loss: 1.3142 - val_accuracy: 0.6677\n","Epoch 8/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.0986 - accuracy: 0.7068 - val_loss: 1.4016 - val_accuracy: 0.6454\n","Epoch 9/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.9740 - accuracy: 0.7414 - val_loss: 1.1905 - val_accuracy: 0.6837\n","Epoch 10/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.8989 - accuracy: 0.7421 - val_loss: 1.1071 - val_accuracy: 0.6997\n","Epoch 11/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.8417 - accuracy: 0.7692 - val_loss: 1.0378 - val_accuracy: 0.7093\n","Epoch 12/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.7765 - accuracy: 0.7840 - val_loss: 1.0659 - val_accuracy: 0.7093\n","Epoch 13/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.7308 - accuracy: 0.7847 - val_loss: 1.0423 - val_accuracy: 0.7220\n","Epoch 14/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6823 - accuracy: 0.8049 - val_loss: 0.9788 - val_accuracy: 0.7412\n","Epoch 15/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.6236 - accuracy: 0.8300 - val_loss: 0.9923 - val_accuracy: 0.7604\n","Epoch 16/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.6076 - accuracy: 0.8281 - val_loss: 0.8578 - val_accuracy: 0.7444\n","Epoch 17/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.5797 - accuracy: 0.8355 - val_loss: 0.8448 - val_accuracy: 0.7540\n","Epoch 18/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.5681 - accuracy: 0.8353 - val_loss: 0.8541 - val_accuracy: 0.7636\n","Epoch 19/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.5510 - accuracy: 0.8424 - val_loss: 0.8186 - val_accuracy: 0.7732\n","Epoch 20/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5316 - accuracy: 0.8437 - val_loss: 0.8239 - val_accuracy: 0.7700\n","Epoch 21/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4895 - accuracy: 0.8708 - val_loss: 0.7624 - val_accuracy: 0.7955\n","Epoch 22/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4554 - accuracy: 0.8644 - val_loss: 0.7904 - val_accuracy: 0.7732\n","Epoch 23/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4952 - accuracy: 0.8509 - val_loss: 0.7771 - val_accuracy: 0.7891\n","Epoch 24/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4388 - accuracy: 0.8756 - val_loss: 0.7399 - val_accuracy: 0.7891\n","Epoch 25/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4388 - accuracy: 0.8735 - val_loss: 0.7696 - val_accuracy: 0.7796\n","Epoch 26/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3980 - accuracy: 0.8947 - val_loss: 0.7535 - val_accuracy: 0.7796\n","Epoch 27/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4239 - accuracy: 0.8714 - val_loss: 0.7889 - val_accuracy: 0.7604\n","Epoch 28/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4003 - accuracy: 0.8792 - val_loss: 0.7185 - val_accuracy: 0.7796\n","Epoch 29/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3724 - accuracy: 0.8907 - val_loss: 0.7539 - val_accuracy: 0.7987\n","Epoch 30/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3720 - accuracy: 0.8973 - val_loss: 0.8991 - val_accuracy: 0.7764\n","Epoch 31/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3728 - accuracy: 0.8988 - val_loss: 0.7543 - val_accuracy: 0.7923\n","Epoch 32/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3619 - accuracy: 0.9049 - val_loss: 0.7414 - val_accuracy: 0.7732\n","\n","Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 33/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3375 - accuracy: 0.8989 - val_loss: 0.7023 - val_accuracy: 0.7764\n","Epoch 34/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3102 - accuracy: 0.9137 - val_loss: 0.6553 - val_accuracy: 0.8147\n","Epoch 35/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.3153 - accuracy: 0.9153 - val_loss: 0.6573 - val_accuracy: 0.8179\n","Epoch 36/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2841 - accuracy: 0.9263 - val_loss: 0.6724 - val_accuracy: 0.8019\n","Epoch 37/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2895 - accuracy: 0.9202 - val_loss: 0.6761 - val_accuracy: 0.8019\n","Epoch 38/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2766 - accuracy: 0.9280 - val_loss: 0.6968 - val_accuracy: 0.8115\n","\n","Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 39/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2612 - accuracy: 0.9321 - val_loss: 0.6277 - val_accuracy: 0.8115\n","Epoch 40/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2757 - accuracy: 0.9210 - val_loss: 0.6475 - val_accuracy: 0.8179\n","Epoch 41/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2461 - accuracy: 0.9375 - val_loss: 0.6319 - val_accuracy: 0.8115\n","Epoch 42/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2551 - accuracy: 0.9353 - val_loss: 0.6438 - val_accuracy: 0.8179\n","Epoch 43/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2705 - accuracy: 0.9317 - val_loss: 0.6355 - val_accuracy: 0.8115\n","\n","Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 44/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2646 - accuracy: 0.9330 - val_loss: 0.6258 - val_accuracy: 0.8275\n","Epoch 45/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2394 - accuracy: 0.9392 - val_loss: 0.6279 - val_accuracy: 0.8147\n","Epoch 46/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2383 - accuracy: 0.9420 - val_loss: 0.6275 - val_accuracy: 0.8115\n","Epoch 47/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2332 - accuracy: 0.9422 - val_loss: 0.6233 - val_accuracy: 0.8243\n","Epoch 48/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2392 - accuracy: 0.9379 - val_loss: 0.6390 - val_accuracy: 0.8211\n","Epoch 49/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2492 - accuracy: 0.9354 - val_loss: 0.6274 - val_accuracy: 0.8147\n","Epoch 50/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2354 - accuracy: 0.9332 - val_loss: 0.6326 - val_accuracy: 0.8147\n","Epoch 51/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2266 - accuracy: 0.9447 - val_loss: 0.6184 - val_accuracy: 0.8275\n","Epoch 52/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2359 - accuracy: 0.9368 - val_loss: 0.6259 - val_accuracy: 0.8307\n","Epoch 53/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2369 - accuracy: 0.9398 - val_loss: 0.6268 - val_accuracy: 0.8307\n","Epoch 54/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2152 - accuracy: 0.9474 - val_loss: 0.6207 - val_accuracy: 0.8371\n","Epoch 55/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2262 - accuracy: 0.9441 - val_loss: 0.6242 - val_accuracy: 0.8211\n","\n","Epoch 00055: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","Epoch 56/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2188 - accuracy: 0.9442 - val_loss: 0.6247 - val_accuracy: 0.8243\n","Epoch 57/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2172 - accuracy: 0.9437 - val_loss: 0.6199 - val_accuracy: 0.8243\n","Epoch 58/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2193 - accuracy: 0.9437 - val_loss: 0.6144 - val_accuracy: 0.8307\n","Epoch 59/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2230 - accuracy: 0.9413 - val_loss: 0.6229 - val_accuracy: 0.8339\n","Epoch 60/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2312 - accuracy: 0.9451 - val_loss: 0.6197 - val_accuracy: 0.8243\n","Epoch 61/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2307 - accuracy: 0.9393 - val_loss: 0.6141 - val_accuracy: 0.8307\n","Epoch 62/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2158 - accuracy: 0.9462 - val_loss: 0.6198 - val_accuracy: 0.8243\n","Epoch 63/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.2285 - accuracy: 0.9394 - val_loss: 0.6115 - val_accuracy: 0.8275\n","Epoch 64/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2131 - accuracy: 0.9441 - val_loss: 0.6152 - val_accuracy: 0.8307\n","Epoch 65/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2110 - accuracy: 0.9489 - val_loss: 0.6136 - val_accuracy: 0.8339\n","Epoch 66/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2238 - accuracy: 0.9445 - val_loss: 0.6179 - val_accuracy: 0.8275\n","Epoch 67/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2054 - accuracy: 0.9514 - val_loss: 0.6093 - val_accuracy: 0.8339\n","Epoch 68/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2184 - accuracy: 0.9479 - val_loss: 0.6120 - val_accuracy: 0.8307\n","Epoch 69/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2232 - accuracy: 0.9442 - val_loss: 0.6145 - val_accuracy: 0.8371\n","Epoch 70/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2281 - accuracy: 0.9349 - val_loss: 0.6093 - val_accuracy: 0.8243\n","Epoch 71/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.2027 - accuracy: 0.9426 - val_loss: 0.6150 - val_accuracy: 0.8211\n","\n","Epoch 00071: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","Epoch 72/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2327 - accuracy: 0.9433 - val_loss: 0.6115 - val_accuracy: 0.8307\n","Epoch 73/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2170 - accuracy: 0.9458 - val_loss: 0.6119 - val_accuracy: 0.8179\n","Epoch 74/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2207 - accuracy: 0.9463 - val_loss: 0.6070 - val_accuracy: 0.8307\n","Epoch 75/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1999 - accuracy: 0.9472 - val_loss: 0.6134 - val_accuracy: 0.8307\n","Epoch 76/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2221 - accuracy: 0.9454 - val_loss: 0.6168 - val_accuracy: 0.8243\n","Epoch 77/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2034 - accuracy: 0.9529 - val_loss: 0.6082 - val_accuracy: 0.8243\n","Epoch 78/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2225 - accuracy: 0.9389 - val_loss: 0.6078 - val_accuracy: 0.8307\n","\n","Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n","Epoch 79/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2152 - accuracy: 0.9452 - val_loss: 0.6111 - val_accuracy: 0.8307\n","Epoch 80/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2161 - accuracy: 0.9492 - val_loss: 0.6093 - val_accuracy: 0.8275\n","Epoch 81/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2163 - accuracy: 0.9438 - val_loss: 0.6112 - val_accuracy: 0.8275\n","Epoch 82/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2098 - accuracy: 0.9485 - val_loss: 0.6108 - val_accuracy: 0.8275\n","\n","Epoch 00082: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n","10/10 [==============================] - 0s 11ms/step - loss: 0.6070 - accuracy: 0.8307\n","10/10 [==============================] - 0s 11ms/step - loss: 0.6070 - accuracy: 0.8307\n","--------------------Fold_5--------------------\n","Epoch 1/100\n","44/44 [==============================] - 4s 57ms/step - loss: 3.4279 - accuracy: 0.3378 - val_loss: 3.4392 - val_accuracy: 0.2396\n","Epoch 2/100\n","44/44 [==============================] - 2s 47ms/step - loss: 2.0556 - accuracy: 0.5337 - val_loss: 1.9440 - val_accuracy: 0.5463\n","Epoch 3/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.7248 - accuracy: 0.5903 - val_loss: 2.1446 - val_accuracy: 0.5304\n","Epoch 4/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.5707 - accuracy: 0.6246 - val_loss: 1.7896 - val_accuracy: 0.5527\n","Epoch 5/100\n","44/44 [==============================] - 2s 46ms/step - loss: 1.3814 - accuracy: 0.6504 - val_loss: 1.5220 - val_accuracy: 0.5974\n","Epoch 6/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.2676 - accuracy: 0.6763 - val_loss: 1.3993 - val_accuracy: 0.6230\n","Epoch 7/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.1305 - accuracy: 0.7116 - val_loss: 1.2448 - val_accuracy: 0.6645\n","Epoch 8/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.0766 - accuracy: 0.7111 - val_loss: 1.1601 - val_accuracy: 0.6933\n","Epoch 9/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.9717 - accuracy: 0.7430 - val_loss: 1.0923 - val_accuracy: 0.7125\n","Epoch 10/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.9320 - accuracy: 0.7520 - val_loss: 1.0060 - val_accuracy: 0.7188\n","Epoch 11/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.8547 - accuracy: 0.7757 - val_loss: 0.9617 - val_accuracy: 0.7252\n","Epoch 12/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.8054 - accuracy: 0.7751 - val_loss: 0.8789 - val_accuracy: 0.7508\n","Epoch 13/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.7208 - accuracy: 0.8038 - val_loss: 0.8147 - val_accuracy: 0.7859\n","Epoch 14/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6979 - accuracy: 0.8053 - val_loss: 0.7659 - val_accuracy: 0.7859\n","Epoch 15/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6526 - accuracy: 0.8239 - val_loss: 0.7334 - val_accuracy: 0.7891\n","Epoch 16/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6418 - accuracy: 0.8120 - val_loss: 0.7150 - val_accuracy: 0.7955\n","Epoch 17/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5684 - accuracy: 0.8407 - val_loss: 0.8368 - val_accuracy: 0.7476\n","Epoch 18/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5691 - accuracy: 0.8239 - val_loss: 0.7563 - val_accuracy: 0.7923\n","Epoch 19/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5424 - accuracy: 0.8480 - val_loss: 0.6879 - val_accuracy: 0.8115\n","Epoch 20/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5391 - accuracy: 0.8396 - val_loss: 0.6394 - val_accuracy: 0.8211\n","Epoch 21/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4890 - accuracy: 0.8548 - val_loss: 0.6758 - val_accuracy: 0.7859\n","Epoch 22/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5055 - accuracy: 0.8591 - val_loss: 0.7541 - val_accuracy: 0.8083\n","Epoch 23/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4616 - accuracy: 0.8652 - val_loss: 0.6327 - val_accuracy: 0.7923\n","Epoch 24/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4854 - accuracy: 0.8557 - val_loss: 0.5839 - val_accuracy: 0.8211\n","Epoch 25/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4498 - accuracy: 0.8711 - val_loss: 0.5667 - val_accuracy: 0.8115\n","Epoch 26/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4534 - accuracy: 0.8633 - val_loss: 0.6302 - val_accuracy: 0.7955\n","Epoch 27/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4170 - accuracy: 0.8772 - val_loss: 0.6105 - val_accuracy: 0.8211\n","Epoch 28/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3938 - accuracy: 0.8813 - val_loss: 0.5892 - val_accuracy: 0.8019\n","Epoch 29/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3897 - accuracy: 0.8848 - val_loss: 0.5665 - val_accuracy: 0.8243\n","Epoch 30/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3598 - accuracy: 0.8988 - val_loss: 0.6274 - val_accuracy: 0.8083\n","Epoch 31/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3963 - accuracy: 0.8748 - val_loss: 0.5534 - val_accuracy: 0.8211\n","Epoch 32/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3819 - accuracy: 0.8932 - val_loss: 0.5690 - val_accuracy: 0.8211\n","Epoch 33/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3621 - accuracy: 0.8885 - val_loss: 0.5472 - val_accuracy: 0.8179\n","Epoch 34/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3258 - accuracy: 0.9052 - val_loss: 0.5876 - val_accuracy: 0.8307\n","Epoch 35/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3270 - accuracy: 0.9048 - val_loss: 0.6099 - val_accuracy: 0.8019\n","Epoch 36/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3428 - accuracy: 0.8983 - val_loss: 0.5503 - val_accuracy: 0.8243\n","Epoch 37/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3139 - accuracy: 0.9098 - val_loss: 0.5273 - val_accuracy: 0.8339\n","Epoch 38/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3228 - accuracy: 0.8963 - val_loss: 0.5561 - val_accuracy: 0.7987\n","Epoch 39/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3203 - accuracy: 0.9140 - val_loss: 0.5874 - val_accuracy: 0.8147\n","Epoch 40/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3130 - accuracy: 0.8940 - val_loss: 0.5090 - val_accuracy: 0.8371\n","Epoch 41/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3044 - accuracy: 0.9128 - val_loss: 0.5300 - val_accuracy: 0.8403\n","Epoch 42/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2693 - accuracy: 0.9201 - val_loss: 0.5322 - val_accuracy: 0.8435\n","Epoch 43/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2870 - accuracy: 0.9158 - val_loss: 0.5181 - val_accuracy: 0.8339\n","Epoch 44/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2605 - accuracy: 0.9246 - val_loss: 0.5637 - val_accuracy: 0.8179\n","\n","Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 45/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2593 - accuracy: 0.9263 - val_loss: 0.4560 - val_accuracy: 0.8498\n","Epoch 46/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2296 - accuracy: 0.9388 - val_loss: 0.4710 - val_accuracy: 0.8403\n","Epoch 47/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2071 - accuracy: 0.9445 - val_loss: 0.4640 - val_accuracy: 0.8371\n","Epoch 48/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2097 - accuracy: 0.9430 - val_loss: 0.4631 - val_accuracy: 0.8435\n","Epoch 49/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2104 - accuracy: 0.9446 - val_loss: 0.5260 - val_accuracy: 0.8243\n","\n","Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 50/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2423 - accuracy: 0.9360 - val_loss: 0.4628 - val_accuracy: 0.8435\n","Epoch 51/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1999 - accuracy: 0.9491 - val_loss: 0.4423 - val_accuracy: 0.8562\n","Epoch 52/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2035 - accuracy: 0.9469 - val_loss: 0.4518 - val_accuracy: 0.8530\n","Epoch 53/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2048 - accuracy: 0.9466 - val_loss: 0.4461 - val_accuracy: 0.8498\n","Epoch 54/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2039 - accuracy: 0.9448 - val_loss: 0.4432 - val_accuracy: 0.8466\n","Epoch 55/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1918 - accuracy: 0.9476 - val_loss: 0.4425 - val_accuracy: 0.8562\n","\n","Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 56/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1816 - accuracy: 0.9495 - val_loss: 0.4429 - val_accuracy: 0.8562\n","Epoch 57/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1800 - accuracy: 0.9567 - val_loss: 0.4359 - val_accuracy: 0.8435\n","Epoch 58/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1920 - accuracy: 0.9521 - val_loss: 0.4399 - val_accuracy: 0.8435\n","Epoch 59/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.1698 - accuracy: 0.9609 - val_loss: 0.4445 - val_accuracy: 0.8530\n","Epoch 60/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1682 - accuracy: 0.9588 - val_loss: 0.4358 - val_accuracy: 0.8594\n","Epoch 61/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1837 - accuracy: 0.9498 - val_loss: 0.4362 - val_accuracy: 0.8594\n","\n","Epoch 00061: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","Epoch 62/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1737 - accuracy: 0.9506 - val_loss: 0.4404 - val_accuracy: 0.8562\n","Epoch 63/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1685 - accuracy: 0.9549 - val_loss: 0.4476 - val_accuracy: 0.8530\n","Epoch 64/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1665 - accuracy: 0.9677 - val_loss: 0.4404 - val_accuracy: 0.8530\n","Epoch 65/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1694 - accuracy: 0.9595 - val_loss: 0.4441 - val_accuracy: 0.8466\n","\n","Epoch 00065: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","Epoch 66/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1632 - accuracy: 0.9647 - val_loss: 0.4395 - val_accuracy: 0.8562\n","Epoch 67/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1618 - accuracy: 0.9650 - val_loss: 0.4380 - val_accuracy: 0.8530\n","Epoch 68/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1775 - accuracy: 0.9550 - val_loss: 0.4359 - val_accuracy: 0.8562\n","10/10 [==============================] - 0s 11ms/step - loss: 0.4358 - accuracy: 0.8594\n","10/10 [==============================] - 0s 11ms/step - loss: 0.4358 - accuracy: 0.8594\n","--------------------Fold_6--------------------\n","Epoch 1/100\n","44/44 [==============================] - 4s 63ms/step - loss: 3.4188 - accuracy: 0.3562 - val_loss: 3.3141 - val_accuracy: 0.2949\n","Epoch 2/100\n","44/44 [==============================] - 2s 46ms/step - loss: 1.9763 - accuracy: 0.5473 - val_loss: 1.9991 - val_accuracy: 0.5417\n","Epoch 3/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.7946 - accuracy: 0.5669 - val_loss: 2.0607 - val_accuracy: 0.5385\n","Epoch 4/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.5200 - accuracy: 0.6231 - val_loss: 2.0401 - val_accuracy: 0.5481\n","Epoch 5/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.4264 - accuracy: 0.6384 - val_loss: 1.7113 - val_accuracy: 0.5833\n","Epoch 6/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.2333 - accuracy: 0.6801 - val_loss: 1.6293 - val_accuracy: 0.5962\n","Epoch 7/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.1804 - accuracy: 0.6810 - val_loss: 1.4614 - val_accuracy: 0.6186\n","Epoch 8/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.0298 - accuracy: 0.7256 - val_loss: 1.2580 - val_accuracy: 0.6667\n","Epoch 9/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.9724 - accuracy: 0.7380 - val_loss: 1.1684 - val_accuracy: 0.6987\n","Epoch 10/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.9245 - accuracy: 0.7502 - val_loss: 1.1305 - val_accuracy: 0.7115\n","Epoch 11/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.8642 - accuracy: 0.7623 - val_loss: 1.0841 - val_accuracy: 0.7276\n","Epoch 12/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.7989 - accuracy: 0.7786 - val_loss: 0.9969 - val_accuracy: 0.7244\n","Epoch 13/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.7603 - accuracy: 0.7855 - val_loss: 0.9645 - val_accuracy: 0.7372\n","Epoch 14/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6820 - accuracy: 0.8062 - val_loss: 0.9224 - val_accuracy: 0.7500\n","Epoch 15/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.6855 - accuracy: 0.7813 - val_loss: 0.8495 - val_accuracy: 0.7853\n","Epoch 16/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6432 - accuracy: 0.8084 - val_loss: 0.8548 - val_accuracy: 0.7788\n","Epoch 17/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5789 - accuracy: 0.8305 - val_loss: 0.8223 - val_accuracy: 0.7660\n","Epoch 18/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5265 - accuracy: 0.8513 - val_loss: 0.7836 - val_accuracy: 0.7853\n","Epoch 19/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5463 - accuracy: 0.8456 - val_loss: 0.8092 - val_accuracy: 0.7660\n","Epoch 20/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5269 - accuracy: 0.8394 - val_loss: 0.7155 - val_accuracy: 0.8013\n","Epoch 21/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5309 - accuracy: 0.8413 - val_loss: 0.8173 - val_accuracy: 0.7756\n","Epoch 22/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5024 - accuracy: 0.8461 - val_loss: 0.6949 - val_accuracy: 0.8237\n","Epoch 23/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4571 - accuracy: 0.8593 - val_loss: 0.7494 - val_accuracy: 0.7853\n","Epoch 24/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4442 - accuracy: 0.8614 - val_loss: 0.6720 - val_accuracy: 0.8269\n","Epoch 25/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4173 - accuracy: 0.8777 - val_loss: 0.7109 - val_accuracy: 0.8013\n","Epoch 26/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4235 - accuracy: 0.8665 - val_loss: 0.6749 - val_accuracy: 0.8237\n","Epoch 27/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4198 - accuracy: 0.8790 - val_loss: 0.7137 - val_accuracy: 0.8013\n","Epoch 28/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4300 - accuracy: 0.8675 - val_loss: 0.6761 - val_accuracy: 0.8205\n","\n","Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 29/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4003 - accuracy: 0.8855 - val_loss: 0.6315 - val_accuracy: 0.8333\n","Epoch 30/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3459 - accuracy: 0.9075 - val_loss: 0.6337 - val_accuracy: 0.8269\n","Epoch 31/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3504 - accuracy: 0.9022 - val_loss: 0.6311 - val_accuracy: 0.8301\n","Epoch 32/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3415 - accuracy: 0.9026 - val_loss: 0.6853 - val_accuracy: 0.8109\n","Epoch 33/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.3353 - accuracy: 0.8998 - val_loss: 0.6132 - val_accuracy: 0.8365\n","Epoch 34/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3135 - accuracy: 0.9115 - val_loss: 0.6328 - val_accuracy: 0.8205\n","Epoch 35/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3016 - accuracy: 0.9187 - val_loss: 0.6353 - val_accuracy: 0.8269\n","Epoch 36/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3171 - accuracy: 0.9153 - val_loss: 0.6916 - val_accuracy: 0.8141\n","Epoch 37/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3332 - accuracy: 0.9104 - val_loss: 0.6143 - val_accuracy: 0.8397\n","\n","Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 38/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3035 - accuracy: 0.9178 - val_loss: 0.6122 - val_accuracy: 0.8333\n","Epoch 39/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2891 - accuracy: 0.9214 - val_loss: 0.6185 - val_accuracy: 0.8397\n","Epoch 40/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2722 - accuracy: 0.9308 - val_loss: 0.6127 - val_accuracy: 0.8429\n","Epoch 41/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2789 - accuracy: 0.9252 - val_loss: 0.6430 - val_accuracy: 0.8365\n","Epoch 42/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2782 - accuracy: 0.9249 - val_loss: 0.6578 - val_accuracy: 0.8141\n","\n","Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 43/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2548 - accuracy: 0.9316 - val_loss: 0.5939 - val_accuracy: 0.8365\n","Epoch 44/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2713 - accuracy: 0.9281 - val_loss: 0.6035 - val_accuracy: 0.8429\n","Epoch 45/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2712 - accuracy: 0.9298 - val_loss: 0.5900 - val_accuracy: 0.8333\n","Epoch 46/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2605 - accuracy: 0.9316 - val_loss: 0.5928 - val_accuracy: 0.8365\n","Epoch 47/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2648 - accuracy: 0.9348 - val_loss: 0.5918 - val_accuracy: 0.8429\n","Epoch 48/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2651 - accuracy: 0.9314 - val_loss: 0.5931 - val_accuracy: 0.8333\n","Epoch 49/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2597 - accuracy: 0.9336 - val_loss: 0.5912 - val_accuracy: 0.8397\n","\n","Epoch 00049: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","Epoch 50/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2482 - accuracy: 0.9445 - val_loss: 0.5927 - val_accuracy: 0.8397\n","Epoch 51/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2464 - accuracy: 0.9378 - val_loss: 0.5904 - val_accuracy: 0.8397\n","Epoch 52/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2519 - accuracy: 0.9383 - val_loss: 0.5965 - val_accuracy: 0.8333\n","Epoch 53/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2339 - accuracy: 0.9418 - val_loss: 0.5873 - val_accuracy: 0.8365\n","Epoch 54/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2482 - accuracy: 0.9398 - val_loss: 0.5915 - val_accuracy: 0.8429\n","Epoch 55/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2463 - accuracy: 0.9415 - val_loss: 0.5852 - val_accuracy: 0.8429\n","Epoch 56/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2392 - accuracy: 0.9429 - val_loss: 0.5904 - val_accuracy: 0.8333\n","Epoch 57/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2485 - accuracy: 0.9381 - val_loss: 0.5846 - val_accuracy: 0.8397\n","Epoch 58/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2488 - accuracy: 0.9279 - val_loss: 0.5874 - val_accuracy: 0.8365\n","Epoch 59/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2561 - accuracy: 0.9328 - val_loss: 0.5897 - val_accuracy: 0.8365\n","Epoch 60/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2389 - accuracy: 0.9370 - val_loss: 0.5875 - val_accuracy: 0.8397\n","Epoch 61/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2245 - accuracy: 0.9511 - val_loss: 0.5925 - val_accuracy: 0.8365\n","\n","Epoch 00061: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","Epoch 62/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2403 - accuracy: 0.9356 - val_loss: 0.5899 - val_accuracy: 0.8301\n","Epoch 63/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2342 - accuracy: 0.9414 - val_loss: 0.5876 - val_accuracy: 0.8397\n","Epoch 64/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2389 - accuracy: 0.9404 - val_loss: 0.5859 - val_accuracy: 0.8365\n","Epoch 65/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2376 - accuracy: 0.9393 - val_loss: 0.5859 - val_accuracy: 0.8429\n","\n","Epoch 00065: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n","10/10 [==============================] - 0s 13ms/step - loss: 0.5846 - accuracy: 0.8397\n","10/10 [==============================] - 0s 8ms/step - loss: 0.5846 - accuracy: 0.8397\n","--------------------Fold_7--------------------\n","Epoch 1/100\n","44/44 [==============================] - 4s 58ms/step - loss: 3.4261 - accuracy: 0.3292 - val_loss: 3.4108 - val_accuracy: 0.2212\n","Epoch 2/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.9774 - accuracy: 0.5514 - val_loss: 1.8769 - val_accuracy: 0.5385\n","Epoch 3/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.7193 - accuracy: 0.5810 - val_loss: 1.9796 - val_accuracy: 0.5449\n","Epoch 4/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.5072 - accuracy: 0.6313 - val_loss: 1.7791 - val_accuracy: 0.5513\n","Epoch 5/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.3915 - accuracy: 0.6479 - val_loss: 1.6448 - val_accuracy: 0.5737\n","Epoch 6/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.2870 - accuracy: 0.6613 - val_loss: 1.3768 - val_accuracy: 0.6314\n","Epoch 7/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.1339 - accuracy: 0.7001 - val_loss: 1.3386 - val_accuracy: 0.6378\n","Epoch 8/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.0873 - accuracy: 0.7073 - val_loss: 1.2218 - val_accuracy: 0.6571\n","Epoch 9/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.0001 - accuracy: 0.7415 - val_loss: 1.0754 - val_accuracy: 0.6955\n","Epoch 10/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.9168 - accuracy: 0.7516 - val_loss: 1.0118 - val_accuracy: 0.7340\n","Epoch 11/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.8366 - accuracy: 0.7755 - val_loss: 0.9561 - val_accuracy: 0.7179\n","Epoch 12/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.7708 - accuracy: 0.7846 - val_loss: 0.9036 - val_accuracy: 0.7724\n","Epoch 13/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.7418 - accuracy: 0.7899 - val_loss: 0.8610 - val_accuracy: 0.7308\n","Epoch 14/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.7017 - accuracy: 0.8043 - val_loss: 0.8170 - val_accuracy: 0.7692\n","Epoch 15/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6925 - accuracy: 0.8085 - val_loss: 0.7568 - val_accuracy: 0.7756\n","Epoch 16/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6266 - accuracy: 0.8145 - val_loss: 0.7518 - val_accuracy: 0.7949\n","Epoch 17/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5917 - accuracy: 0.8334 - val_loss: 0.7112 - val_accuracy: 0.8141\n","Epoch 18/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.6069 - accuracy: 0.8219 - val_loss: 0.7281 - val_accuracy: 0.8013\n","Epoch 19/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5251 - accuracy: 0.8519 - val_loss: 0.7577 - val_accuracy: 0.7788\n","Epoch 20/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.5276 - accuracy: 0.8501 - val_loss: 0.6542 - val_accuracy: 0.8173\n","Epoch 21/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5130 - accuracy: 0.8588 - val_loss: 0.6456 - val_accuracy: 0.8109\n","Epoch 22/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4950 - accuracy: 0.8547 - val_loss: 0.7715 - val_accuracy: 0.7724\n","Epoch 23/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4803 - accuracy: 0.8587 - val_loss: 0.6247 - val_accuracy: 0.8013\n","Epoch 24/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4769 - accuracy: 0.8547 - val_loss: 0.6607 - val_accuracy: 0.7949\n","Epoch 25/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4728 - accuracy: 0.8682 - val_loss: 0.6080 - val_accuracy: 0.8141\n","Epoch 26/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3931 - accuracy: 0.8906 - val_loss: 0.5716 - val_accuracy: 0.8526\n","Epoch 27/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4078 - accuracy: 0.8746 - val_loss: 0.5706 - val_accuracy: 0.8462\n","Epoch 28/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3903 - accuracy: 0.8898 - val_loss: 0.5449 - val_accuracy: 0.8077\n","Epoch 29/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3979 - accuracy: 0.8917 - val_loss: 0.5634 - val_accuracy: 0.8462\n","Epoch 30/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3764 - accuracy: 0.8903 - val_loss: 0.5867 - val_accuracy: 0.8237\n","Epoch 31/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.3635 - accuracy: 0.8961 - val_loss: 0.5583 - val_accuracy: 0.8269\n","Epoch 32/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.3538 - accuracy: 0.8942 - val_loss: 0.5899 - val_accuracy: 0.8269\n","\n","Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 33/100\n","44/44 [==============================] - 2s 51ms/step - loss: 0.3118 - accuracy: 0.9158 - val_loss: 0.5145 - val_accuracy: 0.8494\n","Epoch 34/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.3191 - accuracy: 0.9111 - val_loss: 0.5605 - val_accuracy: 0.8462\n","Epoch 35/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3284 - accuracy: 0.9078 - val_loss: 0.5469 - val_accuracy: 0.8429\n","Epoch 36/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2867 - accuracy: 0.9256 - val_loss: 0.5441 - val_accuracy: 0.8365\n","Epoch 37/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3089 - accuracy: 0.9120 - val_loss: 0.5007 - val_accuracy: 0.8429\n","Epoch 38/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3208 - accuracy: 0.9099 - val_loss: 0.5366 - val_accuracy: 0.8269\n","Epoch 39/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2766 - accuracy: 0.9241 - val_loss: 0.5378 - val_accuracy: 0.8365\n","Epoch 40/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2792 - accuracy: 0.9293 - val_loss: 0.5208 - val_accuracy: 0.8462\n","Epoch 41/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2928 - accuracy: 0.9132 - val_loss: 0.5261 - val_accuracy: 0.8365\n","\n","Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 42/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2501 - accuracy: 0.9394 - val_loss: 0.4956 - val_accuracy: 0.8462\n","Epoch 43/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2536 - accuracy: 0.9387 - val_loss: 0.4965 - val_accuracy: 0.8365\n","Epoch 44/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2594 - accuracy: 0.9332 - val_loss: 0.4874 - val_accuracy: 0.8429\n","Epoch 45/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2471 - accuracy: 0.9345 - val_loss: 0.4911 - val_accuracy: 0.8494\n","Epoch 46/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2608 - accuracy: 0.9265 - val_loss: 0.4818 - val_accuracy: 0.8526\n","Epoch 47/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2389 - accuracy: 0.9387 - val_loss: 0.4803 - val_accuracy: 0.8462\n","Epoch 48/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2445 - accuracy: 0.9380 - val_loss: 0.4912 - val_accuracy: 0.8429\n","Epoch 49/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2367 - accuracy: 0.9347 - val_loss: 0.5130 - val_accuracy: 0.8462\n","Epoch 50/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2250 - accuracy: 0.9425 - val_loss: 0.4839 - val_accuracy: 0.8494\n","Epoch 51/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2249 - accuracy: 0.9424 - val_loss: 0.4821 - val_accuracy: 0.8429\n","\n","Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 52/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2310 - accuracy: 0.9458 - val_loss: 0.4807 - val_accuracy: 0.8526\n","Epoch 53/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2345 - accuracy: 0.9413 - val_loss: 0.4695 - val_accuracy: 0.8462\n","Epoch 54/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2335 - accuracy: 0.9379 - val_loss: 0.4779 - val_accuracy: 0.8526\n","Epoch 55/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2259 - accuracy: 0.9441 - val_loss: 0.4766 - val_accuracy: 0.8462\n","Epoch 56/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2194 - accuracy: 0.9436 - val_loss: 0.4679 - val_accuracy: 0.8558\n","Epoch 57/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2395 - accuracy: 0.9376 - val_loss: 0.4669 - val_accuracy: 0.8526\n","Epoch 58/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2310 - accuracy: 0.9393 - val_loss: 0.4560 - val_accuracy: 0.8558\n","Epoch 59/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2319 - accuracy: 0.9415 - val_loss: 0.4646 - val_accuracy: 0.8558\n","Epoch 60/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2281 - accuracy: 0.9474 - val_loss: 0.4579 - val_accuracy: 0.8558\n","Epoch 61/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2066 - accuracy: 0.9494 - val_loss: 0.4665 - val_accuracy: 0.8462\n","Epoch 62/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2170 - accuracy: 0.9498 - val_loss: 0.4722 - val_accuracy: 0.8526\n","\n","Epoch 00062: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","Epoch 63/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2123 - accuracy: 0.9447 - val_loss: 0.4674 - val_accuracy: 0.8526\n","Epoch 64/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2216 - accuracy: 0.9385 - val_loss: 0.4593 - val_accuracy: 0.8558\n","Epoch 65/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1984 - accuracy: 0.9549 - val_loss: 0.4639 - val_accuracy: 0.8558\n","Epoch 66/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2099 - accuracy: 0.9486 - val_loss: 0.4611 - val_accuracy: 0.8526\n","\n","Epoch 00066: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","10/10 [==============================] - 0s 11ms/step - loss: 0.4560 - accuracy: 0.8558\n","10/10 [==============================] - 0s 11ms/step - loss: 0.4560 - accuracy: 0.8558\n","--------------------Fold_8--------------------\n","Epoch 1/100\n","44/44 [==============================] - 4s 57ms/step - loss: 3.3943 - accuracy: 0.3472 - val_loss: 3.2593 - val_accuracy: 0.3173\n","Epoch 2/100\n","44/44 [==============================] - 2s 47ms/step - loss: 2.0112 - accuracy: 0.5406 - val_loss: 1.8587 - val_accuracy: 0.5385\n","Epoch 3/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.7212 - accuracy: 0.5914 - val_loss: 1.8769 - val_accuracy: 0.5641\n","Epoch 4/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.5069 - accuracy: 0.6342 - val_loss: 1.8546 - val_accuracy: 0.5673\n","Epoch 5/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.4494 - accuracy: 0.6290 - val_loss: 1.5724 - val_accuracy: 0.6058\n","Epoch 6/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.2841 - accuracy: 0.6585 - val_loss: 1.4325 - val_accuracy: 0.6378\n","Epoch 7/100\n","44/44 [==============================] - 2s 50ms/step - loss: 1.1323 - accuracy: 0.7068 - val_loss: 1.2474 - val_accuracy: 0.6474\n","Epoch 8/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.0402 - accuracy: 0.7174 - val_loss: 1.2045 - val_accuracy: 0.6603\n","Epoch 9/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.9882 - accuracy: 0.7353 - val_loss: 1.0541 - val_accuracy: 0.7019\n","Epoch 10/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.8676 - accuracy: 0.7675 - val_loss: 1.0094 - val_accuracy: 0.7147\n","Epoch 11/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.8299 - accuracy: 0.7701 - val_loss: 0.9498 - val_accuracy: 0.7147\n","Epoch 12/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.7941 - accuracy: 0.7883 - val_loss: 0.9026 - val_accuracy: 0.7244\n","Epoch 13/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.7050 - accuracy: 0.8091 - val_loss: 0.8125 - val_accuracy: 0.7276\n","Epoch 14/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.7034 - accuracy: 0.8068 - val_loss: 0.9015 - val_accuracy: 0.7788\n","Epoch 15/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.6797 - accuracy: 0.8041 - val_loss: 0.7837 - val_accuracy: 0.7564\n","Epoch 16/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6194 - accuracy: 0.8244 - val_loss: 0.8264 - val_accuracy: 0.7660\n","Epoch 17/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.6090 - accuracy: 0.8226 - val_loss: 0.7090 - val_accuracy: 0.7788\n","Epoch 18/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5696 - accuracy: 0.8338 - val_loss: 0.7007 - val_accuracy: 0.7981\n","Epoch 19/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5510 - accuracy: 0.8331 - val_loss: 0.6655 - val_accuracy: 0.7821\n","Epoch 20/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5438 - accuracy: 0.8321 - val_loss: 0.6047 - val_accuracy: 0.8173\n","Epoch 21/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4854 - accuracy: 0.8558 - val_loss: 0.6204 - val_accuracy: 0.7853\n","Epoch 22/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4846 - accuracy: 0.8659 - val_loss: 0.6210 - val_accuracy: 0.7724\n","Epoch 23/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4606 - accuracy: 0.8656 - val_loss: 0.6304 - val_accuracy: 0.7724\n","Epoch 24/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4708 - accuracy: 0.8663 - val_loss: 0.5978 - val_accuracy: 0.8141\n","Epoch 25/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4200 - accuracy: 0.8792 - val_loss: 0.6169 - val_accuracy: 0.7853\n","Epoch 26/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4257 - accuracy: 0.8803 - val_loss: 0.5820 - val_accuracy: 0.8173\n","Epoch 27/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4099 - accuracy: 0.8891 - val_loss: 0.5787 - val_accuracy: 0.8173\n","Epoch 28/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3955 - accuracy: 0.8868 - val_loss: 0.5631 - val_accuracy: 0.8269\n","Epoch 29/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3596 - accuracy: 0.8979 - val_loss: 0.6093 - val_accuracy: 0.7949\n","Epoch 30/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.3820 - accuracy: 0.8898 - val_loss: 0.5926 - val_accuracy: 0.8301\n","Epoch 31/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3555 - accuracy: 0.8930 - val_loss: 0.6164 - val_accuracy: 0.8109\n","Epoch 32/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3899 - accuracy: 0.8845 - val_loss: 0.5821 - val_accuracy: 0.8141\n","\n","Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 33/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3230 - accuracy: 0.9102 - val_loss: 0.5443 - val_accuracy: 0.8429\n","Epoch 34/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.3188 - accuracy: 0.9085 - val_loss: 0.5024 - val_accuracy: 0.8365\n","Epoch 35/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2979 - accuracy: 0.9218 - val_loss: 0.5249 - val_accuracy: 0.8205\n","Epoch 36/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2883 - accuracy: 0.9293 - val_loss: 0.5102 - val_accuracy: 0.8301\n","Epoch 37/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3141 - accuracy: 0.9089 - val_loss: 0.5162 - val_accuracy: 0.8333\n","Epoch 38/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2918 - accuracy: 0.9211 - val_loss: 0.5485 - val_accuracy: 0.8013\n","\n","Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 39/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2854 - accuracy: 0.9258 - val_loss: 0.4593 - val_accuracy: 0.8558\n","Epoch 40/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2748 - accuracy: 0.9301 - val_loss: 0.4615 - val_accuracy: 0.8622\n","Epoch 41/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2586 - accuracy: 0.9381 - val_loss: 0.4562 - val_accuracy: 0.8654\n","Epoch 42/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2548 - accuracy: 0.9350 - val_loss: 0.4531 - val_accuracy: 0.8590\n","Epoch 43/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2669 - accuracy: 0.9299 - val_loss: 0.4431 - val_accuracy: 0.8654\n","Epoch 44/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2566 - accuracy: 0.9389 - val_loss: 0.4437 - val_accuracy: 0.8654\n","Epoch 45/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2720 - accuracy: 0.9260 - val_loss: 0.4660 - val_accuracy: 0.8397\n","Epoch 46/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2524 - accuracy: 0.9350 - val_loss: 0.4661 - val_accuracy: 0.8494\n","Epoch 47/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2479 - accuracy: 0.9347 - val_loss: 0.4624 - val_accuracy: 0.8654\n","\n","Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 48/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2483 - accuracy: 0.9380 - val_loss: 0.4466 - val_accuracy: 0.8622\n","Epoch 49/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2338 - accuracy: 0.9407 - val_loss: 0.4437 - val_accuracy: 0.8590\n","Epoch 50/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2333 - accuracy: 0.9439 - val_loss: 0.4420 - val_accuracy: 0.8654\n","Epoch 51/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2270 - accuracy: 0.9402 - val_loss: 0.4529 - val_accuracy: 0.8494\n","Epoch 52/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2349 - accuracy: 0.9387 - val_loss: 0.4424 - val_accuracy: 0.8654\n","Epoch 53/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2522 - accuracy: 0.9373 - val_loss: 0.4372 - val_accuracy: 0.8590\n","Epoch 54/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2299 - accuracy: 0.9460 - val_loss: 0.4374 - val_accuracy: 0.8686\n","Epoch 55/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2422 - accuracy: 0.9348 - val_loss: 0.4413 - val_accuracy: 0.8622\n","Epoch 56/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2613 - accuracy: 0.9288 - val_loss: 0.4409 - val_accuracy: 0.8622\n","Epoch 57/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2504 - accuracy: 0.9373 - val_loss: 0.4443 - val_accuracy: 0.8654\n","\n","Epoch 00057: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","Epoch 58/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2274 - accuracy: 0.9440 - val_loss: 0.4350 - val_accuracy: 0.8654\n","Epoch 59/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2317 - accuracy: 0.9437 - val_loss: 0.4333 - val_accuracy: 0.8622\n","Epoch 60/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2203 - accuracy: 0.9403 - val_loss: 0.4342 - val_accuracy: 0.8654\n","Epoch 61/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2105 - accuracy: 0.9518 - val_loss: 0.4318 - val_accuracy: 0.8654\n","Epoch 62/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2330 - accuracy: 0.9411 - val_loss: 0.4310 - val_accuracy: 0.8622\n","Epoch 63/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2389 - accuracy: 0.9381 - val_loss: 0.4323 - val_accuracy: 0.8654\n","Epoch 64/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2282 - accuracy: 0.9374 - val_loss: 0.4345 - val_accuracy: 0.8590\n","Epoch 65/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.2238 - accuracy: 0.9466 - val_loss: 0.4352 - val_accuracy: 0.8686\n","Epoch 66/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2330 - accuracy: 0.9410 - val_loss: 0.4339 - val_accuracy: 0.8622\n","\n","Epoch 00066: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","Epoch 67/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2345 - accuracy: 0.9375 - val_loss: 0.4305 - val_accuracy: 0.8718\n","Epoch 68/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2294 - accuracy: 0.9367 - val_loss: 0.4310 - val_accuracy: 0.8686\n","Epoch 69/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2109 - accuracy: 0.9474 - val_loss: 0.4314 - val_accuracy: 0.8686\n","Epoch 70/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2298 - accuracy: 0.9376 - val_loss: 0.4303 - val_accuracy: 0.8622\n","Epoch 71/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2241 - accuracy: 0.9429 - val_loss: 0.4297 - val_accuracy: 0.8622\n","Epoch 72/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2241 - accuracy: 0.9437 - val_loss: 0.4274 - val_accuracy: 0.8686\n","Epoch 73/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2189 - accuracy: 0.9512 - val_loss: 0.4267 - val_accuracy: 0.8686\n","Epoch 74/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2216 - accuracy: 0.9455 - val_loss: 0.4267 - val_accuracy: 0.8686\n","Epoch 75/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2272 - accuracy: 0.9421 - val_loss: 0.4265 - val_accuracy: 0.8654\n","Epoch 76/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2253 - accuracy: 0.9388 - val_loss: 0.4257 - val_accuracy: 0.8654\n","Epoch 77/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2179 - accuracy: 0.9395 - val_loss: 0.4287 - val_accuracy: 0.8718\n","Epoch 78/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2090 - accuracy: 0.9552 - val_loss: 0.4277 - val_accuracy: 0.8718\n","Epoch 79/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2184 - accuracy: 0.9466 - val_loss: 0.4276 - val_accuracy: 0.8750\n","Epoch 80/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2227 - accuracy: 0.9449 - val_loss: 0.4275 - val_accuracy: 0.8686\n","\n","Epoch 00080: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n","Epoch 81/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2251 - accuracy: 0.9454 - val_loss: 0.4265 - val_accuracy: 0.8654\n","Epoch 82/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2035 - accuracy: 0.9528 - val_loss: 0.4269 - val_accuracy: 0.8654\n","Epoch 83/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2245 - accuracy: 0.9472 - val_loss: 0.4258 - val_accuracy: 0.8686\n","Epoch 84/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2234 - accuracy: 0.9410 - val_loss: 0.4253 - val_accuracy: 0.8654\n","Epoch 85/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2256 - accuracy: 0.9412 - val_loss: 0.4259 - val_accuracy: 0.8686\n","Epoch 86/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2193 - accuracy: 0.9446 - val_loss: 0.4254 - val_accuracy: 0.8654\n","Epoch 87/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2098 - accuracy: 0.9434 - val_loss: 0.4253 - val_accuracy: 0.8686\n","Epoch 88/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2126 - accuracy: 0.9452 - val_loss: 0.4250 - val_accuracy: 0.8686\n","Epoch 89/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2184 - accuracy: 0.9463 - val_loss: 0.4250 - val_accuracy: 0.8686\n","Epoch 90/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2173 - accuracy: 0.9458 - val_loss: 0.4252 - val_accuracy: 0.8654\n","Epoch 91/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2069 - accuracy: 0.9488 - val_loss: 0.4256 - val_accuracy: 0.8654\n","Epoch 92/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2137 - accuracy: 0.9418 - val_loss: 0.4254 - val_accuracy: 0.8686\n","\n","Epoch 00092: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n","Epoch 93/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2159 - accuracy: 0.9454 - val_loss: 0.4254 - val_accuracy: 0.8654\n","Epoch 94/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2065 - accuracy: 0.9502 - val_loss: 0.4259 - val_accuracy: 0.8654\n","Epoch 95/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2134 - accuracy: 0.9448 - val_loss: 0.4253 - val_accuracy: 0.8654\n","Epoch 96/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2050 - accuracy: 0.9465 - val_loss: 0.4256 - val_accuracy: 0.8654\n","\n","Epoch 00096: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n","Epoch 97/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2052 - accuracy: 0.9471 - val_loss: 0.4250 - val_accuracy: 0.8654\n","Epoch 98/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2111 - accuracy: 0.9494 - val_loss: 0.4254 - val_accuracy: 0.8654\n","Epoch 99/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2201 - accuracy: 0.9472 - val_loss: 0.4255 - val_accuracy: 0.8654\n","Epoch 100/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2078 - accuracy: 0.9534 - val_loss: 0.4253 - val_accuracy: 0.8654\n","\n","Epoch 00100: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n","10/10 [==============================] - 0s 11ms/step - loss: 0.4250 - accuracy: 0.8654\n","10/10 [==============================] - 0s 11ms/step - loss: 0.4250 - accuracy: 0.8654\n","--------------------Fold_9--------------------\n","Epoch 1/100\n","44/44 [==============================] - 4s 58ms/step - loss: 3.3783 - accuracy: 0.3489 - val_loss: 2.9251 - val_accuracy: 0.4615\n","Epoch 2/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.9826 - accuracy: 0.5604 - val_loss: 1.9077 - val_accuracy: 0.5449\n","Epoch 3/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.7717 - accuracy: 0.5709 - val_loss: 1.8145 - val_accuracy: 0.5609\n","Epoch 4/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.5372 - accuracy: 0.6239 - val_loss: 1.6922 - val_accuracy: 0.5609\n","Epoch 5/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.3953 - accuracy: 0.6417 - val_loss: 1.5709 - val_accuracy: 0.5833\n","Epoch 6/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.2645 - accuracy: 0.6650 - val_loss: 1.3291 - val_accuracy: 0.6282\n","Epoch 7/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.1646 - accuracy: 0.6902 - val_loss: 1.2590 - val_accuracy: 0.6538\n","Epoch 8/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.0608 - accuracy: 0.7219 - val_loss: 1.1637 - val_accuracy: 0.6667\n","Epoch 9/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.0092 - accuracy: 0.7306 - val_loss: 1.0650 - val_accuracy: 0.7179\n","Epoch 10/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.9024 - accuracy: 0.7552 - val_loss: 1.0370 - val_accuracy: 0.7115\n","Epoch 11/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.8650 - accuracy: 0.7582 - val_loss: 1.0008 - val_accuracy: 0.7019\n","Epoch 12/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.7520 - accuracy: 0.7968 - val_loss: 0.8978 - val_accuracy: 0.7500\n","Epoch 13/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.7722 - accuracy: 0.7743 - val_loss: 0.8355 - val_accuracy: 0.7500\n","Epoch 14/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.6737 - accuracy: 0.8137 - val_loss: 0.8357 - val_accuracy: 0.7692\n","Epoch 15/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.6622 - accuracy: 0.8093 - val_loss: 0.7517 - val_accuracy: 0.7885\n","Epoch 16/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6034 - accuracy: 0.8257 - val_loss: 0.7348 - val_accuracy: 0.7788\n","Epoch 17/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.6170 - accuracy: 0.8116 - val_loss: 0.7586 - val_accuracy: 0.7436\n","Epoch 18/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5572 - accuracy: 0.8356 - val_loss: 0.6675 - val_accuracy: 0.7917\n","Epoch 19/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5337 - accuracy: 0.8454 - val_loss: 0.6890 - val_accuracy: 0.7853\n","Epoch 20/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5310 - accuracy: 0.8407 - val_loss: 0.6865 - val_accuracy: 0.7724\n","Epoch 21/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4773 - accuracy: 0.8573 - val_loss: 0.6807 - val_accuracy: 0.8109\n","Epoch 22/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4963 - accuracy: 0.8450 - val_loss: 0.6161 - val_accuracy: 0.8109\n","Epoch 23/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4598 - accuracy: 0.8745 - val_loss: 0.5957 - val_accuracy: 0.8141\n","Epoch 24/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4358 - accuracy: 0.8714 - val_loss: 0.5849 - val_accuracy: 0.8141\n","Epoch 25/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4527 - accuracy: 0.8641 - val_loss: 0.5716 - val_accuracy: 0.8205\n","Epoch 26/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3942 - accuracy: 0.8944 - val_loss: 0.5806 - val_accuracy: 0.8173\n","Epoch 27/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4026 - accuracy: 0.8818 - val_loss: 0.5789 - val_accuracy: 0.8141\n","Epoch 28/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4386 - accuracy: 0.8675 - val_loss: 0.7228 - val_accuracy: 0.7596\n","Epoch 29/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3916 - accuracy: 0.8922 - val_loss: 0.5788 - val_accuracy: 0.8045\n","\n","Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 30/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3688 - accuracy: 0.8943 - val_loss: 0.5148 - val_accuracy: 0.8269\n","Epoch 31/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3563 - accuracy: 0.9016 - val_loss: 0.5188 - val_accuracy: 0.8397\n","Epoch 32/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3168 - accuracy: 0.9153 - val_loss: 0.5105 - val_accuracy: 0.8365\n","Epoch 33/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3382 - accuracy: 0.9069 - val_loss: 0.5070 - val_accuracy: 0.8141\n","Epoch 34/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3070 - accuracy: 0.9149 - val_loss: 0.4896 - val_accuracy: 0.8333\n","Epoch 35/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3237 - accuracy: 0.9017 - val_loss: 0.4917 - val_accuracy: 0.8397\n","Epoch 36/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2922 - accuracy: 0.9197 - val_loss: 0.4847 - val_accuracy: 0.8365\n","Epoch 37/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.3223 - accuracy: 0.9072 - val_loss: 0.4963 - val_accuracy: 0.8269\n","Epoch 38/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3112 - accuracy: 0.9091 - val_loss: 0.4728 - val_accuracy: 0.8301\n","Epoch 39/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2947 - accuracy: 0.9170 - val_loss: 0.4756 - val_accuracy: 0.8429\n","Epoch 40/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3110 - accuracy: 0.9153 - val_loss: 0.5207 - val_accuracy: 0.8301\n","Epoch 41/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2736 - accuracy: 0.9264 - val_loss: 0.4637 - val_accuracy: 0.8333\n","Epoch 42/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3085 - accuracy: 0.9123 - val_loss: 0.4820 - val_accuracy: 0.8333\n","Epoch 43/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3023 - accuracy: 0.9090 - val_loss: 0.5015 - val_accuracy: 0.8173\n","Epoch 44/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2599 - accuracy: 0.9330 - val_loss: 0.5222 - val_accuracy: 0.8205\n","Epoch 45/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2736 - accuracy: 0.9239 - val_loss: 0.4883 - val_accuracy: 0.8333\n","\n","Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 46/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2676 - accuracy: 0.9243 - val_loss: 0.4673 - val_accuracy: 0.8365\n","Epoch 47/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2443 - accuracy: 0.9329 - val_loss: 0.4640 - val_accuracy: 0.8333\n","Epoch 48/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2596 - accuracy: 0.9317 - val_loss: 0.4646 - val_accuracy: 0.8462\n","Epoch 49/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2552 - accuracy: 0.9312 - val_loss: 0.4595 - val_accuracy: 0.8397\n","Epoch 50/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2341 - accuracy: 0.9366 - val_loss: 0.4548 - val_accuracy: 0.8462\n","Epoch 51/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2232 - accuracy: 0.9417 - val_loss: 0.4715 - val_accuracy: 0.8397\n","Epoch 52/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2583 - accuracy: 0.9307 - val_loss: 0.4362 - val_accuracy: 0.8526\n","Epoch 53/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2514 - accuracy: 0.9325 - val_loss: 0.4845 - val_accuracy: 0.8462\n","Epoch 54/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2337 - accuracy: 0.9309 - val_loss: 0.4542 - val_accuracy: 0.8365\n","Epoch 55/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2154 - accuracy: 0.9473 - val_loss: 0.4463 - val_accuracy: 0.8365\n","Epoch 56/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2192 - accuracy: 0.9395 - val_loss: 0.4798 - val_accuracy: 0.8301\n","\n","Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 57/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2135 - accuracy: 0.9419 - val_loss: 0.4423 - val_accuracy: 0.8397\n","Epoch 58/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2221 - accuracy: 0.9475 - val_loss: 0.4448 - val_accuracy: 0.8494\n","Epoch 59/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2083 - accuracy: 0.9469 - val_loss: 0.4480 - val_accuracy: 0.8526\n","Epoch 60/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2170 - accuracy: 0.9429 - val_loss: 0.4481 - val_accuracy: 0.8429\n","\n","Epoch 00060: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","10/10 [==============================] - 0s 9ms/step - loss: 0.4362 - accuracy: 0.8526\n","10/10 [==============================] - 0s 8ms/step - loss: 0.4362 - accuracy: 0.8526\n","--------------------Fold_10--------------------\n","Epoch 1/100\n","44/44 [==============================] - 4s 58ms/step - loss: 3.4090 - accuracy: 0.3351 - val_loss: 2.9956 - val_accuracy: 0.3814\n","Epoch 2/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.9950 - accuracy: 0.5441 - val_loss: 1.8975 - val_accuracy: 0.5449\n","Epoch 3/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.6928 - accuracy: 0.5886 - val_loss: 1.8587 - val_accuracy: 0.5417\n","Epoch 4/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.5273 - accuracy: 0.6256 - val_loss: 1.6896 - val_accuracy: 0.5769\n","Epoch 5/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.3814 - accuracy: 0.6517 - val_loss: 1.5457 - val_accuracy: 0.6090\n","Epoch 6/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.3004 - accuracy: 0.6588 - val_loss: 1.3728 - val_accuracy: 0.6410\n","Epoch 7/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.1704 - accuracy: 0.6920 - val_loss: 1.2814 - val_accuracy: 0.6571\n","Epoch 8/100\n","44/44 [==============================] - 2s 50ms/step - loss: 1.0711 - accuracy: 0.7136 - val_loss: 1.1081 - val_accuracy: 0.7115\n","Epoch 9/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.0147 - accuracy: 0.7095 - val_loss: 1.0175 - val_accuracy: 0.7147\n","Epoch 10/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.8945 - accuracy: 0.7653 - val_loss: 0.9928 - val_accuracy: 0.7404\n","Epoch 11/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.8486 - accuracy: 0.7615 - val_loss: 0.8745 - val_accuracy: 0.7436\n","Epoch 12/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.7749 - accuracy: 0.7828 - val_loss: 0.8720 - val_accuracy: 0.7596\n","Epoch 13/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.7809 - accuracy: 0.7884 - val_loss: 0.8648 - val_accuracy: 0.7692\n","Epoch 14/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.7097 - accuracy: 0.8044 - val_loss: 0.8256 - val_accuracy: 0.7724\n","Epoch 15/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6766 - accuracy: 0.8114 - val_loss: 0.7567 - val_accuracy: 0.8045\n","Epoch 16/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.6623 - accuracy: 0.8230 - val_loss: 0.6992 - val_accuracy: 0.8077\n","Epoch 17/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6075 - accuracy: 0.8198 - val_loss: 0.6713 - val_accuracy: 0.8077\n","Epoch 18/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.5529 - accuracy: 0.8342 - val_loss: 0.6435 - val_accuracy: 0.8301\n","Epoch 19/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.5386 - accuracy: 0.8442 - val_loss: 0.6305 - val_accuracy: 0.8205\n","Epoch 20/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.5294 - accuracy: 0.8559 - val_loss: 0.6192 - val_accuracy: 0.8365\n","Epoch 21/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.5349 - accuracy: 0.8516 - val_loss: 0.7224 - val_accuracy: 0.7917\n","Epoch 22/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5139 - accuracy: 0.8496 - val_loss: 0.5714 - val_accuracy: 0.8429\n","Epoch 23/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.5074 - accuracy: 0.8525 - val_loss: 0.6128 - val_accuracy: 0.8173\n","Epoch 24/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4431 - accuracy: 0.8721 - val_loss: 0.7061 - val_accuracy: 0.7917\n","Epoch 25/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4597 - accuracy: 0.8635 - val_loss: 0.5298 - val_accuracy: 0.8109\n","Epoch 26/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4112 - accuracy: 0.8754 - val_loss: 0.6273 - val_accuracy: 0.8205\n","Epoch 27/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4389 - accuracy: 0.8669 - val_loss: 0.5502 - val_accuracy: 0.8237\n","Epoch 28/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4158 - accuracy: 0.8760 - val_loss: 0.5593 - val_accuracy: 0.8397\n","Epoch 29/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3959 - accuracy: 0.8820 - val_loss: 0.5460 - val_accuracy: 0.8301\n","\n","Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 30/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3623 - accuracy: 0.8941 - val_loss: 0.5067 - val_accuracy: 0.8365\n","Epoch 31/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3390 - accuracy: 0.9077 - val_loss: 0.4956 - val_accuracy: 0.8494\n","Epoch 32/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.3166 - accuracy: 0.9175 - val_loss: 0.5296 - val_accuracy: 0.8205\n","Epoch 33/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3262 - accuracy: 0.9024 - val_loss: 0.4833 - val_accuracy: 0.8333\n","Epoch 34/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.3428 - accuracy: 0.9054 - val_loss: 0.5381 - val_accuracy: 0.8109\n","Epoch 35/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3118 - accuracy: 0.9228 - val_loss: 0.5160 - val_accuracy: 0.8365\n","Epoch 36/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3033 - accuracy: 0.9241 - val_loss: 0.4947 - val_accuracy: 0.8397\n","Epoch 37/100\n","44/44 [==============================] - 2s 54ms/step - loss: 0.3215 - accuracy: 0.9126 - val_loss: 0.5072 - val_accuracy: 0.8365\n","\n","Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 38/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3001 - accuracy: 0.9239 - val_loss: 0.4691 - val_accuracy: 0.8558\n","Epoch 39/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2884 - accuracy: 0.9263 - val_loss: 0.4765 - val_accuracy: 0.8397\n","Epoch 40/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3101 - accuracy: 0.9170 - val_loss: 0.4484 - val_accuracy: 0.8494\n","Epoch 41/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2680 - accuracy: 0.9347 - val_loss: 0.4607 - val_accuracy: 0.8462\n","Epoch 42/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2813 - accuracy: 0.9248 - val_loss: 0.4764 - val_accuracy: 0.8429\n","Epoch 43/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2802 - accuracy: 0.9278 - val_loss: 0.4658 - val_accuracy: 0.8494\n","Epoch 44/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2947 - accuracy: 0.9229 - val_loss: 0.4642 - val_accuracy: 0.8462\n","\n","Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 45/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2654 - accuracy: 0.9328 - val_loss: 0.4507 - val_accuracy: 0.8558\n","Epoch 46/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2640 - accuracy: 0.9285 - val_loss: 0.4512 - val_accuracy: 0.8462\n","Epoch 47/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2645 - accuracy: 0.9350 - val_loss: 0.4381 - val_accuracy: 0.8494\n","Epoch 48/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2608 - accuracy: 0.9319 - val_loss: 0.4529 - val_accuracy: 0.8558\n","Epoch 49/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2513 - accuracy: 0.9340 - val_loss: 0.4458 - val_accuracy: 0.8494\n","Epoch 50/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2457 - accuracy: 0.9400 - val_loss: 0.4487 - val_accuracy: 0.8622\n","Epoch 51/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2589 - accuracy: 0.9367 - val_loss: 0.4644 - val_accuracy: 0.8590\n","\n","Epoch 00051: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","Epoch 52/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2613 - accuracy: 0.9386 - val_loss: 0.4493 - val_accuracy: 0.8558\n","Epoch 53/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2433 - accuracy: 0.9357 - val_loss: 0.4499 - val_accuracy: 0.8526\n","Epoch 54/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2524 - accuracy: 0.9331 - val_loss: 0.4413 - val_accuracy: 0.8590\n","Epoch 55/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2530 - accuracy: 0.9395 - val_loss: 0.4474 - val_accuracy: 0.8494\n","\n","Epoch 00055: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","10/10 [==============================] - 0s 12ms/step - loss: 0.4381 - accuracy: 0.8494\n","10/10 [==============================] - 0s 11ms/step - loss: 0.4381 - accuracy: 0.8494\n","\n","K-fold cross validation Auc: ['0.8530', '0.8498', '0.8690', '0.8307', '0.8594', '0.8397', '0.8558', '0.8654', '0.8526', '0.8494']\n","\n","K-fold cross validation loss: ['0.5377', '0.4763', '0.4191', '0.6070', '0.4358', '0.5846', '0.4560', '0.4250', '0.4362', '0.4381']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G6Yp7y9DRIj-"},"source":["##### 성능 확인 및 제출"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITlrPaW3ugnj","executionInfo":{"status":"ok","timestamp":1623224651454,"user_tz":-540,"elapsed":319,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"7e76efb4-a5ca-4f25-cc6f-f4996d376bce"},"source":["print(accuracy)\n","print()\n","print(losss)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["['0.8530', '0.8498', '0.8690', '0.8307', '0.8594', '0.8397', '0.8558', '0.8654', '0.8526', '0.8494']\n","\n","['0.5377', '0.4763', '0.4191', '0.6070', '0.4358', '0.5846', '0.4560', '0.4250', '0.4362', '0.4381']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AwYvUlwaRIj-","executionInfo":{"status":"ok","timestamp":1623224659463,"user_tz":-540,"elapsed":274,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"f688180f-9256-4dcd-83ed-9b224130d16b"},"source":["print(sum([float(i) for i in accuracy])/10)\n","print()\n","print(sum([float(i) for i in losss])/10)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["0.8524799999999999\n","\n","0.48158\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-b_mTFALRIj-","executionInfo":{"status":"ok","timestamp":1623224673247,"user_tz":-540,"elapsed":264,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"2f060051-733e-4433-c6c7-c1f6814fe58f"},"source":["test_X=np.array(test_sc.iloc[:,2:]).reshape(782, 600, -1)\n","test_X.shape"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(782, 600, 18)"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mdWapct-RIj_","executionInfo":{"status":"ok","timestamp":1623224702664,"user_tz":-540,"elapsed":4867,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"3c2582a9-5218-41af-c4af-5aa8f288215a"},"source":["preds = []\n","for model in models:\n","    pred = model.predict(test_X)\n","    preds.append(pred)\n","pred = np.mean(preds, axis=0)\n","pred"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[7.2470189e-06, 1.1533992e-06, 1.1257852e-07, ..., 3.9960123e-03,\n","        1.0988898e-05, 1.1997943e-06],\n","       [3.8169941e-04, 1.8457093e-05, 1.0390351e-04, ..., 5.0503504e-06,\n","        1.6737426e-05, 1.0686187e-05],\n","       [2.0089145e-03, 3.1839460e-02, 1.8515424e-05, ..., 7.8253011e-04,\n","        1.0812473e-02, 2.3926019e-03],\n","       ...,\n","       [4.1527214e-04, 3.0153449e-06, 1.0457467e-05, ..., 1.3977917e-05,\n","        1.4334830e-06, 7.2613382e-04],\n","       [3.2944565e-06, 7.9082727e-04, 8.0282553e-07, ..., 5.5139388e-08,\n","        1.1178543e-05, 3.0191931e-09],\n","       [9.0177113e-05, 3.9531028e-06, 1.0852897e-06, ..., 9.2336217e-05,\n","        8.7289544e-07, 1.5278123e-04]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":605},"id":"8OALM4SXRIj_","executionInfo":{"status":"ok","timestamp":1623224725474,"user_tz":-540,"elapsed":505,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"76da4417-c1de-45fb-909d-dc0b174db5a8"},"source":["submission=pd.read_csv('./sample_submission.csv')\n","submission.iloc[:,1:]=pred\n","submission"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>...</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>40</th>\n","      <th>41</th>\n","      <th>42</th>\n","      <th>43</th>\n","      <th>44</th>\n","      <th>45</th>\n","      <th>46</th>\n","      <th>47</th>\n","      <th>48</th>\n","      <th>49</th>\n","      <th>50</th>\n","      <th>51</th>\n","      <th>52</th>\n","      <th>53</th>\n","      <th>54</th>\n","      <th>55</th>\n","      <th>56</th>\n","      <th>57</th>\n","      <th>58</th>\n","      <th>59</th>\n","      <th>60</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3125</td>\n","      <td>0.000007</td>\n","      <td>0.000001</td>\n","      <td>1.125785e-07</td>\n","      <td>1.573663e-08</td>\n","      <td>1.645092e-04</td>\n","      <td>5.226560e-08</td>\n","      <td>2.407942e-04</td>\n","      <td>0.000002</td>\n","      <td>1.063794e-07</td>\n","      <td>0.004792</td>\n","      <td>9.160994e-02</td>\n","      <td>4.992557e-01</td>\n","      <td>9.998514e-05</td>\n","      <td>3.871534e-01</td>\n","      <td>1.771199e-03</td>\n","      <td>2.810795e-06</td>\n","      <td>1.328649e-06</td>\n","      <td>4.494327e-07</td>\n","      <td>2.155945e-07</td>\n","      <td>6.554843e-08</td>\n","      <td>2.606621e-05</td>\n","      <td>0.000003</td>\n","      <td>4.531218e-07</td>\n","      <td>0.004511</td>\n","      <td>...</td>\n","      <td>0.001019</td>\n","      <td>6.485892e-04</td>\n","      <td>5.894235e-05</td>\n","      <td>1.023497e-05</td>\n","      <td>1.787201e-07</td>\n","      <td>3.030667e-09</td>\n","      <td>0.000218</td>\n","      <td>0.000048</td>\n","      <td>2.816566e-05</td>\n","      <td>5.710984e-04</td>\n","      <td>3.238592e-05</td>\n","      <td>3.975732e-08</td>\n","      <td>2.001992e-06</td>\n","      <td>1.211115e-06</td>\n","      <td>1.602767e-08</td>\n","      <td>4.088182e-04</td>\n","      <td>4.474497e-04</td>\n","      <td>1.162772e-05</td>\n","      <td>1.544156e-06</td>\n","      <td>9.496107e-08</td>\n","      <td>9.604988e-08</td>\n","      <td>3.366253e-10</td>\n","      <td>3.996012e-03</td>\n","      <td>1.098890e-05</td>\n","      <td>1.199794e-06</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3126</td>\n","      <td>0.000382</td>\n","      <td>0.000018</td>\n","      <td>1.039035e-04</td>\n","      <td>1.047097e-03</td>\n","      <td>4.526797e-05</td>\n","      <td>5.437997e-04</td>\n","      <td>2.677999e-06</td>\n","      <td>0.000048</td>\n","      <td>6.923312e-06</td>\n","      <td>0.000009</td>\n","      <td>2.052716e-05</td>\n","      <td>1.203505e-06</td>\n","      <td>9.830789e-09</td>\n","      <td>1.402822e-06</td>\n","      <td>7.159186e-07</td>\n","      <td>2.764910e-04</td>\n","      <td>2.276689e-05</td>\n","      <td>9.619429e-06</td>\n","      <td>1.695045e-05</td>\n","      <td>3.189403e-06</td>\n","      <td>2.470777e-05</td>\n","      <td>0.000175</td>\n","      <td>1.531403e-03</td>\n","      <td>0.000192</td>\n","      <td>...</td>\n","      <td>0.000292</td>\n","      <td>3.530122e-06</td>\n","      <td>1.860230e-06</td>\n","      <td>2.340005e-08</td>\n","      <td>4.176740e-04</td>\n","      <td>3.140490e-04</td>\n","      <td>0.000010</td>\n","      <td>0.000005</td>\n","      <td>1.567733e-06</td>\n","      <td>3.204430e-06</td>\n","      <td>7.910401e-06</td>\n","      <td>1.487479e-05</td>\n","      <td>1.173921e-04</td>\n","      <td>2.126083e-03</td>\n","      <td>5.898821e-04</td>\n","      <td>6.466215e-06</td>\n","      <td>1.412719e-08</td>\n","      <td>7.406805e-05</td>\n","      <td>1.550944e-04</td>\n","      <td>1.032698e-04</td>\n","      <td>2.719077e-06</td>\n","      <td>1.398877e-04</td>\n","      <td>5.050350e-06</td>\n","      <td>1.673743e-05</td>\n","      <td>1.068619e-05</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3127</td>\n","      <td>0.002009</td>\n","      <td>0.031839</td>\n","      <td>1.851542e-05</td>\n","      <td>9.032035e-06</td>\n","      <td>3.370696e-05</td>\n","      <td>5.860966e-04</td>\n","      <td>1.110294e-01</td>\n","      <td>0.000136</td>\n","      <td>1.700231e-05</td>\n","      <td>0.000044</td>\n","      <td>1.836617e-03</td>\n","      <td>4.006464e-04</td>\n","      <td>3.046253e-05</td>\n","      <td>2.656796e-03</td>\n","      <td>1.984251e-02</td>\n","      <td>9.236598e-04</td>\n","      <td>1.582152e-04</td>\n","      <td>1.496239e-04</td>\n","      <td>4.579605e-05</td>\n","      <td>2.768337e-04</td>\n","      <td>2.299291e-04</td>\n","      <td>0.000286</td>\n","      <td>4.692571e-07</td>\n","      <td>0.000488</td>\n","      <td>...</td>\n","      <td>0.000444</td>\n","      <td>4.784974e-02</td>\n","      <td>5.777194e-03</td>\n","      <td>3.964584e-04</td>\n","      <td>2.270599e-04</td>\n","      <td>1.269780e-04</td>\n","      <td>0.057941</td>\n","      <td>0.194148</td>\n","      <td>7.234436e-03</td>\n","      <td>4.453482e-01</td>\n","      <td>2.230210e-04</td>\n","      <td>2.124737e-03</td>\n","      <td>2.350495e-03</td>\n","      <td>3.311582e-03</td>\n","      <td>2.403293e-06</td>\n","      <td>3.948251e-05</td>\n","      <td>2.229922e-05</td>\n","      <td>1.372128e-06</td>\n","      <td>9.597297e-04</td>\n","      <td>9.875500e-07</td>\n","      <td>2.872108e-03</td>\n","      <td>6.034688e-08</td>\n","      <td>7.825301e-04</td>\n","      <td>1.081247e-02</td>\n","      <td>2.392602e-03</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3128</td>\n","      <td>0.000650</td>\n","      <td>0.000008</td>\n","      <td>2.718153e-05</td>\n","      <td>8.027258e-05</td>\n","      <td>1.686308e-05</td>\n","      <td>1.196244e-04</td>\n","      <td>4.666803e-06</td>\n","      <td>0.000095</td>\n","      <td>1.386064e-03</td>\n","      <td>0.000056</td>\n","      <td>1.643861e-04</td>\n","      <td>1.078128e-04</td>\n","      <td>2.473066e-06</td>\n","      <td>2.186282e-06</td>\n","      <td>1.124039e-05</td>\n","      <td>6.361053e-04</td>\n","      <td>1.277015e-06</td>\n","      <td>3.361098e-06</td>\n","      <td>7.678640e-06</td>\n","      <td>6.616545e-06</td>\n","      <td>4.754822e-06</td>\n","      <td>0.000082</td>\n","      <td>8.013546e-05</td>\n","      <td>0.000126</td>\n","      <td>...</td>\n","      <td>0.000278</td>\n","      <td>2.341399e-06</td>\n","      <td>3.371463e-05</td>\n","      <td>1.335878e-06</td>\n","      <td>1.222249e-05</td>\n","      <td>1.231583e-05</td>\n","      <td>0.000010</td>\n","      <td>0.000019</td>\n","      <td>2.256918e-05</td>\n","      <td>3.594953e-05</td>\n","      <td>9.336409e-06</td>\n","      <td>4.902353e-05</td>\n","      <td>3.511614e-03</td>\n","      <td>7.693263e-03</td>\n","      <td>6.089889e-04</td>\n","      <td>1.591863e-04</td>\n","      <td>4.910899e-06</td>\n","      <td>2.359450e-05</td>\n","      <td>9.636163e-05</td>\n","      <td>2.678980e-05</td>\n","      <td>2.221527e-06</td>\n","      <td>4.800652e-05</td>\n","      <td>2.119675e-05</td>\n","      <td>4.790917e-06</td>\n","      <td>6.638248e-03</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3129</td>\n","      <td>0.004836</td>\n","      <td>0.000037</td>\n","      <td>2.668857e-06</td>\n","      <td>3.178180e-04</td>\n","      <td>2.960055e-04</td>\n","      <td>2.491190e-05</td>\n","      <td>9.227766e-07</td>\n","      <td>0.000004</td>\n","      <td>2.734654e-05</td>\n","      <td>0.000089</td>\n","      <td>2.491495e-05</td>\n","      <td>6.642930e-06</td>\n","      <td>1.215390e-06</td>\n","      <td>3.307329e-07</td>\n","      <td>1.620924e-05</td>\n","      <td>4.636788e-04</td>\n","      <td>1.061642e-05</td>\n","      <td>3.308251e-06</td>\n","      <td>5.656105e-05</td>\n","      <td>2.357707e-06</td>\n","      <td>4.282958e-06</td>\n","      <td>0.000008</td>\n","      <td>3.459562e-04</td>\n","      <td>0.000008</td>\n","      <td>...</td>\n","      <td>0.000020</td>\n","      <td>4.249396e-07</td>\n","      <td>1.029824e-05</td>\n","      <td>6.092514e-06</td>\n","      <td>7.385822e-07</td>\n","      <td>5.948510e-06</td>\n","      <td>0.000083</td>\n","      <td>0.000043</td>\n","      <td>9.106713e-08</td>\n","      <td>1.106624e-06</td>\n","      <td>3.600655e-06</td>\n","      <td>1.121859e-06</td>\n","      <td>3.631009e-04</td>\n","      <td>1.091350e-03</td>\n","      <td>6.779860e-04</td>\n","      <td>5.684753e-06</td>\n","      <td>7.777033e-07</td>\n","      <td>1.604089e-05</td>\n","      <td>2.560084e-06</td>\n","      <td>3.139246e-06</td>\n","      <td>5.109708e-07</td>\n","      <td>2.355495e-03</td>\n","      <td>7.735231e-05</td>\n","      <td>2.669615e-06</td>\n","      <td>4.266609e-03</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>777</th>\n","      <td>3902</td>\n","      <td>0.007624</td>\n","      <td>0.000048</td>\n","      <td>2.691176e-07</td>\n","      <td>1.313805e-04</td>\n","      <td>3.405496e-04</td>\n","      <td>3.641476e-05</td>\n","      <td>2.551924e-07</td>\n","      <td>0.000005</td>\n","      <td>5.048125e-05</td>\n","      <td>0.000009</td>\n","      <td>5.803399e-06</td>\n","      <td>5.316843e-07</td>\n","      <td>7.761487e-08</td>\n","      <td>4.214978e-08</td>\n","      <td>2.936631e-06</td>\n","      <td>5.512687e-04</td>\n","      <td>1.926939e-05</td>\n","      <td>5.040561e-06</td>\n","      <td>1.276073e-04</td>\n","      <td>1.653727e-06</td>\n","      <td>9.372082e-07</td>\n","      <td>0.000002</td>\n","      <td>2.247843e-04</td>\n","      <td>0.000001</td>\n","      <td>...</td>\n","      <td>0.000005</td>\n","      <td>1.067801e-07</td>\n","      <td>1.628949e-06</td>\n","      <td>2.425142e-07</td>\n","      <td>1.546383e-06</td>\n","      <td>1.041205e-05</td>\n","      <td>0.000014</td>\n","      <td>0.000004</td>\n","      <td>1.010540e-07</td>\n","      <td>5.672574e-07</td>\n","      <td>2.123301e-06</td>\n","      <td>7.995540e-07</td>\n","      <td>2.088641e-04</td>\n","      <td>3.425491e-04</td>\n","      <td>1.631873e-03</td>\n","      <td>2.068542e-07</td>\n","      <td>2.944445e-08</td>\n","      <td>2.605800e-06</td>\n","      <td>5.208200e-07</td>\n","      <td>2.202719e-07</td>\n","      <td>2.501272e-07</td>\n","      <td>1.355841e-02</td>\n","      <td>1.862887e-05</td>\n","      <td>1.966490e-06</td>\n","      <td>5.888015e-03</td>\n","    </tr>\n","    <tr>\n","      <th>778</th>\n","      <td>3903</td>\n","      <td>0.000087</td>\n","      <td>0.000005</td>\n","      <td>1.406178e-06</td>\n","      <td>1.657091e-04</td>\n","      <td>5.535872e-05</td>\n","      <td>2.469908e-05</td>\n","      <td>1.952268e-08</td>\n","      <td>0.000002</td>\n","      <td>6.343804e-07</td>\n","      <td>0.000001</td>\n","      <td>6.771439e-07</td>\n","      <td>1.079770e-07</td>\n","      <td>5.757150e-09</td>\n","      <td>4.293053e-08</td>\n","      <td>7.466963e-08</td>\n","      <td>7.372438e-05</td>\n","      <td>2.382649e-05</td>\n","      <td>1.911262e-06</td>\n","      <td>8.382199e-05</td>\n","      <td>6.165108e-07</td>\n","      <td>1.288446e-06</td>\n","      <td>0.000002</td>\n","      <td>6.077067e-04</td>\n","      <td>0.000005</td>\n","      <td>...</td>\n","      <td>0.000002</td>\n","      <td>3.562483e-08</td>\n","      <td>7.287049e-07</td>\n","      <td>6.790546e-09</td>\n","      <td>8.160891e-06</td>\n","      <td>2.134513e-05</td>\n","      <td>0.000009</td>\n","      <td>0.000003</td>\n","      <td>2.201405e-08</td>\n","      <td>2.310623e-07</td>\n","      <td>8.487663e-07</td>\n","      <td>1.008175e-07</td>\n","      <td>1.258439e-05</td>\n","      <td>7.633474e-05</td>\n","      <td>5.450140e-05</td>\n","      <td>1.698040e-07</td>\n","      <td>2.775441e-09</td>\n","      <td>6.455104e-06</td>\n","      <td>7.352405e-07</td>\n","      <td>8.658537e-07</td>\n","      <td>2.048461e-07</td>\n","      <td>2.873587e-03</td>\n","      <td>2.768043e-06</td>\n","      <td>3.997831e-07</td>\n","      <td>2.875478e-05</td>\n","    </tr>\n","    <tr>\n","      <th>779</th>\n","      <td>3904</td>\n","      <td>0.000415</td>\n","      <td>0.000003</td>\n","      <td>1.045747e-05</td>\n","      <td>8.264054e-05</td>\n","      <td>1.747382e-05</td>\n","      <td>9.405917e-05</td>\n","      <td>1.184736e-06</td>\n","      <td>0.000017</td>\n","      <td>2.884763e-05</td>\n","      <td>0.000024</td>\n","      <td>3.804064e-05</td>\n","      <td>7.754567e-06</td>\n","      <td>2.682793e-07</td>\n","      <td>4.793420e-07</td>\n","      <td>7.617256e-06</td>\n","      <td>1.690295e-04</td>\n","      <td>6.984290e-07</td>\n","      <td>4.834840e-07</td>\n","      <td>2.764723e-06</td>\n","      <td>7.750986e-07</td>\n","      <td>5.288783e-06</td>\n","      <td>0.000043</td>\n","      <td>1.485167e-04</td>\n","      <td>0.000033</td>\n","      <td>...</td>\n","      <td>0.000127</td>\n","      <td>4.078461e-07</td>\n","      <td>3.075986e-06</td>\n","      <td>4.786186e-07</td>\n","      <td>2.674543e-06</td>\n","      <td>5.078318e-06</td>\n","      <td>0.000008</td>\n","      <td>0.000011</td>\n","      <td>9.218687e-07</td>\n","      <td>3.833084e-06</td>\n","      <td>1.409927e-06</td>\n","      <td>8.047994e-06</td>\n","      <td>4.917436e-04</td>\n","      <td>2.678306e-03</td>\n","      <td>2.005461e-04</td>\n","      <td>1.138869e-05</td>\n","      <td>2.977498e-07</td>\n","      <td>1.132421e-05</td>\n","      <td>1.963683e-05</td>\n","      <td>1.184793e-05</td>\n","      <td>3.595622e-07</td>\n","      <td>6.003856e-05</td>\n","      <td>1.397792e-05</td>\n","      <td>1.433483e-06</td>\n","      <td>7.261338e-04</td>\n","    </tr>\n","    <tr>\n","      <th>780</th>\n","      <td>3905</td>\n","      <td>0.000003</td>\n","      <td>0.000791</td>\n","      <td>8.028255e-07</td>\n","      <td>9.681497e-10</td>\n","      <td>9.667219e-09</td>\n","      <td>2.100891e-08</td>\n","      <td>1.543012e-02</td>\n","      <td>0.000002</td>\n","      <td>1.452740e-08</td>\n","      <td>0.000002</td>\n","      <td>1.196007e-06</td>\n","      <td>1.254677e-05</td>\n","      <td>2.471249e-09</td>\n","      <td>2.116609e-04</td>\n","      <td>5.956566e-08</td>\n","      <td>3.576505e-07</td>\n","      <td>1.288919e-05</td>\n","      <td>3.248188e-07</td>\n","      <td>8.096649e-10</td>\n","      <td>2.435592e-08</td>\n","      <td>3.698794e-07</td>\n","      <td>0.000003</td>\n","      <td>3.423148e-10</td>\n","      <td>0.000006</td>\n","      <td>...</td>\n","      <td>0.000006</td>\n","      <td>9.826911e-01</td>\n","      <td>1.333122e-06</td>\n","      <td>2.689445e-07</td>\n","      <td>1.085280e-05</td>\n","      <td>2.299107e-08</td>\n","      <td>0.000004</td>\n","      <td>0.000007</td>\n","      <td>2.582577e-05</td>\n","      <td>2.821627e-04</td>\n","      <td>2.462788e-07</td>\n","      <td>3.822813e-04</td>\n","      <td>4.396119e-07</td>\n","      <td>6.803999e-07</td>\n","      <td>2.407390e-10</td>\n","      <td>3.880878e-07</td>\n","      <td>5.003605e-09</td>\n","      <td>4.994854e-09</td>\n","      <td>1.869232e-05</td>\n","      <td>6.196854e-08</td>\n","      <td>1.577206e-06</td>\n","      <td>2.640714e-12</td>\n","      <td>5.513939e-08</td>\n","      <td>1.117854e-05</td>\n","      <td>3.019193e-09</td>\n","    </tr>\n","    <tr>\n","      <th>781</th>\n","      <td>3906</td>\n","      <td>0.000090</td>\n","      <td>0.000004</td>\n","      <td>1.085290e-06</td>\n","      <td>1.329757e-04</td>\n","      <td>3.341439e-05</td>\n","      <td>2.746793e-05</td>\n","      <td>9.732891e-08</td>\n","      <td>0.000002</td>\n","      <td>2.651907e-07</td>\n","      <td>0.000002</td>\n","      <td>1.047003e-05</td>\n","      <td>3.329918e-07</td>\n","      <td>6.183820e-08</td>\n","      <td>1.537224e-07</td>\n","      <td>4.065555e-06</td>\n","      <td>3.989738e-05</td>\n","      <td>8.006275e-06</td>\n","      <td>7.437868e-07</td>\n","      <td>1.291412e-05</td>\n","      <td>3.938566e-07</td>\n","      <td>7.455957e-07</td>\n","      <td>0.000001</td>\n","      <td>7.176952e-04</td>\n","      <td>0.000015</td>\n","      <td>...</td>\n","      <td>0.000005</td>\n","      <td>7.259875e-08</td>\n","      <td>2.992330e-06</td>\n","      <td>1.934879e-08</td>\n","      <td>4.660028e-06</td>\n","      <td>2.178445e-05</td>\n","      <td>0.000060</td>\n","      <td>0.000013</td>\n","      <td>8.764854e-08</td>\n","      <td>7.809778e-07</td>\n","      <td>1.109434e-06</td>\n","      <td>1.418796e-07</td>\n","      <td>2.535175e-05</td>\n","      <td>1.465254e-04</td>\n","      <td>8.567573e-05</td>\n","      <td>1.388314e-07</td>\n","      <td>1.598963e-08</td>\n","      <td>2.892452e-06</td>\n","      <td>1.021607e-06</td>\n","      <td>3.306447e-07</td>\n","      <td>9.022536e-08</td>\n","      <td>2.187958e-03</td>\n","      <td>9.233622e-05</td>\n","      <td>8.728954e-07</td>\n","      <td>1.527812e-04</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>782 rows × 62 columns</p>\n","</div>"],"text/plain":["       id         0         1             2             3             4  \\\n","0    3125  0.000007  0.000001  1.125785e-07  1.573663e-08  1.645092e-04   \n","1    3126  0.000382  0.000018  1.039035e-04  1.047097e-03  4.526797e-05   \n","2    3127  0.002009  0.031839  1.851542e-05  9.032035e-06  3.370696e-05   \n","3    3128  0.000650  0.000008  2.718153e-05  8.027258e-05  1.686308e-05   \n","4    3129  0.004836  0.000037  2.668857e-06  3.178180e-04  2.960055e-04   \n","..    ...       ...       ...           ...           ...           ...   \n","777  3902  0.007624  0.000048  2.691176e-07  1.313805e-04  3.405496e-04   \n","778  3903  0.000087  0.000005  1.406178e-06  1.657091e-04  5.535872e-05   \n","779  3904  0.000415  0.000003  1.045747e-05  8.264054e-05  1.747382e-05   \n","780  3905  0.000003  0.000791  8.028255e-07  9.681497e-10  9.667219e-09   \n","781  3906  0.000090  0.000004  1.085290e-06  1.329757e-04  3.341439e-05   \n","\n","                5             6         7             8         9  \\\n","0    5.226560e-08  2.407942e-04  0.000002  1.063794e-07  0.004792   \n","1    5.437997e-04  2.677999e-06  0.000048  6.923312e-06  0.000009   \n","2    5.860966e-04  1.110294e-01  0.000136  1.700231e-05  0.000044   \n","3    1.196244e-04  4.666803e-06  0.000095  1.386064e-03  0.000056   \n","4    2.491190e-05  9.227766e-07  0.000004  2.734654e-05  0.000089   \n","..            ...           ...       ...           ...       ...   \n","777  3.641476e-05  2.551924e-07  0.000005  5.048125e-05  0.000009   \n","778  2.469908e-05  1.952268e-08  0.000002  6.343804e-07  0.000001   \n","779  9.405917e-05  1.184736e-06  0.000017  2.884763e-05  0.000024   \n","780  2.100891e-08  1.543012e-02  0.000002  1.452740e-08  0.000002   \n","781  2.746793e-05  9.732891e-08  0.000002  2.651907e-07  0.000002   \n","\n","               10            11            12            13            14  \\\n","0    9.160994e-02  4.992557e-01  9.998514e-05  3.871534e-01  1.771199e-03   \n","1    2.052716e-05  1.203505e-06  9.830789e-09  1.402822e-06  7.159186e-07   \n","2    1.836617e-03  4.006464e-04  3.046253e-05  2.656796e-03  1.984251e-02   \n","3    1.643861e-04  1.078128e-04  2.473066e-06  2.186282e-06  1.124039e-05   \n","4    2.491495e-05  6.642930e-06  1.215390e-06  3.307329e-07  1.620924e-05   \n","..            ...           ...           ...           ...           ...   \n","777  5.803399e-06  5.316843e-07  7.761487e-08  4.214978e-08  2.936631e-06   \n","778  6.771439e-07  1.079770e-07  5.757150e-09  4.293053e-08  7.466963e-08   \n","779  3.804064e-05  7.754567e-06  2.682793e-07  4.793420e-07  7.617256e-06   \n","780  1.196007e-06  1.254677e-05  2.471249e-09  2.116609e-04  5.956566e-08   \n","781  1.047003e-05  3.329918e-07  6.183820e-08  1.537224e-07  4.065555e-06   \n","\n","               15            16            17            18            19  \\\n","0    2.810795e-06  1.328649e-06  4.494327e-07  2.155945e-07  6.554843e-08   \n","1    2.764910e-04  2.276689e-05  9.619429e-06  1.695045e-05  3.189403e-06   \n","2    9.236598e-04  1.582152e-04  1.496239e-04  4.579605e-05  2.768337e-04   \n","3    6.361053e-04  1.277015e-06  3.361098e-06  7.678640e-06  6.616545e-06   \n","4    4.636788e-04  1.061642e-05  3.308251e-06  5.656105e-05  2.357707e-06   \n","..            ...           ...           ...           ...           ...   \n","777  5.512687e-04  1.926939e-05  5.040561e-06  1.276073e-04  1.653727e-06   \n","778  7.372438e-05  2.382649e-05  1.911262e-06  8.382199e-05  6.165108e-07   \n","779  1.690295e-04  6.984290e-07  4.834840e-07  2.764723e-06  7.750986e-07   \n","780  3.576505e-07  1.288919e-05  3.248188e-07  8.096649e-10  2.435592e-08   \n","781  3.989738e-05  8.006275e-06  7.437868e-07  1.291412e-05  3.938566e-07   \n","\n","               20        21            22        23  ...        36  \\\n","0    2.606621e-05  0.000003  4.531218e-07  0.004511  ...  0.001019   \n","1    2.470777e-05  0.000175  1.531403e-03  0.000192  ...  0.000292   \n","2    2.299291e-04  0.000286  4.692571e-07  0.000488  ...  0.000444   \n","3    4.754822e-06  0.000082  8.013546e-05  0.000126  ...  0.000278   \n","4    4.282958e-06  0.000008  3.459562e-04  0.000008  ...  0.000020   \n","..            ...       ...           ...       ...  ...       ...   \n","777  9.372082e-07  0.000002  2.247843e-04  0.000001  ...  0.000005   \n","778  1.288446e-06  0.000002  6.077067e-04  0.000005  ...  0.000002   \n","779  5.288783e-06  0.000043  1.485167e-04  0.000033  ...  0.000127   \n","780  3.698794e-07  0.000003  3.423148e-10  0.000006  ...  0.000006   \n","781  7.455957e-07  0.000001  7.176952e-04  0.000015  ...  0.000005   \n","\n","               37            38            39            40            41  \\\n","0    6.485892e-04  5.894235e-05  1.023497e-05  1.787201e-07  3.030667e-09   \n","1    3.530122e-06  1.860230e-06  2.340005e-08  4.176740e-04  3.140490e-04   \n","2    4.784974e-02  5.777194e-03  3.964584e-04  2.270599e-04  1.269780e-04   \n","3    2.341399e-06  3.371463e-05  1.335878e-06  1.222249e-05  1.231583e-05   \n","4    4.249396e-07  1.029824e-05  6.092514e-06  7.385822e-07  5.948510e-06   \n","..            ...           ...           ...           ...           ...   \n","777  1.067801e-07  1.628949e-06  2.425142e-07  1.546383e-06  1.041205e-05   \n","778  3.562483e-08  7.287049e-07  6.790546e-09  8.160891e-06  2.134513e-05   \n","779  4.078461e-07  3.075986e-06  4.786186e-07  2.674543e-06  5.078318e-06   \n","780  9.826911e-01  1.333122e-06  2.689445e-07  1.085280e-05  2.299107e-08   \n","781  7.259875e-08  2.992330e-06  1.934879e-08  4.660028e-06  2.178445e-05   \n","\n","           42        43            44            45            46  \\\n","0    0.000218  0.000048  2.816566e-05  5.710984e-04  3.238592e-05   \n","1    0.000010  0.000005  1.567733e-06  3.204430e-06  7.910401e-06   \n","2    0.057941  0.194148  7.234436e-03  4.453482e-01  2.230210e-04   \n","3    0.000010  0.000019  2.256918e-05  3.594953e-05  9.336409e-06   \n","4    0.000083  0.000043  9.106713e-08  1.106624e-06  3.600655e-06   \n","..        ...       ...           ...           ...           ...   \n","777  0.000014  0.000004  1.010540e-07  5.672574e-07  2.123301e-06   \n","778  0.000009  0.000003  2.201405e-08  2.310623e-07  8.487663e-07   \n","779  0.000008  0.000011  9.218687e-07  3.833084e-06  1.409927e-06   \n","780  0.000004  0.000007  2.582577e-05  2.821627e-04  2.462788e-07   \n","781  0.000060  0.000013  8.764854e-08  7.809778e-07  1.109434e-06   \n","\n","               47            48            49            50            51  \\\n","0    3.975732e-08  2.001992e-06  1.211115e-06  1.602767e-08  4.088182e-04   \n","1    1.487479e-05  1.173921e-04  2.126083e-03  5.898821e-04  6.466215e-06   \n","2    2.124737e-03  2.350495e-03  3.311582e-03  2.403293e-06  3.948251e-05   \n","3    4.902353e-05  3.511614e-03  7.693263e-03  6.089889e-04  1.591863e-04   \n","4    1.121859e-06  3.631009e-04  1.091350e-03  6.779860e-04  5.684753e-06   \n","..            ...           ...           ...           ...           ...   \n","777  7.995540e-07  2.088641e-04  3.425491e-04  1.631873e-03  2.068542e-07   \n","778  1.008175e-07  1.258439e-05  7.633474e-05  5.450140e-05  1.698040e-07   \n","779  8.047994e-06  4.917436e-04  2.678306e-03  2.005461e-04  1.138869e-05   \n","780  3.822813e-04  4.396119e-07  6.803999e-07  2.407390e-10  3.880878e-07   \n","781  1.418796e-07  2.535175e-05  1.465254e-04  8.567573e-05  1.388314e-07   \n","\n","               52            53            54            55            56  \\\n","0    4.474497e-04  1.162772e-05  1.544156e-06  9.496107e-08  9.604988e-08   \n","1    1.412719e-08  7.406805e-05  1.550944e-04  1.032698e-04  2.719077e-06   \n","2    2.229922e-05  1.372128e-06  9.597297e-04  9.875500e-07  2.872108e-03   \n","3    4.910899e-06  2.359450e-05  9.636163e-05  2.678980e-05  2.221527e-06   \n","4    7.777033e-07  1.604089e-05  2.560084e-06  3.139246e-06  5.109708e-07   \n","..            ...           ...           ...           ...           ...   \n","777  2.944445e-08  2.605800e-06  5.208200e-07  2.202719e-07  2.501272e-07   \n","778  2.775441e-09  6.455104e-06  7.352405e-07  8.658537e-07  2.048461e-07   \n","779  2.977498e-07  1.132421e-05  1.963683e-05  1.184793e-05  3.595622e-07   \n","780  5.003605e-09  4.994854e-09  1.869232e-05  6.196854e-08  1.577206e-06   \n","781  1.598963e-08  2.892452e-06  1.021607e-06  3.306447e-07  9.022536e-08   \n","\n","               57            58            59            60  \n","0    3.366253e-10  3.996012e-03  1.098890e-05  1.199794e-06  \n","1    1.398877e-04  5.050350e-06  1.673743e-05  1.068619e-05  \n","2    6.034688e-08  7.825301e-04  1.081247e-02  2.392602e-03  \n","3    4.800652e-05  2.119675e-05  4.790917e-06  6.638248e-03  \n","4    2.355495e-03  7.735231e-05  2.669615e-06  4.266609e-03  \n","..            ...           ...           ...           ...  \n","777  1.355841e-02  1.862887e-05  1.966490e-06  5.888015e-03  \n","778  2.873587e-03  2.768043e-06  3.997831e-07  2.875478e-05  \n","779  6.003856e-05  1.397792e-05  1.433483e-06  7.261338e-04  \n","780  2.640714e-12  5.513939e-08  1.117854e-05  3.019193e-09  \n","781  2.187958e-03  9.233622e-05  8.728954e-07  1.527812e-04  \n","\n","[782 rows x 62 columns]"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"9TpxR538RIj_","executionInfo":{"status":"ok","timestamp":1623224741387,"user_tz":-540,"elapsed":275,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["submission.to_csv('sub_kfold_stratified_10_adam_fft_0.5.csv',index=False)"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"um5jH_DhRIkA"},"source":[""],"execution_count":null,"outputs":[]}]}