{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Private_2위 _ Public -0.43308 _ 1D-CNN.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C4998E9lRQgp","executionInfo":{"status":"ok","timestamp":1623818137850,"user_tz":-540,"elapsed":31477,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"cbe5406a-a95b-4761-d7e1-31e9e1cf04dc"},"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive/DATA')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YYpf3sAqRIjv","executionInfo":{"status":"ok","timestamp":1623818148175,"user_tz":-540,"elapsed":302,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy import signal\n","from tqdm import tqdm\n","from numpy.fft import fft, fftshift\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import random\n","import warnings\n","warnings.filterwarnings(action='ignore')\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Ilop2aVRIjz"},"source":["#### Data loading"]},{"cell_type":"code","metadata":{"id":"nUTH0kRaRIjz","executionInfo":{"status":"ok","timestamp":1623818160072,"user_tz":-540,"elapsed":9809,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["train=pd.read_csv('./train_features.csv')\n","train_labels=pd.read_csv('./train_labels.csv')\n","test=pd.read_csv('./0616Lee.csv')\n","\n","submission=pd.read_csv('./sample_submission_0616Lee.csv')\n","\n","pd.options.display.max_columns=50"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"iJ9jnkT7Ws_U","executionInfo":{"status":"ok","timestamp":1623818162503,"user_tz":-540,"elapsed":295,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"ffed7a1a-fc87-4e7c-f247-0101eeb68027"},"source":["test = test.drop('exercise',axis=1)\n","test.rename(columns = {'Time' : 'time'}, inplace = True)\n","test"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>time</th>\n","      <th>acc_x</th>\n","      <th>acc_y</th>\n","      <th>acc_z</th>\n","      <th>gy_x</th>\n","      <th>gy_y</th>\n","      <th>gy_z</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2021-06-15 10:43:34:05505</td>\n","      <td>0.886565</td>\n","      <td>-0.693919</td>\n","      <td>0.003907</td>\n","      <td>-30.66</td>\n","      <td>-40.60</td>\n","      <td>-43.75</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>2021-06-15 10:43:34:4155</td>\n","      <td>0.962501</td>\n","      <td>0.129896</td>\n","      <td>-0.198751</td>\n","      <td>-1.05</td>\n","      <td>17.36</td>\n","      <td>27.72</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>2021-06-15 10:43:34:43266</td>\n","      <td>0.971779</td>\n","      <td>0.098155</td>\n","      <td>-0.140151</td>\n","      <td>24.08</td>\n","      <td>25.55</td>\n","      <td>39.20</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>2021-06-15 10:43:34:45299</td>\n","      <td>1.077747</td>\n","      <td>-0.172625</td>\n","      <td>-0.000488</td>\n","      <td>0.56</td>\n","      <td>40.81</td>\n","      <td>43.19</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>2021-06-15 10:43:34:47443</td>\n","      <td>0.991801</td>\n","      <td>-0.340367</td>\n","      <td>-0.140884</td>\n","      <td>-33.60</td>\n","      <td>32.90</td>\n","      <td>42.98</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>45595</th>\n","      <td>75</td>\n","      <td>2021-06-15 12:54:27:86189</td>\n","      <td>0.833581</td>\n","      <td>-0.556209</td>\n","      <td>-0.130384</td>\n","      <td>-6.72</td>\n","      <td>4.55</td>\n","      <td>1.61</td>\n","    </tr>\n","    <tr>\n","      <th>45596</th>\n","      <td>75</td>\n","      <td>2021-06-15 12:54:27:87987</td>\n","      <td>0.825280</td>\n","      <td>-0.566220</td>\n","      <td>-0.110119</td>\n","      <td>-4.41</td>\n","      <td>3.50</td>\n","      <td>1.12</td>\n","    </tr>\n","    <tr>\n","      <th>45597</th>\n","      <td>75</td>\n","      <td>2021-06-15 12:54:27:90266</td>\n","      <td>0.804037</td>\n","      <td>-0.575742</td>\n","      <td>-0.105235</td>\n","      <td>-2.10</td>\n","      <td>2.80</td>\n","      <td>0.98</td>\n","    </tr>\n","    <tr>\n","      <th>45598</th>\n","      <td>75</td>\n","      <td>2021-06-15 12:54:27:91982</td>\n","      <td>0.808188</td>\n","      <td>-0.582091</td>\n","      <td>-0.107433</td>\n","      <td>-0.98</td>\n","      <td>2.87</td>\n","      <td>0.91</td>\n","    </tr>\n","    <tr>\n","      <th>45599</th>\n","      <td>75</td>\n","      <td>2021-06-15 12:54:27:94159</td>\n","      <td>0.819176</td>\n","      <td>-0.573301</td>\n","      <td>-0.127699</td>\n","      <td>-1.33</td>\n","      <td>2.52</td>\n","      <td>1.05</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>45600 rows × 8 columns</p>\n","</div>"],"text/plain":["       id                       time     acc_x     acc_y     acc_z   gy_x  \\\n","0       0  2021-06-15 10:43:34:05505  0.886565 -0.693919  0.003907 -30.66   \n","1       0   2021-06-15 10:43:34:4155  0.962501  0.129896 -0.198751  -1.05   \n","2       0  2021-06-15 10:43:34:43266  0.971779  0.098155 -0.140151  24.08   \n","3       0  2021-06-15 10:43:34:45299  1.077747 -0.172625 -0.000488   0.56   \n","4       0  2021-06-15 10:43:34:47443  0.991801 -0.340367 -0.140884 -33.60   \n","...    ..                        ...       ...       ...       ...    ...   \n","45595  75  2021-06-15 12:54:27:86189  0.833581 -0.556209 -0.130384  -6.72   \n","45596  75  2021-06-15 12:54:27:87987  0.825280 -0.566220 -0.110119  -4.41   \n","45597  75  2021-06-15 12:54:27:90266  0.804037 -0.575742 -0.105235  -2.10   \n","45598  75  2021-06-15 12:54:27:91982  0.808188 -0.582091 -0.107433  -0.98   \n","45599  75  2021-06-15 12:54:27:94159  0.819176 -0.573301 -0.127699  -1.33   \n","\n","        gy_y   gy_z  \n","0     -40.60 -43.75  \n","1      17.36  27.72  \n","2      25.55  39.20  \n","3      40.81  43.19  \n","4      32.90  42.98  \n","...      ...    ...  \n","45595   4.55   1.61  \n","45596   3.50   1.12  \n","45597   2.80   0.98  \n","45598   2.87   0.91  \n","45599   2.52   1.05  \n","\n","[45600 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"AgLoSBRQ2npq","executionInfo":{"status":"ok","timestamp":1623818172771,"user_tz":-540,"elapsed":306,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"48d7de4c-8e03-46fa-8a6c-fc7c4cb8b825"},"source":["train"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>time</th>\n","      <th>acc_x</th>\n","      <th>acc_y</th>\n","      <th>acc_z</th>\n","      <th>gy_x</th>\n","      <th>gy_y</th>\n","      <th>gy_z</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.206087</td>\n","      <td>-0.179371</td>\n","      <td>-0.148447</td>\n","      <td>-0.591608</td>\n","      <td>-30.549010</td>\n","      <td>-31.676112</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1.287696</td>\n","      <td>-0.198974</td>\n","      <td>-0.182444</td>\n","      <td>0.303100</td>\n","      <td>-39.139103</td>\n","      <td>-24.927216</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1.304609</td>\n","      <td>-0.195114</td>\n","      <td>-0.253382</td>\n","      <td>-3.617278</td>\n","      <td>-44.122565</td>\n","      <td>-25.019629</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1.293095</td>\n","      <td>-0.230366</td>\n","      <td>-0.215210</td>\n","      <td>2.712986</td>\n","      <td>-53.597843</td>\n","      <td>-27.454013</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1.300887</td>\n","      <td>-0.187757</td>\n","      <td>-0.222523</td>\n","      <td>4.286707</td>\n","      <td>-57.906561</td>\n","      <td>-27.961234</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1874995</th>\n","      <td>3124</td>\n","      <td>595</td>\n","      <td>-0.712530</td>\n","      <td>-0.658357</td>\n","      <td>0.293707</td>\n","      <td>-29.367857</td>\n","      <td>-104.013664</td>\n","      <td>-76.290437</td>\n","    </tr>\n","    <tr>\n","      <th>1874996</th>\n","      <td>3124</td>\n","      <td>596</td>\n","      <td>-0.683037</td>\n","      <td>-0.658466</td>\n","      <td>0.329223</td>\n","      <td>-30.149089</td>\n","      <td>-101.796809</td>\n","      <td>-76.625087</td>\n","    </tr>\n","    <tr>\n","      <th>1874997</th>\n","      <td>3124</td>\n","      <td>597</td>\n","      <td>-0.664730</td>\n","      <td>-0.666625</td>\n","      <td>0.364114</td>\n","      <td>-27.873095</td>\n","      <td>-98.776072</td>\n","      <td>-79.365125</td>\n","    </tr>\n","    <tr>\n","      <th>1874998</th>\n","      <td>3124</td>\n","      <td>598</td>\n","      <td>-0.630534</td>\n","      <td>-0.682565</td>\n","      <td>0.373696</td>\n","      <td>-23.636550</td>\n","      <td>-99.139495</td>\n","      <td>-80.259478</td>\n","    </tr>\n","    <tr>\n","      <th>1874999</th>\n","      <td>3124</td>\n","      <td>599</td>\n","      <td>-0.578351</td>\n","      <td>-0.700235</td>\n","      <td>0.384390</td>\n","      <td>-17.917626</td>\n","      <td>-100.181873</td>\n","      <td>-80.676229</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1875000 rows × 8 columns</p>\n","</div>"],"text/plain":["           id  time     acc_x     acc_y     acc_z       gy_x        gy_y  \\\n","0           0     0  1.206087 -0.179371 -0.148447  -0.591608  -30.549010   \n","1           0     1  1.287696 -0.198974 -0.182444   0.303100  -39.139103   \n","2           0     2  1.304609 -0.195114 -0.253382  -3.617278  -44.122565   \n","3           0     3  1.293095 -0.230366 -0.215210   2.712986  -53.597843   \n","4           0     4  1.300887 -0.187757 -0.222523   4.286707  -57.906561   \n","...       ...   ...       ...       ...       ...        ...         ...   \n","1874995  3124   595 -0.712530 -0.658357  0.293707 -29.367857 -104.013664   \n","1874996  3124   596 -0.683037 -0.658466  0.329223 -30.149089 -101.796809   \n","1874997  3124   597 -0.664730 -0.666625  0.364114 -27.873095  -98.776072   \n","1874998  3124   598 -0.630534 -0.682565  0.373696 -23.636550  -99.139495   \n","1874999  3124   599 -0.578351 -0.700235  0.384390 -17.917626 -100.181873   \n","\n","              gy_z  \n","0       -31.676112  \n","1       -24.927216  \n","2       -25.019629  \n","3       -27.454013  \n","4       -27.961234  \n","...            ...  \n","1874995 -76.290437  \n","1874996 -76.625087  \n","1874997 -79.365125  \n","1874998 -80.259478  \n","1874999 -80.676229  \n","\n","[1875000 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"yZVNqPqWRIjz"},"source":["#### Feature engneering"]},{"cell_type":"markdown","metadata":{"id":"iEaSGImURIj0"},"source":["#####  가속도, 자이로, (자이로-가속도) 센서값을 에너지로 표현"]},{"cell_type":"code","metadata":{"id":"GPa5xSmNRIj0","executionInfo":{"status":"ok","timestamp":1623818176683,"user_tz":-540,"elapsed":751,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["# 왜 세제곱근일까? \n","train['acc_Energy']=(train['acc_x']**2+train['acc_y']**2+train['acc_z']**2)**(1/2)\n","test['acc_Energy']=(test['acc_x']**2+test['acc_y']**2+test['acc_z']**2)**(1/2)\n","\n","train['gy_Energy']=(train['gy_x']**2+train['gy_y']**2+train['gy_z']**2)**(1/2)\n","test['gy_Energy']=(test['gy_x']**2+test['gy_y']**2+test['gy_z']**2)**(1/2)\n","\n","train['gy_acc_Energy']=((train['gy_x']-train['acc_x'])**2+(train['gy_y']-train['acc_y'])**2+(train['gy_z']-train['acc_z'])**2)**(1/2)\n","test['gy_acc_Energy']=((test['gy_x']-test['acc_x'])**2+(test['gy_y']-test['acc_y'])**2+(test['gy_z']-test['acc_z'])**2)**(1/2)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N685v4FCRIj1"},"source":["###### id별 데이터는 0.02초마다 측정된 값들이기 때문에 이전 시간 대비 변화량 적용"]},{"cell_type":"code","metadata":{"id":"tUKG7gYjRIj1","executionInfo":{"status":"ok","timestamp":1623818178900,"user_tz":-540,"elapsed":302,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["dt=0.02 \n","def jerk_signal(signal): \n","        return np.array([(signal[i+1]-signal[i])/dt for i in range(len(signal)-1)])"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CDxON2S4RIj1","executionInfo":{"status":"ok","timestamp":1623818225160,"user_tz":-540,"elapsed":45055,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"e606d34d-cd0e-4707-c5d4-1f413cc71ed0"},"source":["train_dt=[]\n","# tqdm => 진행표시바 나오게 함\n","# np.unique => 고유한 원소의 집합 \n","for i in tqdm(train['id'].unique()): # id 0~3124 \n","    # 한 아이디에 600rows가 있음\n","    temp=train.loc[train['id']==i]\n","    # acc xyz, gy xyz, 위에서 생성한 energy 컬럼 3개\n","    for v in train.columns[2:]:\n","        # 컬럼의 값들만 jerk_signal 함수에 넣은 뒤 반환\n","        values=jerk_signal(temp[v].values)\n","        \n","        # values의 0번째 위치에 0 삽입\n","        values=np.insert(values,0,0)\n","        # v+'_dt' 라는 컬럼에 values값 삽입\n","        temp.loc[:,v+'_dt']=values\n","    train_dt.append(temp)\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["100%|██████████| 3125/3125 [00:44<00:00, 69.65it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":437},"id":"GwCLXMiLep0D","executionInfo":{"status":"ok","timestamp":1623818228855,"user_tz":-540,"elapsed":539,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"a5f96f8d-cd18-43ea-b71d-36a9a553ac9a"},"source":["train_dt[0]"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>time</th>\n","      <th>acc_x</th>\n","      <th>acc_y</th>\n","      <th>acc_z</th>\n","      <th>gy_x</th>\n","      <th>gy_y</th>\n","      <th>gy_z</th>\n","      <th>acc_Energy</th>\n","      <th>gy_Energy</th>\n","      <th>gy_acc_Energy</th>\n","      <th>acc_x_dt</th>\n","      <th>acc_y_dt</th>\n","      <th>acc_z_dt</th>\n","      <th>gy_x_dt</th>\n","      <th>gy_y_dt</th>\n","      <th>gy_z_dt</th>\n","      <th>acc_Energy_dt</th>\n","      <th>gy_Energy_dt</th>\n","      <th>gy_acc_Energy_dt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.206087</td>\n","      <td>-0.179371</td>\n","      <td>-0.148447</td>\n","      <td>-0.591608</td>\n","      <td>-30.549010</td>\n","      <td>-31.676112</td>\n","      <td>1.228355</td>\n","      <td>44.010999</td>\n","      <td>43.812559</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1.287696</td>\n","      <td>-0.198974</td>\n","      <td>-0.182444</td>\n","      <td>0.303100</td>\n","      <td>-39.139103</td>\n","      <td>-24.927216</td>\n","      <td>1.315689</td>\n","      <td>46.403958</td>\n","      <td>46.147664</td>\n","      <td>4.080495</td>\n","      <td>-0.980114</td>\n","      <td>-1.699854</td>\n","      <td>44.735403</td>\n","      <td>-429.504677</td>\n","      <td>337.444793</td>\n","      <td>4.366731</td>\n","      <td>119.647963</td>\n","      <td>116.755248</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1.304609</td>\n","      <td>-0.195114</td>\n","      <td>-0.253382</td>\n","      <td>-3.617278</td>\n","      <td>-44.122565</td>\n","      <td>-25.019629</td>\n","      <td>1.343234</td>\n","      <td>50.851423</td>\n","      <td>50.667670</td>\n","      <td>0.845632</td>\n","      <td>0.192961</td>\n","      <td>-3.546937</td>\n","      <td>-196.018888</td>\n","      <td>-249.173073</td>\n","      <td>-4.620631</td>\n","      <td>1.377227</td>\n","      <td>222.373226</td>\n","      <td>226.000319</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1.293095</td>\n","      <td>-0.230366</td>\n","      <td>-0.215210</td>\n","      <td>2.712986</td>\n","      <td>-53.597843</td>\n","      <td>-27.454013</td>\n","      <td>1.330969</td>\n","      <td>60.281107</td>\n","      <td>59.933763</td>\n","      <td>-0.575711</td>\n","      <td>-1.762585</td>\n","      <td>1.908626</td>\n","      <td>316.513181</td>\n","      <td>-473.763910</td>\n","      <td>-121.719195</td>\n","      <td>-0.613248</td>\n","      <td>471.484195</td>\n","      <td>463.304675</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1.300887</td>\n","      <td>-0.187757</td>\n","      <td>-0.222523</td>\n","      <td>4.286707</td>\n","      <td>-57.906561</td>\n","      <td>-27.961234</td>\n","      <td>1.333070</td>\n","      <td>64.446693</td>\n","      <td>64.107811</td>\n","      <td>0.389598</td>\n","      <td>2.130453</td>\n","      <td>-0.365665</td>\n","      <td>78.686055</td>\n","      <td>-215.435892</td>\n","      <td>-25.361098</td>\n","      <td>0.105059</td>\n","      <td>208.279312</td>\n","      <td>208.702400</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>595</th>\n","      <td>0</td>\n","      <td>595</td>\n","      <td>0.985242</td>\n","      <td>-0.326122</td>\n","      <td>-0.354528</td>\n","      <td>-14.903280</td>\n","      <td>20.172339</td>\n","      <td>22.973018</td>\n","      <td>1.096698</td>\n","      <td>34.011624</td>\n","      <td>34.882752</td>\n","      <td>1.177958</td>\n","      <td>-4.242820</td>\n","      <td>-0.324471</td>\n","      <td>-220.368219</td>\n","      <td>-286.214412</td>\n","      <td>340.765651</td>\n","      <td>2.295085</td>\n","      <td>86.679887</td>\n","      <td>95.725074</td>\n","    </tr>\n","    <tr>\n","      <th>596</th>\n","      <td>0</td>\n","      <td>596</td>\n","      <td>1.052837</td>\n","      <td>-0.220710</td>\n","      <td>-0.413472</td>\n","      <td>-10.857025</td>\n","      <td>19.786856</td>\n","      <td>23.174597</td>\n","      <td>1.152449</td>\n","      <td>32.348982</td>\n","      <td>33.144299</td>\n","      <td>3.379771</td>\n","      <td>5.270607</td>\n","      <td>-2.947208</td>\n","      <td>202.312749</td>\n","      <td>-19.274131</td>\n","      <td>10.078975</td>\n","      <td>2.787526</td>\n","      <td>-83.132095</td>\n","      <td>-86.922683</td>\n","    </tr>\n","    <tr>\n","      <th>597</th>\n","      <td>0</td>\n","      <td>597</td>\n","      <td>1.025643</td>\n","      <td>-0.227845</td>\n","      <td>-0.354516</td>\n","      <td>-2.334243</td>\n","      <td>25.768654</td>\n","      <td>18.932070</td>\n","      <td>1.108846</td>\n","      <td>32.060810</td>\n","      <td>32.543498</td>\n","      <td>-1.359731</td>\n","      <td>-0.356764</td>\n","      <td>2.947780</td>\n","      <td>426.139108</td>\n","      <td>299.089890</td>\n","      <td>-212.126333</td>\n","      <td>-2.180170</td>\n","      <td>-14.408606</td>\n","      <td>-30.040044</td>\n","    </tr>\n","    <tr>\n","      <th>598</th>\n","      <td>0</td>\n","      <td>598</td>\n","      <td>1.031553</td>\n","      <td>-0.387862</td>\n","      <td>-0.277857</td>\n","      <td>-9.710746</td>\n","      <td>28.697694</td>\n","      <td>20.631577</td>\n","      <td>1.136549</td>\n","      <td>36.654034</td>\n","      <td>37.397473</td>\n","      <td>0.295512</td>\n","      <td>-8.000850</td>\n","      <td>3.832979</td>\n","      <td>-368.825182</td>\n","      <td>146.451989</td>\n","      <td>84.975330</td>\n","      <td>1.385165</td>\n","      <td>229.661212</td>\n","      <td>242.698772</td>\n","    </tr>\n","    <tr>\n","      <th>599</th>\n","      <td>0</td>\n","      <td>599</td>\n","      <td>1.138159</td>\n","      <td>-0.426846</td>\n","      <td>-0.430263</td>\n","      <td>-15.891015</td>\n","      <td>21.675950</td>\n","      <td>32.123007</td>\n","      <td>1.289469</td>\n","      <td>41.883873</td>\n","      <td>42.874722</td>\n","      <td>5.330310</td>\n","      <td>-1.949185</td>\n","      <td>-7.620324</td>\n","      <td>-309.013460</td>\n","      <td>-351.087173</td>\n","      <td>574.571503</td>\n","      <td>7.646011</td>\n","      <td>261.491936</td>\n","      <td>273.862455</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>600 rows × 20 columns</p>\n","</div>"],"text/plain":["     id  time     acc_x     acc_y     acc_z       gy_x       gy_y       gy_z  \\\n","0     0     0  1.206087 -0.179371 -0.148447  -0.591608 -30.549010 -31.676112   \n","1     0     1  1.287696 -0.198974 -0.182444   0.303100 -39.139103 -24.927216   \n","2     0     2  1.304609 -0.195114 -0.253382  -3.617278 -44.122565 -25.019629   \n","3     0     3  1.293095 -0.230366 -0.215210   2.712986 -53.597843 -27.454013   \n","4     0     4  1.300887 -0.187757 -0.222523   4.286707 -57.906561 -27.961234   \n","..   ..   ...       ...       ...       ...        ...        ...        ...   \n","595   0   595  0.985242 -0.326122 -0.354528 -14.903280  20.172339  22.973018   \n","596   0   596  1.052837 -0.220710 -0.413472 -10.857025  19.786856  23.174597   \n","597   0   597  1.025643 -0.227845 -0.354516  -2.334243  25.768654  18.932070   \n","598   0   598  1.031553 -0.387862 -0.277857  -9.710746  28.697694  20.631577   \n","599   0   599  1.138159 -0.426846 -0.430263 -15.891015  21.675950  32.123007   \n","\n","     acc_Energy  gy_Energy  gy_acc_Energy  acc_x_dt  acc_y_dt  acc_z_dt  \\\n","0      1.228355  44.010999      43.812559  0.000000  0.000000  0.000000   \n","1      1.315689  46.403958      46.147664  4.080495 -0.980114 -1.699854   \n","2      1.343234  50.851423      50.667670  0.845632  0.192961 -3.546937   \n","3      1.330969  60.281107      59.933763 -0.575711 -1.762585  1.908626   \n","4      1.333070  64.446693      64.107811  0.389598  2.130453 -0.365665   \n","..          ...        ...            ...       ...       ...       ...   \n","595    1.096698  34.011624      34.882752  1.177958 -4.242820 -0.324471   \n","596    1.152449  32.348982      33.144299  3.379771  5.270607 -2.947208   \n","597    1.108846  32.060810      32.543498 -1.359731 -0.356764  2.947780   \n","598    1.136549  36.654034      37.397473  0.295512 -8.000850  3.832979   \n","599    1.289469  41.883873      42.874722  5.330310 -1.949185 -7.620324   \n","\n","        gy_x_dt     gy_y_dt     gy_z_dt  acc_Energy_dt  gy_Energy_dt  \\\n","0      0.000000    0.000000    0.000000       0.000000      0.000000   \n","1     44.735403 -429.504677  337.444793       4.366731    119.647963   \n","2   -196.018888 -249.173073   -4.620631       1.377227    222.373226   \n","3    316.513181 -473.763910 -121.719195      -0.613248    471.484195   \n","4     78.686055 -215.435892  -25.361098       0.105059    208.279312   \n","..          ...         ...         ...            ...           ...   \n","595 -220.368219 -286.214412  340.765651       2.295085     86.679887   \n","596  202.312749  -19.274131   10.078975       2.787526    -83.132095   \n","597  426.139108  299.089890 -212.126333      -2.180170    -14.408606   \n","598 -368.825182  146.451989   84.975330       1.385165    229.661212   \n","599 -309.013460 -351.087173  574.571503       7.646011    261.491936   \n","\n","     gy_acc_Energy_dt  \n","0            0.000000  \n","1          116.755248  \n","2          226.000319  \n","3          463.304675  \n","4          208.702400  \n","..                ...  \n","595         95.725074  \n","596        -86.922683  \n","597        -30.040044  \n","598        242.698772  \n","599        273.862455  \n","\n","[600 rows x 20 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nMaANKeqRIj3","executionInfo":{"status":"ok","timestamp":1623818234857,"user_tz":-540,"elapsed":1532,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"1f299333-2f2b-4c39-e376-41d773c2f541"},"source":["test_dt=[]\n","# train과 같은 방식으로 진행\n","for i in tqdm(test['id'].unique()):\n","    temp=test.loc[test['id']==i]\n","    for v in train.columns[2:]:\n","        values=jerk_signal(temp[v].values)\n","        values=np.insert(values,0,0)\n","        temp.loc[:,v+'_dt']=values\n","    test_dt.append(temp)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["100%|██████████| 76/76 [00:00<00:00, 80.71it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"_-ucCBgYRIj3","executionInfo":{"status":"ok","timestamp":1623803655311,"user_tz":-540,"elapsed":5,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":[""],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WF6mze99RIj4"},"source":["##### 가속도, 자이로 센서값들을 푸리에 변환"]},{"cell_type":"code","metadata":{"id":"0uYSfDmFRIj4","executionInfo":{"status":"ok","timestamp":1623818244097,"user_tz":-540,"elapsed":302,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["from scipy import fftpack\n","from numpy.fft import *\n","\n","def fourier_transform_one_signal(t_signal):\n","    # fft => Fast Fourier transforms\n","    # 매개변수로 받은 t_signal을 푸리에 변환 시킨다\n","    complex_f_signal= fftpack.fft(t_signal)\n","     \n","    # 주파수 해석의 목적은 각 주파수 별로 크기 (magnitude)가 어느 정도인지 알아내는 것\n","    # 따라서 절대값 함수 abs()를 이용해서 크기를 연산할 수 있음\n","    amplitude_f_signal=np.abs(complex_f_signal)\n","    return amplitude_f_signal"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"2VhFjH7tRIj4","executionInfo":{"status":"ok","timestamp":1623818246573,"user_tz":-540,"elapsed":733,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["train=pd.concat(train_dt)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"2F2nkl_YmdyM","executionInfo":{"status":"ok","timestamp":1623818248935,"user_tz":-540,"elapsed":2,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["# train"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qTOVn5vDRIj4","executionInfo":{"status":"ok","timestamp":1623818261958,"user_tz":-540,"elapsed":11197,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"794b9ae0-8a5b-4730-9dfe-a197717e5e2f"},"source":["fft=[]\n","for i in tqdm(train['id'].unique()):\n","    temp=train.loc[train['id']==i]\n","    # acc값과 gy 값만 사용\n","    for i in train.columns[2:8]:\n","        # 위에서 선언한 함수 사용해서 푸리에 변환\n","        temp[i]=fourier_transform_one_signal(temp[i].values)\n","    fft.append(temp)\n","train=pd.concat(fft)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["100%|██████████| 3125/3125 [00:10<00:00, 295.12it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-BRkSLvvm9ej","executionInfo":{"status":"ok","timestamp":1623803666307,"user_tz":-540,"elapsed":15,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["# train"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQ6FK8qpRIj5","executionInfo":{"status":"ok","timestamp":1623818269571,"user_tz":-540,"elapsed":291,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["test=pd.concat(test_dt)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PqGodzjzRIj5","executionInfo":{"status":"ok","timestamp":1623818272774,"user_tz":-540,"elapsed":398,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"9058a2b7-70f9-4225-ace1-1cd879c3a526"},"source":["fft_t=[]\n","for i in tqdm(test['id'].unique()):\n","    temp=test.loc[test['id']==i]\n","    for i in test.columns[2:8]:\n","        temp[i]=fourier_transform_one_signal(temp[i].values)\n","    fft_t.append(temp)\n","test=pd.concat(fft_t)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["100%|██████████| 76/76 [00:00<00:00, 576.73it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"EpQHDrVRRIj5"},"source":["##### Standard scaling 적용\n"]},{"cell_type":"code","metadata":{"id":"Wu1gsSdcRIj6","executionInfo":{"status":"ok","timestamp":1623818275956,"user_tz":-540,"elapsed":304,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["col=train.columns\n","train_s=train.copy()\n","test_s=test.copy()"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":437},"id":"XgMEJI0-9LJh","executionInfo":{"status":"ok","timestamp":1623818277766,"user_tz":-540,"elapsed":309,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"f417053f-dcb8-4687-a01e-04f80e9209b7"},"source":["train_s"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>time</th>\n","      <th>acc_x</th>\n","      <th>acc_y</th>\n","      <th>acc_z</th>\n","      <th>gy_x</th>\n","      <th>gy_y</th>\n","      <th>gy_z</th>\n","      <th>acc_Energy</th>\n","      <th>gy_Energy</th>\n","      <th>gy_acc_Energy</th>\n","      <th>acc_x_dt</th>\n","      <th>acc_y_dt</th>\n","      <th>acc_z_dt</th>\n","      <th>gy_x_dt</th>\n","      <th>gy_y_dt</th>\n","      <th>gy_z_dt</th>\n","      <th>acc_Energy_dt</th>\n","      <th>gy_Energy_dt</th>\n","      <th>gy_acc_Energy_dt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>558.797337</td>\n","      <td>131.082711</td>\n","      <td>222.252919</td>\n","      <td>1119.161589</td>\n","      <td>2015.703683</td>\n","      <td>709.264425</td>\n","      <td>1.228355</td>\n","      <td>44.010999</td>\n","      <td>43.812559</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3.233175</td>\n","      <td>15.689279</td>\n","      <td>12.229014</td>\n","      <td>221.599635</td>\n","      <td>361.903330</td>\n","      <td>477.080942</td>\n","      <td>1.315689</td>\n","      <td>46.403958</td>\n","      <td>46.147664</td>\n","      <td>4.080495</td>\n","      <td>-0.980114</td>\n","      <td>-1.699854</td>\n","      <td>44.735403</td>\n","      <td>-429.504677</td>\n","      <td>337.444793</td>\n","      <td>4.366731</td>\n","      <td>119.647963</td>\n","      <td>116.755248</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>4.832535</td>\n","      <td>8.199566</td>\n","      <td>3.901211</td>\n","      <td>357.200415</td>\n","      <td>430.568986</td>\n","      <td>452.096143</td>\n","      <td>1.343234</td>\n","      <td>50.851423</td>\n","      <td>50.667670</td>\n","      <td>0.845632</td>\n","      <td>0.192961</td>\n","      <td>-3.546937</td>\n","      <td>-196.018888</td>\n","      <td>-249.173073</td>\n","      <td>-4.620631</td>\n","      <td>1.377227</td>\n","      <td>222.373226</td>\n","      <td>226.000319</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>5.675383</td>\n","      <td>5.330015</td>\n","      <td>2.527445</td>\n","      <td>340.433376</td>\n","      <td>787.558320</td>\n","      <td>467.307109</td>\n","      <td>1.330969</td>\n","      <td>60.281107</td>\n","      <td>59.933763</td>\n","      <td>-0.575711</td>\n","      <td>-1.762585</td>\n","      <td>1.908626</td>\n","      <td>316.513181</td>\n","      <td>-473.763910</td>\n","      <td>-121.719195</td>\n","      <td>-0.613248</td>\n","      <td>471.484195</td>\n","      <td>463.304675</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>7.415275</td>\n","      <td>7.980024</td>\n","      <td>6.566908</td>\n","      <td>128.188871</td>\n","      <td>1372.095224</td>\n","      <td>715.824074</td>\n","      <td>1.333070</td>\n","      <td>64.446693</td>\n","      <td>64.107811</td>\n","      <td>0.389598</td>\n","      <td>2.130453</td>\n","      <td>-0.365665</td>\n","      <td>78.686055</td>\n","      <td>-215.435892</td>\n","      <td>-25.361098</td>\n","      <td>0.105059</td>\n","      <td>208.279312</td>\n","      <td>208.702400</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1874995</th>\n","      <td>3124</td>\n","      <td>595</td>\n","      <td>11.743654</td>\n","      <td>3.796333</td>\n","      <td>12.513870</td>\n","      <td>715.873677</td>\n","      <td>1124.494889</td>\n","      <td>645.627066</td>\n","      <td>1.013606</td>\n","      <td>132.293402</td>\n","      <td>131.789903</td>\n","      <td>1.484646</td>\n","      <td>0.303666</td>\n","      <td>0.800069</td>\n","      <td>-150.644663</td>\n","      <td>-34.630282</td>\n","      <td>-8.380088</td>\n","      <td>-1.027613</td>\n","      <td>64.000025</td>\n","      <td>64.267455</td>\n","    </tr>\n","    <tr>\n","      <th>1874996</th>\n","      <td>3124</td>\n","      <td>596</td>\n","      <td>211.498089</td>\n","      <td>82.888508</td>\n","      <td>86.807874</td>\n","      <td>5515.261695</td>\n","      <td>28917.564390</td>\n","      <td>20218.747027</td>\n","      <td>1.004243</td>\n","      <td>130.931134</td>\n","      <td>130.457574</td>\n","      <td>1.474659</td>\n","      <td>-0.005442</td>\n","      <td>1.775771</td>\n","      <td>-39.061611</td>\n","      <td>110.842743</td>\n","      <td>-16.732496</td>\n","      <td>-0.468141</td>\n","      <td>-68.113373</td>\n","      <td>-66.616447</td>\n","    </tr>\n","    <tr>\n","      <th>1874997</th>\n","      <td>3124</td>\n","      <td>597</td>\n","      <td>12.175349</td>\n","      <td>6.200258</td>\n","      <td>2.084554</td>\n","      <td>343.695161</td>\n","      <td>464.375112</td>\n","      <td>78.097163</td>\n","      <td>1.009373</td>\n","      <td>129.739912</td>\n","      <td>129.315545</td>\n","      <td>0.915321</td>\n","      <td>-0.407957</td>\n","      <td>1.744566</td>\n","      <td>113.799702</td>\n","      <td>151.036858</td>\n","      <td>-137.001896</td>\n","      <td>0.256510</td>\n","      <td>-59.561085</td>\n","      <td>-57.101417</td>\n","    </tr>\n","    <tr>\n","      <th>1874998</th>\n","      <td>3124</td>\n","      <td>598</td>\n","      <td>19.116783</td>\n","      <td>3.830800</td>\n","      <td>6.938661</td>\n","      <td>791.376179</td>\n","      <td>2724.373764</td>\n","      <td>1131.590078</td>\n","      <td>1.001557</td>\n","      <td>129.726288</td>\n","      <td>129.324215</td>\n","      <td>1.709833</td>\n","      <td>-0.796984</td>\n","      <td>0.479107</td>\n","      <td>211.827245</td>\n","      <td>-18.171144</td>\n","      <td>-44.717652</td>\n","      <td>-0.390820</td>\n","      <td>-0.681232</td>\n","      <td>0.433504</td>\n","    </tr>\n","    <tr>\n","      <th>1874999</th>\n","      <td>3124</td>\n","      <td>599</td>\n","      <td>22.306532</td>\n","      <td>4.721920</td>\n","      <td>15.463388</td>\n","      <td>357.639418</td>\n","      <td>2058.364675</td>\n","      <td>977.868201</td>\n","      <td>0.986192</td>\n","      <td>129.869561</td>\n","      <td>129.491585</td>\n","      <td>2.609116</td>\n","      <td>-0.883512</td>\n","      <td>0.534668</td>\n","      <td>285.946217</td>\n","      <td>-52.118894</td>\n","      <td>-20.837588</td>\n","      <td>-0.768244</td>\n","      <td>7.163681</td>\n","      <td>8.368506</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1875000 rows × 20 columns</p>\n","</div>"],"text/plain":["           id  time       acc_x       acc_y       acc_z         gy_x  \\\n","0           0     0  558.797337  131.082711  222.252919  1119.161589   \n","1           0     1    3.233175   15.689279   12.229014   221.599635   \n","2           0     2    4.832535    8.199566    3.901211   357.200415   \n","3           0     3    5.675383    5.330015    2.527445   340.433376   \n","4           0     4    7.415275    7.980024    6.566908   128.188871   \n","...       ...   ...         ...         ...         ...          ...   \n","1874995  3124   595   11.743654    3.796333   12.513870   715.873677   \n","1874996  3124   596  211.498089   82.888508   86.807874  5515.261695   \n","1874997  3124   597   12.175349    6.200258    2.084554   343.695161   \n","1874998  3124   598   19.116783    3.830800    6.938661   791.376179   \n","1874999  3124   599   22.306532    4.721920   15.463388   357.639418   \n","\n","                 gy_y          gy_z  acc_Energy   gy_Energy  gy_acc_Energy  \\\n","0         2015.703683    709.264425    1.228355   44.010999      43.812559   \n","1          361.903330    477.080942    1.315689   46.403958      46.147664   \n","2          430.568986    452.096143    1.343234   50.851423      50.667670   \n","3          787.558320    467.307109    1.330969   60.281107      59.933763   \n","4         1372.095224    715.824074    1.333070   64.446693      64.107811   \n","...               ...           ...         ...         ...            ...   \n","1874995   1124.494889    645.627066    1.013606  132.293402     131.789903   \n","1874996  28917.564390  20218.747027    1.004243  130.931134     130.457574   \n","1874997    464.375112     78.097163    1.009373  129.739912     129.315545   \n","1874998   2724.373764   1131.590078    1.001557  129.726288     129.324215   \n","1874999   2058.364675    977.868201    0.986192  129.869561     129.491585   \n","\n","         acc_x_dt  acc_y_dt  acc_z_dt     gy_x_dt     gy_y_dt     gy_z_dt  \\\n","0        0.000000  0.000000  0.000000    0.000000    0.000000    0.000000   \n","1        4.080495 -0.980114 -1.699854   44.735403 -429.504677  337.444793   \n","2        0.845632  0.192961 -3.546937 -196.018888 -249.173073   -4.620631   \n","3       -0.575711 -1.762585  1.908626  316.513181 -473.763910 -121.719195   \n","4        0.389598  2.130453 -0.365665   78.686055 -215.435892  -25.361098   \n","...           ...       ...       ...         ...         ...         ...   \n","1874995  1.484646  0.303666  0.800069 -150.644663  -34.630282   -8.380088   \n","1874996  1.474659 -0.005442  1.775771  -39.061611  110.842743  -16.732496   \n","1874997  0.915321 -0.407957  1.744566  113.799702  151.036858 -137.001896   \n","1874998  1.709833 -0.796984  0.479107  211.827245  -18.171144  -44.717652   \n","1874999  2.609116 -0.883512  0.534668  285.946217  -52.118894  -20.837588   \n","\n","         acc_Energy_dt  gy_Energy_dt  gy_acc_Energy_dt  \n","0             0.000000      0.000000          0.000000  \n","1             4.366731    119.647963        116.755248  \n","2             1.377227    222.373226        226.000319  \n","3            -0.613248    471.484195        463.304675  \n","4             0.105059    208.279312        208.702400  \n","...                ...           ...               ...  \n","1874995      -1.027613     64.000025         64.267455  \n","1874996      -0.468141    -68.113373        -66.616447  \n","1874997       0.256510    -59.561085        -57.101417  \n","1874998      -0.390820     -0.681232          0.433504  \n","1874999      -0.768244      7.163681          8.368506  \n","\n","[1875000 rows x 20 columns]"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"P7gj7zitRIj6","executionInfo":{"status":"ok","timestamp":1623818285476,"user_tz":-540,"elapsed":5493,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["from sklearn.preprocessing import StandardScaler,MinMaxScaler\n","\n","# Standard => 기본 스케일. 평균과 표준편차 사용\n","# 평균을 제거하고 데이터를 단위 분산으로 조정\n","# 이상치가 있다면 평균과 표준편차에 영향을 미쳐 변환된 데이터의 확산은 매우 달라지게 된다\n","# 따라서 이상치가 있는 경우 균형 잡힌 척도를 보장할 수 없다\n","scaler = StandardScaler()\n","\n","\n","# 변환만을 생각한다면 fit() , transform()을 함께 사용하지 않고 \n","# transform()만 사용하면 될텐데 두개 메소드를 함께 사용하는 이유가 있다\n","\n","# 학습데이터 세트에서 변환을 위한 기반 설정(예를 들어 학습 데이터 세트의 최대값/최소값등)을 \n","# 먼저 fit()을 통해서 설정한 뒤에 이를 기반으로 학습 데이터의 transform()을 수행하되 \n","# 학습 데이터에서 설정된 변환을 위한 기반 설정을 그대로 테스트 데이터에도 적용하기 위해서이다\n","# 참고 https://subinium.github.io/MLwithPython-3-3/\n","train_s.iloc[:,2:]= scaler.fit_transform(train_s.iloc[:,2:])\n","train_sc = pd.DataFrame(data = train_s,columns =col)\n","\n","test_s.iloc[:,2:]= scaler.transform(test_s.iloc[:,2:])\n","test_sc = pd.DataFrame(data = test_s,columns =col)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":605},"id":"Or0MTXauRIj6","executionInfo":{"status":"ok","timestamp":1623818287929,"user_tz":-540,"elapsed":304,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"a22fcfd1-34f1-4019-dc85-47832523967d"},"source":["test_s"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>time</th>\n","      <th>acc_x</th>\n","      <th>acc_y</th>\n","      <th>acc_z</th>\n","      <th>gy_x</th>\n","      <th>gy_y</th>\n","      <th>gy_z</th>\n","      <th>acc_Energy</th>\n","      <th>gy_Energy</th>\n","      <th>gy_acc_Energy</th>\n","      <th>acc_x_dt</th>\n","      <th>acc_y_dt</th>\n","      <th>acc_z_dt</th>\n","      <th>gy_x_dt</th>\n","      <th>gy_y_dt</th>\n","      <th>gy_z_dt</th>\n","      <th>acc_Energy_dt</th>\n","      <th>gy_Energy_dt</th>\n","      <th>gy_acc_Energy_dt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2021-06-15 10:43:34:05505</td>\n","      <td>28.174976</td>\n","      <td>10.419897</td>\n","      <td>5.217735</td>\n","      <td>-0.085647</td>\n","      <td>0.053592</td>\n","      <td>0.217031</td>\n","      <td>0.127639</td>\n","      <td>-0.095441</td>\n","      <td>-0.095624</td>\n","      <td>0.000027</td>\n","      <td>0.000298</td>\n","      <td>-0.000433</td>\n","      <td>0.000347</td>\n","      <td>0.000373</td>\n","      <td>0.000273</td>\n","      <td>0.000050</td>\n","      <td>0.001067</td>\n","      <td>0.001067</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>2021-06-15 10:43:34:4155</td>\n","      <td>0.051227</td>\n","      <td>0.386192</td>\n","      <td>0.619065</td>\n","      <td>-0.065798</td>\n","      <td>-0.036759</td>\n","      <td>0.329390</td>\n","      <td>-0.211443</td>\n","      <td>-0.472347</td>\n","      <td>-0.470884</td>\n","      <td>0.387855</td>\n","      <td>5.006484</td>\n","      <td>-1.518231</td>\n","      <td>1.072311</td>\n","      <td>2.358335</td>\n","      <td>3.990670</td>\n","      <td>-0.726108</td>\n","      <td>-1.465528</td>\n","      <td>-1.459423</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>2021-06-15 10:43:34:43266</td>\n","      <td>0.237732</td>\n","      <td>0.063659</td>\n","      <td>-0.025030</td>\n","      <td>0.002916</td>\n","      <td>-0.150036</td>\n","      <td>0.273969</td>\n","      <td>-0.223107</td>\n","      <td>-0.254161</td>\n","      <td>-0.258470</td>\n","      <td>0.047414</td>\n","      <td>-0.192590</td>\n","      <td>0.438448</td>\n","      <td>0.910123</td>\n","      <td>0.333563</td>\n","      <td>0.641238</td>\n","      <td>-0.024929</td>\n","      <td>0.850058</td>\n","      <td>0.827769</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>2021-06-15 10:43:34:45299</td>\n","      <td>0.547696</td>\n","      <td>0.284304</td>\n","      <td>0.629582</td>\n","      <td>0.422472</td>\n","      <td>0.309511</td>\n","      <td>0.403600</td>\n","      <td>0.041000</td>\n","      <td>-0.179612</td>\n","      <td>-0.178458</td>\n","      <td>0.541240</td>\n","      <td>-1.645185</td>\n","      <td>1.045567</td>\n","      <td>-0.851143</td>\n","      <td>0.621189</td>\n","      <td>0.223047</td>\n","      <td>0.565647</td>\n","      <td>0.291151</td>\n","      <td>0.312470</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>2021-06-15 10:43:34:47443</td>\n","      <td>1.352864</td>\n","      <td>0.283086</td>\n","      <td>0.017130</td>\n","      <td>1.554221</td>\n","      <td>1.016960</td>\n","      <td>1.486389</td>\n","      <td>-0.043416</td>\n","      <td>-0.132640</td>\n","      <td>-0.124045</td>\n","      <td>-0.438929</td>\n","      <td>-1.019041</td>\n","      <td>-1.051919</td>\n","      <td>-1.236340</td>\n","      <td>-0.321427</td>\n","      <td>-0.011452</td>\n","      <td>-0.180731</td>\n","      <td>0.183840</td>\n","      <td>0.212839</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>45595</th>\n","      <td>75</td>\n","      <td>2021-06-15 12:54:27:86189</td>\n","      <td>1.150294</td>\n","      <td>0.618547</td>\n","      <td>-0.014429</td>\n","      <td>-0.257918</td>\n","      <td>0.203665</td>\n","      <td>0.903406</td>\n","      <td>-0.163028</td>\n","      <td>-0.740427</td>\n","      <td>-0.729478</td>\n","      <td>-0.162088</td>\n","      <td>0.000298</td>\n","      <td>0.211693</td>\n","      <td>0.030757</td>\n","      <td>-0.036649</td>\n","      <td>-0.046627</td>\n","      <td>-0.163430</td>\n","      <td>-0.057310</td>\n","      <td>-0.058424</td>\n","    </tr>\n","    <tr>\n","      <th>45596</th>\n","      <td>75</td>\n","      <td>2021-06-15 12:54:27:87987</td>\n","      <td>0.535259</td>\n","      <td>0.357470</td>\n","      <td>0.297271</td>\n","      <td>-0.124754</td>\n","      <td>0.273266</td>\n","      <td>0.903835</td>\n","      <td>-0.172286</td>\n","      <td>-0.768202</td>\n","      <td>-0.757326</td>\n","      <td>-0.042372</td>\n","      <td>-0.060536</td>\n","      <td>0.151347</td>\n","      <td>0.083975</td>\n","      <td>-0.042344</td>\n","      <td>-0.027086</td>\n","      <td>-0.019777</td>\n","      <td>-0.107007</td>\n","      <td>-0.107317</td>\n","    </tr>\n","    <tr>\n","      <th>45597</th>\n","      <td>75</td>\n","      <td>2021-06-15 12:54:27:90266</td>\n","      <td>2.918196</td>\n","      <td>1.368807</td>\n","      <td>0.177573</td>\n","      <td>-0.182874</td>\n","      <td>0.095839</td>\n","      <td>1.778981</td>\n","      <td>-0.203511</td>\n","      <td>-0.791290</td>\n","      <td>-0.780994</td>\n","      <td>-0.108465</td>\n","      <td>-0.057568</td>\n","      <td>0.036140</td>\n","      <td>0.083975</td>\n","      <td>-0.028105</td>\n","      <td>-0.007544</td>\n","      <td>-0.066821</td>\n","      <td>-0.088773</td>\n","      <td>-0.091045</td>\n","    </tr>\n","    <tr>\n","      <th>45598</th>\n","      <td>75</td>\n","      <td>2021-06-15 12:54:27:91982</td>\n","      <td>1.713706</td>\n","      <td>0.384604</td>\n","      <td>0.620387</td>\n","      <td>-0.019725</td>\n","      <td>0.507979</td>\n","      <td>0.928336</td>\n","      <td>-0.185187</td>\n","      <td>-0.796425</td>\n","      <td>-0.787184</td>\n","      <td>0.021227</td>\n","      <td>-0.038279</td>\n","      <td>-0.016891</td>\n","      <td>0.040894</td>\n","      <td>0.003220</td>\n","      <td>-0.003636</td>\n","      <td>0.039293</td>\n","      <td>-0.018913</td>\n","      <td>-0.023026</td>\n","    </tr>\n","    <tr>\n","      <th>45599</th>\n","      <td>75</td>\n","      <td>2021-06-15 12:54:27:94159</td>\n","      <td>7.714422</td>\n","      <td>4.966981</td>\n","      <td>0.959320</td>\n","      <td>-0.246481</td>\n","      <td>0.260869</td>\n","      <td>0.865641</td>\n","      <td>-0.169519</td>\n","      <td>-0.797845</td>\n","      <td>-0.787976</td>\n","      <td>0.056144</td>\n","      <td>0.053714</td>\n","      <td>-0.152213</td>\n","      <td>-0.012324</td>\n","      <td>-0.013866</td>\n","      <td>0.008089</td>\n","      <td>0.033604</td>\n","      <td>-0.004460</td>\n","      <td>-0.002015</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>45600 rows × 20 columns</p>\n","</div>"],"text/plain":["       id                       time      acc_x      acc_y     acc_z  \\\n","0       0  2021-06-15 10:43:34:05505  28.174976  10.419897  5.217735   \n","1       0   2021-06-15 10:43:34:4155   0.051227   0.386192  0.619065   \n","2       0  2021-06-15 10:43:34:43266   0.237732   0.063659 -0.025030   \n","3       0  2021-06-15 10:43:34:45299   0.547696   0.284304  0.629582   \n","4       0  2021-06-15 10:43:34:47443   1.352864   0.283086  0.017130   \n","...    ..                        ...        ...        ...       ...   \n","45595  75  2021-06-15 12:54:27:86189   1.150294   0.618547 -0.014429   \n","45596  75  2021-06-15 12:54:27:87987   0.535259   0.357470  0.297271   \n","45597  75  2021-06-15 12:54:27:90266   2.918196   1.368807  0.177573   \n","45598  75  2021-06-15 12:54:27:91982   1.713706   0.384604  0.620387   \n","45599  75  2021-06-15 12:54:27:94159   7.714422   4.966981  0.959320   \n","\n","           gy_x      gy_y      gy_z  acc_Energy  gy_Energy  gy_acc_Energy  \\\n","0     -0.085647  0.053592  0.217031    0.127639  -0.095441      -0.095624   \n","1     -0.065798 -0.036759  0.329390   -0.211443  -0.472347      -0.470884   \n","2      0.002916 -0.150036  0.273969   -0.223107  -0.254161      -0.258470   \n","3      0.422472  0.309511  0.403600    0.041000  -0.179612      -0.178458   \n","4      1.554221  1.016960  1.486389   -0.043416  -0.132640      -0.124045   \n","...         ...       ...       ...         ...        ...            ...   \n","45595 -0.257918  0.203665  0.903406   -0.163028  -0.740427      -0.729478   \n","45596 -0.124754  0.273266  0.903835   -0.172286  -0.768202      -0.757326   \n","45597 -0.182874  0.095839  1.778981   -0.203511  -0.791290      -0.780994   \n","45598 -0.019725  0.507979  0.928336   -0.185187  -0.796425      -0.787184   \n","45599 -0.246481  0.260869  0.865641   -0.169519  -0.797845      -0.787976   \n","\n","       acc_x_dt  acc_y_dt  acc_z_dt   gy_x_dt   gy_y_dt   gy_z_dt  \\\n","0      0.000027  0.000298 -0.000433  0.000347  0.000373  0.000273   \n","1      0.387855  5.006484 -1.518231  1.072311  2.358335  3.990670   \n","2      0.047414 -0.192590  0.438448  0.910123  0.333563  0.641238   \n","3      0.541240 -1.645185  1.045567 -0.851143  0.621189  0.223047   \n","4     -0.438929 -1.019041 -1.051919 -1.236340 -0.321427 -0.011452   \n","...         ...       ...       ...       ...       ...       ...   \n","45595 -0.162088  0.000298  0.211693  0.030757 -0.036649 -0.046627   \n","45596 -0.042372 -0.060536  0.151347  0.083975 -0.042344 -0.027086   \n","45597 -0.108465 -0.057568  0.036140  0.083975 -0.028105 -0.007544   \n","45598  0.021227 -0.038279 -0.016891  0.040894  0.003220 -0.003636   \n","45599  0.056144  0.053714 -0.152213 -0.012324 -0.013866  0.008089   \n","\n","       acc_Energy_dt  gy_Energy_dt  gy_acc_Energy_dt  \n","0           0.000050      0.001067          0.001067  \n","1          -0.726108     -1.465528         -1.459423  \n","2          -0.024929      0.850058          0.827769  \n","3           0.565647      0.291151          0.312470  \n","4          -0.180731      0.183840          0.212839  \n","...              ...           ...               ...  \n","45595      -0.163430     -0.057310         -0.058424  \n","45596      -0.019777     -0.107007         -0.107317  \n","45597      -0.066821     -0.088773         -0.091045  \n","45598       0.039293     -0.018913         -0.023026  \n","45599       0.033604     -0.004460         -0.002015  \n","\n","[45600 rows x 20 columns]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"D02aM-hTRIj6","executionInfo":{"status":"ok","timestamp":1623803671598,"user_tz":-540,"elapsed":12,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":[""],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GZOmEXVGRIj7"},"source":["##### 모델링\n","\n","+ CNN, LSTM, CNN+LSTM 등 여러 구조 적용해보다가 CNN에서 Flatten 없이 Global average pooling 한 구조가 가장 성능이 좋아 채택했습니다.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ceOmiD7kCK6h","executionInfo":{"status":"ok","timestamp":1623818294250,"user_tz":-540,"elapsed":3553,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"8ab31b6c-5611-4a0c-d9a5-e72a32bebb38"},"source":["pip install tensorflow_addons"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Collecting tensorflow_addons\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/4b/e893d194e626c24b3df2253066aa418f46a432fdb68250cde14bf9bb0700/tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679kB)\n","\r\u001b[K     |▌                               | 10kB 21.9MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 17.8MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 14.9MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 13.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51kB 8.7MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 10.1MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71kB 9.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81kB 10.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92kB 9.9MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 112kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 143kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 174kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 194kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 204kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 225kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 235kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 256kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 266kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 286kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 307kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 327kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 348kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 358kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 368kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 378kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 389kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 399kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 409kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 419kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 430kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 440kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 450kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 460kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 471kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 481kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 491kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 501kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 512kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 522kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 532kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 542kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 552kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 563kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 573kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 583kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 593kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 604kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 614kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 624kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 634kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 645kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 655kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 665kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 675kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 686kB 8.4MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.13.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtx1cXMxCjr3","executionInfo":{"status":"ok","timestamp":1623818299298,"user_tz":-540,"elapsed":3780,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"56a1c7c5-6dea-4133-d627-b320f3a2b75b"},"source":["pip install np_utils"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Collecting np_utils\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/18/5704a782fd72727a9e63198fcc76fadb86975f45bcdf579c10f668329508/np_utils-0.5.12.1.tar.gz (61kB)\n","\r\u001b[K     |█████▍                          | 10kB 23.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 20kB 28.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30kB 20.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 40kB 16.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 51kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from np_utils) (1.19.5)\n","Requirement already satisfied: future>=0.16 in /usr/local/lib/python3.7/dist-packages (from np_utils) (0.16.0)\n","Building wheels for collected packages: np-utils\n","  Building wheel for np-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for np-utils: filename=np_utils-0.5.12.1-cp37-none-any.whl size=57133 sha256=371ad33d61f839bf1c50aa26aeb6bc547c1f337a2920239b9b5f37cb1b2c69aa\n","  Stored in directory: /root/.cache/pip/wheels/92/4b/81/206efd0d01330a96f3aebe5021d2d5f0b264b7ade827c306ef\n","Successfully built np-utils\n","Installing collected packages: np-utils\n","Successfully installed np-utils-0.5.12.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KWhdQQZ8RIj7","executionInfo":{"status":"ok","timestamp":1623818303286,"user_tz":-540,"elapsed":2004,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["import tensorflow as tf\n","import tensorflow.keras as keras\n","import tensorflow_addons as tfa\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM,Bidirectional,Dropout\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","from keras.utils.np_utils import to_categorical\n","from keras import backend as K \n","from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\n","from sklearn.model_selection import KFold,StratifiedKFold\n","from numpy.random import seed\n","import keras"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M-qSe_IORIj7","executionInfo":{"status":"ok","timestamp":1623818305054,"user_tz":-540,"elapsed":287,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"094dc71d-05fe-4dde-fb70-db42f35d53e4"},"source":["X=np.array(train_sc.iloc[:,2:]).reshape(3125, 600, -1)\n","# 아이디 별로 600개의 데이터가 있고, 3125개의 아이디가 있다(0~3124)\n","X.shape"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3125, 600, 18)"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":605},"id":"-K2h4-PYZdhY","executionInfo":{"status":"ok","timestamp":1623818307061,"user_tz":-540,"elapsed":470,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"16777ea1-6372-4a03-f9df-a370084f4ed5"},"source":["test_sc"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>time</th>\n","      <th>acc_x</th>\n","      <th>acc_y</th>\n","      <th>acc_z</th>\n","      <th>gy_x</th>\n","      <th>gy_y</th>\n","      <th>gy_z</th>\n","      <th>acc_Energy</th>\n","      <th>gy_Energy</th>\n","      <th>gy_acc_Energy</th>\n","      <th>acc_x_dt</th>\n","      <th>acc_y_dt</th>\n","      <th>acc_z_dt</th>\n","      <th>gy_x_dt</th>\n","      <th>gy_y_dt</th>\n","      <th>gy_z_dt</th>\n","      <th>acc_Energy_dt</th>\n","      <th>gy_Energy_dt</th>\n","      <th>gy_acc_Energy_dt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2021-06-15 10:43:34:05505</td>\n","      <td>28.174976</td>\n","      <td>10.419897</td>\n","      <td>5.217735</td>\n","      <td>-0.085647</td>\n","      <td>0.053592</td>\n","      <td>0.217031</td>\n","      <td>0.127639</td>\n","      <td>-0.095441</td>\n","      <td>-0.095624</td>\n","      <td>0.000027</td>\n","      <td>0.000298</td>\n","      <td>-0.000433</td>\n","      <td>0.000347</td>\n","      <td>0.000373</td>\n","      <td>0.000273</td>\n","      <td>0.000050</td>\n","      <td>0.001067</td>\n","      <td>0.001067</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>2021-06-15 10:43:34:4155</td>\n","      <td>0.051227</td>\n","      <td>0.386192</td>\n","      <td>0.619065</td>\n","      <td>-0.065798</td>\n","      <td>-0.036759</td>\n","      <td>0.329390</td>\n","      <td>-0.211443</td>\n","      <td>-0.472347</td>\n","      <td>-0.470884</td>\n","      <td>0.387855</td>\n","      <td>5.006484</td>\n","      <td>-1.518231</td>\n","      <td>1.072311</td>\n","      <td>2.358335</td>\n","      <td>3.990670</td>\n","      <td>-0.726108</td>\n","      <td>-1.465528</td>\n","      <td>-1.459423</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>2021-06-15 10:43:34:43266</td>\n","      <td>0.237732</td>\n","      <td>0.063659</td>\n","      <td>-0.025030</td>\n","      <td>0.002916</td>\n","      <td>-0.150036</td>\n","      <td>0.273969</td>\n","      <td>-0.223107</td>\n","      <td>-0.254161</td>\n","      <td>-0.258470</td>\n","      <td>0.047414</td>\n","      <td>-0.192590</td>\n","      <td>0.438448</td>\n","      <td>0.910123</td>\n","      <td>0.333563</td>\n","      <td>0.641238</td>\n","      <td>-0.024929</td>\n","      <td>0.850058</td>\n","      <td>0.827769</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>2021-06-15 10:43:34:45299</td>\n","      <td>0.547696</td>\n","      <td>0.284304</td>\n","      <td>0.629582</td>\n","      <td>0.422472</td>\n","      <td>0.309511</td>\n","      <td>0.403600</td>\n","      <td>0.041000</td>\n","      <td>-0.179612</td>\n","      <td>-0.178458</td>\n","      <td>0.541240</td>\n","      <td>-1.645185</td>\n","      <td>1.045567</td>\n","      <td>-0.851143</td>\n","      <td>0.621189</td>\n","      <td>0.223047</td>\n","      <td>0.565647</td>\n","      <td>0.291151</td>\n","      <td>0.312470</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>2021-06-15 10:43:34:47443</td>\n","      <td>1.352864</td>\n","      <td>0.283086</td>\n","      <td>0.017130</td>\n","      <td>1.554221</td>\n","      <td>1.016960</td>\n","      <td>1.486389</td>\n","      <td>-0.043416</td>\n","      <td>-0.132640</td>\n","      <td>-0.124045</td>\n","      <td>-0.438929</td>\n","      <td>-1.019041</td>\n","      <td>-1.051919</td>\n","      <td>-1.236340</td>\n","      <td>-0.321427</td>\n","      <td>-0.011452</td>\n","      <td>-0.180731</td>\n","      <td>0.183840</td>\n","      <td>0.212839</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>45595</th>\n","      <td>75</td>\n","      <td>2021-06-15 12:54:27:86189</td>\n","      <td>1.150294</td>\n","      <td>0.618547</td>\n","      <td>-0.014429</td>\n","      <td>-0.257918</td>\n","      <td>0.203665</td>\n","      <td>0.903406</td>\n","      <td>-0.163028</td>\n","      <td>-0.740427</td>\n","      <td>-0.729478</td>\n","      <td>-0.162088</td>\n","      <td>0.000298</td>\n","      <td>0.211693</td>\n","      <td>0.030757</td>\n","      <td>-0.036649</td>\n","      <td>-0.046627</td>\n","      <td>-0.163430</td>\n","      <td>-0.057310</td>\n","      <td>-0.058424</td>\n","    </tr>\n","    <tr>\n","      <th>45596</th>\n","      <td>75</td>\n","      <td>2021-06-15 12:54:27:87987</td>\n","      <td>0.535259</td>\n","      <td>0.357470</td>\n","      <td>0.297271</td>\n","      <td>-0.124754</td>\n","      <td>0.273266</td>\n","      <td>0.903835</td>\n","      <td>-0.172286</td>\n","      <td>-0.768202</td>\n","      <td>-0.757326</td>\n","      <td>-0.042372</td>\n","      <td>-0.060536</td>\n","      <td>0.151347</td>\n","      <td>0.083975</td>\n","      <td>-0.042344</td>\n","      <td>-0.027086</td>\n","      <td>-0.019777</td>\n","      <td>-0.107007</td>\n","      <td>-0.107317</td>\n","    </tr>\n","    <tr>\n","      <th>45597</th>\n","      <td>75</td>\n","      <td>2021-06-15 12:54:27:90266</td>\n","      <td>2.918196</td>\n","      <td>1.368807</td>\n","      <td>0.177573</td>\n","      <td>-0.182874</td>\n","      <td>0.095839</td>\n","      <td>1.778981</td>\n","      <td>-0.203511</td>\n","      <td>-0.791290</td>\n","      <td>-0.780994</td>\n","      <td>-0.108465</td>\n","      <td>-0.057568</td>\n","      <td>0.036140</td>\n","      <td>0.083975</td>\n","      <td>-0.028105</td>\n","      <td>-0.007544</td>\n","      <td>-0.066821</td>\n","      <td>-0.088773</td>\n","      <td>-0.091045</td>\n","    </tr>\n","    <tr>\n","      <th>45598</th>\n","      <td>75</td>\n","      <td>2021-06-15 12:54:27:91982</td>\n","      <td>1.713706</td>\n","      <td>0.384604</td>\n","      <td>0.620387</td>\n","      <td>-0.019725</td>\n","      <td>0.507979</td>\n","      <td>0.928336</td>\n","      <td>-0.185187</td>\n","      <td>-0.796425</td>\n","      <td>-0.787184</td>\n","      <td>0.021227</td>\n","      <td>-0.038279</td>\n","      <td>-0.016891</td>\n","      <td>0.040894</td>\n","      <td>0.003220</td>\n","      <td>-0.003636</td>\n","      <td>0.039293</td>\n","      <td>-0.018913</td>\n","      <td>-0.023026</td>\n","    </tr>\n","    <tr>\n","      <th>45599</th>\n","      <td>75</td>\n","      <td>2021-06-15 12:54:27:94159</td>\n","      <td>7.714422</td>\n","      <td>4.966981</td>\n","      <td>0.959320</td>\n","      <td>-0.246481</td>\n","      <td>0.260869</td>\n","      <td>0.865641</td>\n","      <td>-0.169519</td>\n","      <td>-0.797845</td>\n","      <td>-0.787976</td>\n","      <td>0.056144</td>\n","      <td>0.053714</td>\n","      <td>-0.152213</td>\n","      <td>-0.012324</td>\n","      <td>-0.013866</td>\n","      <td>0.008089</td>\n","      <td>0.033604</td>\n","      <td>-0.004460</td>\n","      <td>-0.002015</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>45600 rows × 20 columns</p>\n","</div>"],"text/plain":["       id                       time      acc_x      acc_y     acc_z  \\\n","0       0  2021-06-15 10:43:34:05505  28.174976  10.419897  5.217735   \n","1       0   2021-06-15 10:43:34:4155   0.051227   0.386192  0.619065   \n","2       0  2021-06-15 10:43:34:43266   0.237732   0.063659 -0.025030   \n","3       0  2021-06-15 10:43:34:45299   0.547696   0.284304  0.629582   \n","4       0  2021-06-15 10:43:34:47443   1.352864   0.283086  0.017130   \n","...    ..                        ...        ...        ...       ...   \n","45595  75  2021-06-15 12:54:27:86189   1.150294   0.618547 -0.014429   \n","45596  75  2021-06-15 12:54:27:87987   0.535259   0.357470  0.297271   \n","45597  75  2021-06-15 12:54:27:90266   2.918196   1.368807  0.177573   \n","45598  75  2021-06-15 12:54:27:91982   1.713706   0.384604  0.620387   \n","45599  75  2021-06-15 12:54:27:94159   7.714422   4.966981  0.959320   \n","\n","           gy_x      gy_y      gy_z  acc_Energy  gy_Energy  gy_acc_Energy  \\\n","0     -0.085647  0.053592  0.217031    0.127639  -0.095441      -0.095624   \n","1     -0.065798 -0.036759  0.329390   -0.211443  -0.472347      -0.470884   \n","2      0.002916 -0.150036  0.273969   -0.223107  -0.254161      -0.258470   \n","3      0.422472  0.309511  0.403600    0.041000  -0.179612      -0.178458   \n","4      1.554221  1.016960  1.486389   -0.043416  -0.132640      -0.124045   \n","...         ...       ...       ...         ...        ...            ...   \n","45595 -0.257918  0.203665  0.903406   -0.163028  -0.740427      -0.729478   \n","45596 -0.124754  0.273266  0.903835   -0.172286  -0.768202      -0.757326   \n","45597 -0.182874  0.095839  1.778981   -0.203511  -0.791290      -0.780994   \n","45598 -0.019725  0.507979  0.928336   -0.185187  -0.796425      -0.787184   \n","45599 -0.246481  0.260869  0.865641   -0.169519  -0.797845      -0.787976   \n","\n","       acc_x_dt  acc_y_dt  acc_z_dt   gy_x_dt   gy_y_dt   gy_z_dt  \\\n","0      0.000027  0.000298 -0.000433  0.000347  0.000373  0.000273   \n","1      0.387855  5.006484 -1.518231  1.072311  2.358335  3.990670   \n","2      0.047414 -0.192590  0.438448  0.910123  0.333563  0.641238   \n","3      0.541240 -1.645185  1.045567 -0.851143  0.621189  0.223047   \n","4     -0.438929 -1.019041 -1.051919 -1.236340 -0.321427 -0.011452   \n","...         ...       ...       ...       ...       ...       ...   \n","45595 -0.162088  0.000298  0.211693  0.030757 -0.036649 -0.046627   \n","45596 -0.042372 -0.060536  0.151347  0.083975 -0.042344 -0.027086   \n","45597 -0.108465 -0.057568  0.036140  0.083975 -0.028105 -0.007544   \n","45598  0.021227 -0.038279 -0.016891  0.040894  0.003220 -0.003636   \n","45599  0.056144  0.053714 -0.152213 -0.012324 -0.013866  0.008089   \n","\n","       acc_Energy_dt  gy_Energy_dt  gy_acc_Energy_dt  \n","0           0.000050      0.001067          0.001067  \n","1          -0.726108     -1.465528         -1.459423  \n","2          -0.024929      0.850058          0.827769  \n","3           0.565647      0.291151          0.312470  \n","4          -0.180731      0.183840          0.212839  \n","...              ...           ...               ...  \n","45595      -0.163430     -0.057310         -0.058424  \n","45596      -0.019777     -0.107007         -0.107317  \n","45597      -0.066821     -0.088773         -0.091045  \n","45598       0.039293     -0.018913         -0.023026  \n","45599       0.033604     -0.004460         -0.002015  \n","\n","[45600 rows x 20 columns]"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aB5_USOdRIj8","executionInfo":{"status":"ok","timestamp":1623818311270,"user_tz":-540,"elapsed":286,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"ab1a4780-bcfa-4ee0-c306-c80139062989"},"source":["test_x=np.array(test_sc.iloc[:,2:]).reshape(76, 600, -1)\n","# reshape()의 ‘-1’이 의미하는 바는, 변경된 배열의 ‘-1’ \n","# 위치의 차원은 “원래 배열의 길이와 남은 차원으로 부터 추정”\n","# => 차원을 따로 지정하지않고 추정하겠다라는 의미인듯\n","test_x.shape"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(76, 600, 18)"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"GFCsPybfFh-I","executionInfo":{"status":"ok","timestamp":1623818314489,"user_tz":-540,"elapsed":427,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"5e504c96-46ee-456d-8375-3ef79f8c08db"},"source":["train_labels"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>label_desc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>37</td>\n","      <td>Shoulder Press (dumbbell)</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>26</td>\n","      <td>Non-Exercise</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>Biceps Curl (band)</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>26</td>\n","      <td>Non-Exercise</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>26</td>\n","      <td>Non-Exercise</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3120</th>\n","      <td>3120</td>\n","      <td>26</td>\n","      <td>Non-Exercise</td>\n","    </tr>\n","    <tr>\n","      <th>3121</th>\n","      <td>3121</td>\n","      <td>26</td>\n","      <td>Non-Exercise</td>\n","    </tr>\n","    <tr>\n","      <th>3122</th>\n","      <td>3122</td>\n","      <td>15</td>\n","      <td>Dynamic Stretch (at your own pace)</td>\n","    </tr>\n","    <tr>\n","      <th>3123</th>\n","      <td>3123</td>\n","      <td>26</td>\n","      <td>Non-Exercise</td>\n","    </tr>\n","    <tr>\n","      <th>3124</th>\n","      <td>3124</td>\n","      <td>2</td>\n","      <td>Bicep Curl</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3125 rows × 3 columns</p>\n","</div>"],"text/plain":["        id  label                          label_desc\n","0        0     37           Shoulder Press (dumbbell)\n","1        1     26                        Non-Exercise\n","2        2      3                  Biceps Curl (band)\n","3        3     26                        Non-Exercise\n","4        4     26                        Non-Exercise\n","...    ...    ...                                 ...\n","3120  3120     26                        Non-Exercise\n","3121  3121     26                        Non-Exercise\n","3122  3122     15  Dynamic Stretch (at your own pace)\n","3123  3123     26                        Non-Exercise\n","3124  3124      2                          Bicep Curl\n","\n","[3125 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W8ynpblPRIj8","executionInfo":{"status":"ok","timestamp":1623818318221,"user_tz":-540,"elapsed":311,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"05f6d433-b369-48e1-df7f-26e94f4b3159"},"source":["y = train_labels['label'].values\n","y = tf.keras.utils.to_categorical(train_labels['label']) \n","print(y)\n","print(\"y.shape => \",y.shape)\n","y[0]\n","\n","# y[0] 출력해보면 알겠지만 label 번째 인덱스 값만 1이다(0부터 시작했을 경우)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["[[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 1. ... 0. 0. 0.]]\n","y.shape =>  (3125, 61)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"iL9yHVlnRIj8"},"source":["##### 모델 구조 "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vjIuDxRTINUq","executionInfo":{"status":"ok","timestamp":1623818321162,"user_tz":-540,"elapsed":295,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"076600c4-9460-4624-9651-7af2a2b8b61c"},"source":["# seed를 지정하면 계속 똑같은 값만 나온다\n","seed(1)\n","np.random.rand(10)"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([4.17022005e-01, 7.20324493e-01, 1.14374817e-04, 3.02332573e-01,\n","       1.46755891e-01, 9.23385948e-02, 1.86260211e-01, 3.45560727e-01,\n","       3.96767474e-01, 5.38816734e-01])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"pkNLUw8ZRIj8","executionInfo":{"status":"ok","timestamp":1623818323104,"user_tz":-540,"elapsed":302,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["def cnn_model(input_shape, classes):\n","    # random.seed를 설정해주면 같은 랜덤셋이 나온다했는데..\n","    # 왜 자꾸 seed를 지정해주는지는 모르겠다 \n","    seed(2021)\n","    tf.random.set_seed(2021)\n","    \n","    # keras input은 데이터의 입력 모양을 모델에 알려주는 역할\n","    input_layer = keras.layers.Input(input_shape)\n","    # Conv1D => 1D 컨볼루션 레이어\n","    # Layers 부분 너무 어렵다 ㅋ 이해 불가\n","    conv1 = keras.layers.Conv1D(filters=128, kernel_size=9, padding='same')(input_layer)\n","    conv1 = keras.layers.BatchNormalization()(conv1)\n","    conv1 = keras.layers.Activation(activation='relu')(conv1)\n","    conv1 = keras.layers.Dropout(rate=0.3)(conv1)\n","\n","    conv2 = keras.layers.Conv1D(filters=256, kernel_size=6, padding='same')(conv1)\n","    conv2 = keras.layers.BatchNormalization()(conv2)\n","    conv2 = keras.layers.Activation('relu')(conv2)\n","    conv2 = keras.layers.Dropout(rate=0.4)(conv2)\n","    \n","    conv3 = keras.layers.Conv1D(128, kernel_size=3,padding='same')(conv2)\n","    conv3 = keras.layers.BatchNormalization()(conv3)\n","    conv3 = keras.layers.Activation('relu')(conv3)\n","    conv3 = keras.layers.Dropout(rate=0.5)(conv3)\n","    \n","    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n","    \n","    output_layer = keras.layers.Dense(classes, activation='softmax')(gap)\n","    \n","    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n","    \n","    model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(), \n","        metrics=['accuracy'])\n","    \n","    return model"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GUOQ-tWcRIj9"},"source":["##### 10-fold StratifiedKFold"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"bAlIw_XMXllo","executionInfo":{"status":"ok","timestamp":1623818329334,"user_tz":-540,"elapsed":288,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"53e42170-ae7d-4d3d-88b9-6331264de678"},"source":["os.getcwd()"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/DATA'"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"colab":{"base_uri":"https://localhost:8080/"},"id":"-3NqgM5BRIj9","executionInfo":{"status":"ok","timestamp":1623820100238,"user_tz":-540,"elapsed":1769021,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"afb65a59-4a41-4f78-c15e-c6b2b30b380c"},"source":["# 과적합을 방지하기 위한 교차검증\n","# StratifiedKFold => target에 속성값 개수를 동일하게 가져감으로써 기존의 kfold와 같은 데이터 몰림현상 방지\n","skf = StratifiedKFold(n_splits = 10, random_state = 2021, shuffle = True)\n","\n","# ReduceLROnPlateau => keras의 콜백함수\n","# 학습률이 개선되지 않을 때 학습률을 동적으로 조정하여 개선하는 효과를 기대할 수 있음\n","# patience => epoch 4 동안 개선되지 않으면 callback 호출\n","# verbos => 1일 경우, EarlyStopping이 적용될 때, 화면에 적용되었다고 나타냄, 0일 경우, 화면에 나타냄 없이 종료\n","# factor => callback 호출 시 학습률을 0.5로 줄인다(1/2)\n","reLR = ReduceLROnPlateau(patience = 4,verbose = 1,factor = 0.5) \n","\n","# EarlyStopping => 더 이상 개선의 여지가 없을 때 학습을 종료시키는 콜백함수\n","# monitor = 'val_loss' => 검증 손실을 기준으로 callback이 호출\n","# patience => epoch 8 동안 개선되지 않으면 callback 호출\n","# mode => 관찰 항목이 val_loss인 경우 감소되는 것이 멈출 때 종료되야 하므로 'min'으로 설정\n","es =EarlyStopping(monitor='val_loss', patience=8, mode='min')\n","\n","accuracy = []\n","losss=[]\n","models=[]\n","\n","for i, (train, validation) in enumerate(skf.split(X, y.argmax(1))) :\n","    # ModelCheckpoint => 모델을 저장할 때 사용하는 콜백 함수\n","    # filepath => 모델을 저장할 경로\n","    # save_best_only => True 인 경우, monitor 되고 있는 값을 기준으로 가장 좋은 값으로 모델이 저장\n","    #                => False인 경우, 매 epoch마다 모델이 filepath{epoch}으로 저장 ex) model0,model1,model2..\n","    # verbose, monitor, mode => 위랑 일맥상통\n","    # save_weights_only => True인 경우, 모델의 weights만 저장\n","    #                   -> False인 경우, 모델 레이어 및 weights 모두 저장\n","    mc = ModelCheckpoint(f'./model_kf/cv_study{i + 1}.h5',save_best_only=True, verbose=0, monitor = 'val_loss', mode = 'min', save_weights_only=True)\n","    print(\"-\" * 20 +\"Fold_\"+str(i+1)+ \"-\" * 20)\n","    model = cnn_model((600,18),61)\n","    history = model.fit(X[train], y[train], epochs = 100, validation_data= (X[validation], y[validation]), \n","                        verbose=1,batch_size=64,callbacks=[es,mc,reLR])\n","    model.load_weights(f'./model_kf/cv_study{i + 1}.h5')\n","    \n","    # model.evaluate => 테스트 데이터를 통해 학습한 모델에 대한 정확도를 평가\n","    # 첫번째 인자 = 테스트 데이터에 해당\n","    # 두번째 인자 = 지도 학습에서 레이블 테스트 데이터에 해당\n","    # batch_size = 배치 크기\n","    k_accuracy = '%.4f' % (model.evaluate(X[validation], y[validation])[1])\n","    k_loss = '%.4f' % (model.evaluate(X[validation], y[validation])[0])\n","    \n","    accuracy.append(k_accuracy)\n","    losss.append(k_loss)\n","    models.append(model)\n","\n","print('\\nK-fold cross validation Auc: {}'.format(accuracy))\n","print('\\nK-fold cross validation loss: {}'.format(losss))"],"execution_count":33,"outputs":[{"output_type":"stream","text":["--------------------Fold_1--------------------\n","Epoch 1/100\n","44/44 [==============================] - 48s 65ms/step - loss: 3.4393 - accuracy: 0.3483 - val_loss: 3.2258 - val_accuracy: 0.2045\n","Epoch 2/100\n","44/44 [==============================] - 2s 45ms/step - loss: 1.9799 - accuracy: 0.5518 - val_loss: 2.0592 - val_accuracy: 0.5208\n","Epoch 3/100\n","44/44 [==============================] - 2s 45ms/step - loss: 1.7910 - accuracy: 0.5596 - val_loss: 2.1110 - val_accuracy: 0.5463\n","Epoch 4/100\n","44/44 [==============================] - 2s 45ms/step - loss: 1.5633 - accuracy: 0.6149 - val_loss: 1.9055 - val_accuracy: 0.5623\n","Epoch 5/100\n","44/44 [==============================] - 2s 45ms/step - loss: 1.3805 - accuracy: 0.6512 - val_loss: 1.5534 - val_accuracy: 0.6070\n","Epoch 6/100\n","44/44 [==============================] - 2s 45ms/step - loss: 1.2710 - accuracy: 0.6785 - val_loss: 1.4004 - val_accuracy: 0.6198\n","Epoch 7/100\n","44/44 [==============================] - 2s 45ms/step - loss: 1.1608 - accuracy: 0.6915 - val_loss: 1.2672 - val_accuracy: 0.6613\n","Epoch 8/100\n","44/44 [==============================] - 2s 45ms/step - loss: 1.0609 - accuracy: 0.7149 - val_loss: 1.1959 - val_accuracy: 0.6933\n","Epoch 9/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.9689 - accuracy: 0.7309 - val_loss: 1.1106 - val_accuracy: 0.7188\n","Epoch 10/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.9032 - accuracy: 0.7617 - val_loss: 0.9896 - val_accuracy: 0.7348\n","Epoch 11/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.8819 - accuracy: 0.7591 - val_loss: 0.9564 - val_accuracy: 0.7796\n","Epoch 12/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.7898 - accuracy: 0.7796 - val_loss: 1.1124 - val_accuracy: 0.7316\n","Epoch 13/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.7495 - accuracy: 0.7892 - val_loss: 0.9360 - val_accuracy: 0.7700\n","Epoch 14/100\n","44/44 [==============================] - 2s 45ms/step - loss: 0.7112 - accuracy: 0.7993 - val_loss: 0.8664 - val_accuracy: 0.7827\n","Epoch 15/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.6845 - accuracy: 0.8006 - val_loss: 0.7773 - val_accuracy: 0.7891\n","Epoch 16/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.6262 - accuracy: 0.8335 - val_loss: 0.7798 - val_accuracy: 0.7987\n","Epoch 17/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.5948 - accuracy: 0.8281 - val_loss: 0.7671 - val_accuracy: 0.7859\n","Epoch 18/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.5878 - accuracy: 0.8244 - val_loss: 0.7523 - val_accuracy: 0.7859\n","Epoch 19/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.5411 - accuracy: 0.8484 - val_loss: 0.7076 - val_accuracy: 0.7955\n","Epoch 20/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.5133 - accuracy: 0.8487 - val_loss: 0.6876 - val_accuracy: 0.7955\n","Epoch 21/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.5140 - accuracy: 0.8578 - val_loss: 0.8022 - val_accuracy: 0.8019\n","Epoch 22/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.4902 - accuracy: 0.8574 - val_loss: 0.6604 - val_accuracy: 0.8051\n","Epoch 23/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.4628 - accuracy: 0.8609 - val_loss: 0.6694 - val_accuracy: 0.8179\n","Epoch 24/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.4322 - accuracy: 0.8739 - val_loss: 0.6837 - val_accuracy: 0.8051\n","Epoch 25/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4729 - accuracy: 0.8653 - val_loss: 0.6671 - val_accuracy: 0.8083\n","Epoch 26/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4083 - accuracy: 0.8881 - val_loss: 0.7282 - val_accuracy: 0.8051\n","\n","Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 27/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3878 - accuracy: 0.8949 - val_loss: 0.6159 - val_accuracy: 0.8403\n","Epoch 28/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3711 - accuracy: 0.8924 - val_loss: 0.6245 - val_accuracy: 0.8307\n","Epoch 29/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3512 - accuracy: 0.9004 - val_loss: 0.6356 - val_accuracy: 0.8051\n","Epoch 30/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3687 - accuracy: 0.9055 - val_loss: 0.6109 - val_accuracy: 0.8435\n","Epoch 31/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3552 - accuracy: 0.8996 - val_loss: 0.6169 - val_accuracy: 0.8243\n","Epoch 32/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3397 - accuracy: 0.9051 - val_loss: 0.6056 - val_accuracy: 0.8307\n","Epoch 33/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3190 - accuracy: 0.9083 - val_loss: 0.6524 - val_accuracy: 0.8147\n","Epoch 34/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3392 - accuracy: 0.9038 - val_loss: 0.5796 - val_accuracy: 0.8435\n","Epoch 35/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3174 - accuracy: 0.9076 - val_loss: 0.6217 - val_accuracy: 0.8275\n","Epoch 36/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3266 - accuracy: 0.9073 - val_loss: 0.6017 - val_accuracy: 0.8275\n","Epoch 37/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3151 - accuracy: 0.9107 - val_loss: 0.5781 - val_accuracy: 0.8147\n","Epoch 38/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3035 - accuracy: 0.9066 - val_loss: 0.6076 - val_accuracy: 0.8307\n","Epoch 39/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3012 - accuracy: 0.9146 - val_loss: 0.6113 - val_accuracy: 0.8179\n","Epoch 40/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3099 - accuracy: 0.9068 - val_loss: 0.5768 - val_accuracy: 0.8371\n","Epoch 41/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2767 - accuracy: 0.9236 - val_loss: 0.5955 - val_accuracy: 0.8403\n","Epoch 42/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.2955 - accuracy: 0.9163 - val_loss: 0.5801 - val_accuracy: 0.8435\n","Epoch 43/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2900 - accuracy: 0.9179 - val_loss: 0.6211 - val_accuracy: 0.8147\n","Epoch 44/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2713 - accuracy: 0.9269 - val_loss: 0.5882 - val_accuracy: 0.8179\n","\n","Epoch 00044: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 45/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2496 - accuracy: 0.9339 - val_loss: 0.5849 - val_accuracy: 0.8403\n","Epoch 46/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2489 - accuracy: 0.9375 - val_loss: 0.5766 - val_accuracy: 0.8339\n","Epoch 47/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2549 - accuracy: 0.9328 - val_loss: 0.5702 - val_accuracy: 0.8371\n","Epoch 48/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.2589 - accuracy: 0.9319 - val_loss: 0.5763 - val_accuracy: 0.8403\n","Epoch 49/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2525 - accuracy: 0.9361 - val_loss: 0.5591 - val_accuracy: 0.8275\n","Epoch 50/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2430 - accuracy: 0.9295 - val_loss: 0.5578 - val_accuracy: 0.8403\n","Epoch 51/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2321 - accuracy: 0.9397 - val_loss: 0.5578 - val_accuracy: 0.8403\n","Epoch 52/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2749 - accuracy: 0.9253 - val_loss: 0.5519 - val_accuracy: 0.8530\n","Epoch 53/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2283 - accuracy: 0.9464 - val_loss: 0.5622 - val_accuracy: 0.8371\n","Epoch 54/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2436 - accuracy: 0.9373 - val_loss: 0.5695 - val_accuracy: 0.8371\n","Epoch 55/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2547 - accuracy: 0.9374 - val_loss: 0.5497 - val_accuracy: 0.8403\n","Epoch 56/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2564 - accuracy: 0.9273 - val_loss: 0.5683 - val_accuracy: 0.8211\n","Epoch 57/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2316 - accuracy: 0.9357 - val_loss: 0.5660 - val_accuracy: 0.8435\n","Epoch 58/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2422 - accuracy: 0.9357 - val_loss: 0.5899 - val_accuracy: 0.8211\n","Epoch 59/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2118 - accuracy: 0.9491 - val_loss: 0.5595 - val_accuracy: 0.8371\n","\n","Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 60/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2352 - accuracy: 0.9383 - val_loss: 0.5671 - val_accuracy: 0.8371\n","Epoch 61/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2358 - accuracy: 0.9410 - val_loss: 0.5540 - val_accuracy: 0.8307\n","Epoch 62/100\n","44/44 [==============================] - 2s 46ms/step - loss: 0.2201 - accuracy: 0.9504 - val_loss: 0.5544 - val_accuracy: 0.8275\n","Epoch 63/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2114 - accuracy: 0.9467 - val_loss: 0.5531 - val_accuracy: 0.8339\n","\n","Epoch 00063: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","10/10 [==============================] - 0s 12ms/step - loss: 0.5497 - accuracy: 0.8403\n","10/10 [==============================] - 0s 7ms/step - loss: 0.5497 - accuracy: 0.8403\n","--------------------Fold_2--------------------\n","Epoch 1/100\n","44/44 [==============================] - 4s 59ms/step - loss: 3.4541 - accuracy: 0.3451 - val_loss: 3.4309 - val_accuracy: 0.2109\n","Epoch 2/100\n","44/44 [==============================] - 2s 47ms/step - loss: 2.0240 - accuracy: 0.5439 - val_loss: 1.9707 - val_accuracy: 0.5335\n","Epoch 3/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.7426 - accuracy: 0.5758 - val_loss: 2.1956 - val_accuracy: 0.5367\n","Epoch 4/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.5343 - accuracy: 0.6182 - val_loss: 1.6351 - val_accuracy: 0.5847\n","Epoch 5/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.4231 - accuracy: 0.6358 - val_loss: 1.4723 - val_accuracy: 0.6230\n","Epoch 6/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.2433 - accuracy: 0.6767 - val_loss: 1.4081 - val_accuracy: 0.6134\n","Epoch 7/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.1760 - accuracy: 0.6855 - val_loss: 1.3245 - val_accuracy: 0.6422\n","Epoch 8/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.1094 - accuracy: 0.7047 - val_loss: 1.2291 - val_accuracy: 0.6613\n","Epoch 9/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.0109 - accuracy: 0.7279 - val_loss: 1.0257 - val_accuracy: 0.7284\n","Epoch 10/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.9481 - accuracy: 0.7431 - val_loss: 0.9377 - val_accuracy: 0.7444\n","Epoch 11/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.8491 - accuracy: 0.7650 - val_loss: 0.9332 - val_accuracy: 0.7572\n","Epoch 12/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.8195 - accuracy: 0.7651 - val_loss: 0.9154 - val_accuracy: 0.7412\n","Epoch 13/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.7643 - accuracy: 0.7887 - val_loss: 0.8217 - val_accuracy: 0.7508\n","Epoch 14/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.7280 - accuracy: 0.7899 - val_loss: 0.8105 - val_accuracy: 0.7508\n","Epoch 15/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6776 - accuracy: 0.7986 - val_loss: 0.7787 - val_accuracy: 0.7764\n","Epoch 16/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6334 - accuracy: 0.8170 - val_loss: 0.7919 - val_accuracy: 0.7891\n","Epoch 17/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.6147 - accuracy: 0.8284 - val_loss: 0.7185 - val_accuracy: 0.7796\n","Epoch 18/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5805 - accuracy: 0.8386 - val_loss: 0.7502 - val_accuracy: 0.7987\n","Epoch 19/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.5347 - accuracy: 0.8562 - val_loss: 0.6442 - val_accuracy: 0.8083\n","Epoch 20/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.5123 - accuracy: 0.8448 - val_loss: 0.6569 - val_accuracy: 0.8147\n","Epoch 21/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4942 - accuracy: 0.8573 - val_loss: 0.6213 - val_accuracy: 0.8083\n","Epoch 22/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4733 - accuracy: 0.8689 - val_loss: 0.6514 - val_accuracy: 0.8179\n","Epoch 23/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4809 - accuracy: 0.8622 - val_loss: 0.6292 - val_accuracy: 0.7955\n","Epoch 24/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4405 - accuracy: 0.8782 - val_loss: 0.7224 - val_accuracy: 0.7859\n","Epoch 25/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4457 - accuracy: 0.8637 - val_loss: 0.6494 - val_accuracy: 0.8083\n","\n","Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 26/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4056 - accuracy: 0.8852 - val_loss: 0.5621 - val_accuracy: 0.8371\n","Epoch 27/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3818 - accuracy: 0.8994 - val_loss: 0.5354 - val_accuracy: 0.8339\n","Epoch 28/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3794 - accuracy: 0.8966 - val_loss: 0.5437 - val_accuracy: 0.8147\n","Epoch 29/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3732 - accuracy: 0.9003 - val_loss: 0.5349 - val_accuracy: 0.8275\n","Epoch 30/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3719 - accuracy: 0.9024 - val_loss: 0.5310 - val_accuracy: 0.8371\n","Epoch 31/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3620 - accuracy: 0.8995 - val_loss: 0.5456 - val_accuracy: 0.8339\n","Epoch 32/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3584 - accuracy: 0.8965 - val_loss: 0.5628 - val_accuracy: 0.8211\n","Epoch 33/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3590 - accuracy: 0.8965 - val_loss: 0.5611 - val_accuracy: 0.8083\n","Epoch 34/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3211 - accuracy: 0.9166 - val_loss: 0.5812 - val_accuracy: 0.8147\n","\n","Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 35/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3256 - accuracy: 0.9250 - val_loss: 0.5206 - val_accuracy: 0.8403\n","Epoch 36/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3304 - accuracy: 0.9140 - val_loss: 0.5032 - val_accuracy: 0.8562\n","Epoch 37/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3103 - accuracy: 0.9170 - val_loss: 0.5141 - val_accuracy: 0.8403\n","Epoch 38/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3288 - accuracy: 0.9120 - val_loss: 0.5128 - val_accuracy: 0.8403\n","Epoch 39/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3079 - accuracy: 0.9209 - val_loss: 0.5068 - val_accuracy: 0.8371\n","Epoch 40/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3130 - accuracy: 0.9176 - val_loss: 0.5177 - val_accuracy: 0.8403\n","\n","Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 41/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3104 - accuracy: 0.9147 - val_loss: 0.5007 - val_accuracy: 0.8403\n","Epoch 42/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3269 - accuracy: 0.9133 - val_loss: 0.4974 - val_accuracy: 0.8498\n","Epoch 43/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2882 - accuracy: 0.9284 - val_loss: 0.4916 - val_accuracy: 0.8466\n","Epoch 44/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3138 - accuracy: 0.9189 - val_loss: 0.4986 - val_accuracy: 0.8307\n","Epoch 45/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3060 - accuracy: 0.9260 - val_loss: 0.5001 - val_accuracy: 0.8403\n","Epoch 46/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2967 - accuracy: 0.9233 - val_loss: 0.5026 - val_accuracy: 0.8403\n","Epoch 47/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2975 - accuracy: 0.9273 - val_loss: 0.4982 - val_accuracy: 0.8403\n","\n","Epoch 00047: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","Epoch 48/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2772 - accuracy: 0.9302 - val_loss: 0.4906 - val_accuracy: 0.8403\n","Epoch 49/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3049 - accuracy: 0.9194 - val_loss: 0.4916 - val_accuracy: 0.8498\n","Epoch 50/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2746 - accuracy: 0.9292 - val_loss: 0.4956 - val_accuracy: 0.8339\n","Epoch 51/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2841 - accuracy: 0.9306 - val_loss: 0.4827 - val_accuracy: 0.8498\n","Epoch 52/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2759 - accuracy: 0.9303 - val_loss: 0.4849 - val_accuracy: 0.8403\n","Epoch 53/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2766 - accuracy: 0.9336 - val_loss: 0.4863 - val_accuracy: 0.8530\n","Epoch 54/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2665 - accuracy: 0.9363 - val_loss: 0.4870 - val_accuracy: 0.8498\n","Epoch 55/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2748 - accuracy: 0.9297 - val_loss: 0.4806 - val_accuracy: 0.8530\n","Epoch 56/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2686 - accuracy: 0.9383 - val_loss: 0.4924 - val_accuracy: 0.8530\n","Epoch 57/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2659 - accuracy: 0.9360 - val_loss: 0.4829 - val_accuracy: 0.8435\n","Epoch 58/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2854 - accuracy: 0.9235 - val_loss: 0.4805 - val_accuracy: 0.8530\n","Epoch 59/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2844 - accuracy: 0.9243 - val_loss: 0.4834 - val_accuracy: 0.8498\n","\n","Epoch 00059: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","Epoch 60/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2717 - accuracy: 0.9298 - val_loss: 0.4819 - val_accuracy: 0.8403\n","Epoch 61/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2742 - accuracy: 0.9253 - val_loss: 0.4805 - val_accuracy: 0.8435\n","Epoch 62/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2723 - accuracy: 0.9272 - val_loss: 0.4808 - val_accuracy: 0.8435\n","Epoch 63/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2563 - accuracy: 0.9443 - val_loss: 0.4787 - val_accuracy: 0.8466\n","Epoch 64/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2755 - accuracy: 0.9360 - val_loss: 0.4832 - val_accuracy: 0.8403\n","Epoch 65/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2705 - accuracy: 0.9263 - val_loss: 0.4822 - val_accuracy: 0.8466\n","Epoch 66/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2871 - accuracy: 0.9190 - val_loss: 0.4849 - val_accuracy: 0.8435\n","Epoch 67/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2831 - accuracy: 0.9334 - val_loss: 0.4811 - val_accuracy: 0.8466\n","\n","Epoch 00067: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n","Epoch 68/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2602 - accuracy: 0.9374 - val_loss: 0.4820 - val_accuracy: 0.8403\n","Epoch 69/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2760 - accuracy: 0.9322 - val_loss: 0.4802 - val_accuracy: 0.8403\n","Epoch 70/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2671 - accuracy: 0.9388 - val_loss: 0.4789 - val_accuracy: 0.8435\n","Epoch 71/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2668 - accuracy: 0.9346 - val_loss: 0.4779 - val_accuracy: 0.8435\n","Epoch 72/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2715 - accuracy: 0.9315 - val_loss: 0.4819 - val_accuracy: 0.8466\n","Epoch 73/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2651 - accuracy: 0.9385 - val_loss: 0.4804 - val_accuracy: 0.8403\n","Epoch 74/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2659 - accuracy: 0.9320 - val_loss: 0.4783 - val_accuracy: 0.8403\n","Epoch 75/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2825 - accuracy: 0.9289 - val_loss: 0.4787 - val_accuracy: 0.8403\n","\n","Epoch 00075: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n","Epoch 76/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2785 - accuracy: 0.9284 - val_loss: 0.4792 - val_accuracy: 0.8435\n","Epoch 77/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2649 - accuracy: 0.9357 - val_loss: 0.4793 - val_accuracy: 0.8435\n","Epoch 78/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2797 - accuracy: 0.9266 - val_loss: 0.4782 - val_accuracy: 0.8435\n","Epoch 79/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2813 - accuracy: 0.9318 - val_loss: 0.4770 - val_accuracy: 0.8435\n","Epoch 80/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2668 - accuracy: 0.9344 - val_loss: 0.4795 - val_accuracy: 0.8466\n","Epoch 81/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2739 - accuracy: 0.9336 - val_loss: 0.4810 - val_accuracy: 0.8466\n","Epoch 82/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2751 - accuracy: 0.9300 - val_loss: 0.4797 - val_accuracy: 0.8435\n","Epoch 83/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2809 - accuracy: 0.9253 - val_loss: 0.4807 - val_accuracy: 0.8466\n","\n","Epoch 00083: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n","Epoch 84/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2700 - accuracy: 0.9345 - val_loss: 0.4813 - val_accuracy: 0.8435\n","Epoch 85/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2517 - accuracy: 0.9370 - val_loss: 0.4805 - val_accuracy: 0.8435\n","Epoch 86/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2796 - accuracy: 0.9265 - val_loss: 0.4815 - val_accuracy: 0.8466\n","Epoch 87/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2567 - accuracy: 0.9371 - val_loss: 0.4812 - val_accuracy: 0.8466\n","\n","Epoch 00087: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n","10/10 [==============================] - 0s 11ms/step - loss: 0.4770 - accuracy: 0.8435\n","10/10 [==============================] - 0s 11ms/step - loss: 0.4770 - accuracy: 0.8435\n","--------------------Fold_3--------------------\n","Epoch 1/100\n","44/44 [==============================] - 4s 59ms/step - loss: 3.4150 - accuracy: 0.3436 - val_loss: 3.2790 - val_accuracy: 0.2236\n","Epoch 2/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.9913 - accuracy: 0.5508 - val_loss: 1.9606 - val_accuracy: 0.5431\n","Epoch 3/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.7131 - accuracy: 0.5902 - val_loss: 1.8425 - val_accuracy: 0.5527\n","Epoch 4/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.5436 - accuracy: 0.6168 - val_loss: 1.5751 - val_accuracy: 0.5942\n","Epoch 5/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.3887 - accuracy: 0.6487 - val_loss: 1.6013 - val_accuracy: 0.6070\n","Epoch 6/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.2394 - accuracy: 0.6739 - val_loss: 1.3152 - val_accuracy: 0.6358\n","Epoch 7/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.1680 - accuracy: 0.7011 - val_loss: 1.1443 - val_accuracy: 0.6869\n","Epoch 8/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.0632 - accuracy: 0.7161 - val_loss: 1.0918 - val_accuracy: 0.6997\n","Epoch 9/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.0478 - accuracy: 0.7145 - val_loss: 1.0723 - val_accuracy: 0.7157\n","Epoch 10/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.9038 - accuracy: 0.7564 - val_loss: 0.9611 - val_accuracy: 0.7412\n","Epoch 11/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.8667 - accuracy: 0.7480 - val_loss: 0.8836 - val_accuracy: 0.7668\n","Epoch 12/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.7923 - accuracy: 0.7804 - val_loss: 0.8631 - val_accuracy: 0.7732\n","Epoch 13/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.7476 - accuracy: 0.7910 - val_loss: 0.7952 - val_accuracy: 0.7827\n","Epoch 14/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.7260 - accuracy: 0.7966 - val_loss: 0.8368 - val_accuracy: 0.7764\n","Epoch 15/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6583 - accuracy: 0.8124 - val_loss: 0.7927 - val_accuracy: 0.7732\n","Epoch 16/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.6140 - accuracy: 0.8216 - val_loss: 0.7339 - val_accuracy: 0.7764\n","Epoch 17/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6141 - accuracy: 0.8159 - val_loss: 0.7578 - val_accuracy: 0.7827\n","Epoch 18/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5713 - accuracy: 0.8351 - val_loss: 0.6863 - val_accuracy: 0.7955\n","Epoch 19/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5430 - accuracy: 0.8586 - val_loss: 0.6741 - val_accuracy: 0.8211\n","Epoch 20/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5120 - accuracy: 0.8432 - val_loss: 0.6816 - val_accuracy: 0.8115\n","Epoch 21/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5081 - accuracy: 0.8526 - val_loss: 0.7386 - val_accuracy: 0.8115\n","Epoch 22/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4767 - accuracy: 0.8691 - val_loss: 0.6431 - val_accuracy: 0.8147\n","Epoch 23/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4853 - accuracy: 0.8627 - val_loss: 0.5825 - val_accuracy: 0.8307\n","Epoch 24/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4737 - accuracy: 0.8533 - val_loss: 0.6286 - val_accuracy: 0.8115\n","Epoch 25/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4422 - accuracy: 0.8649 - val_loss: 0.5760 - val_accuracy: 0.8051\n","Epoch 26/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4147 - accuracy: 0.8747 - val_loss: 0.5535 - val_accuracy: 0.8371\n","Epoch 27/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4381 - accuracy: 0.8716 - val_loss: 0.5271 - val_accuracy: 0.8339\n","Epoch 28/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4075 - accuracy: 0.8783 - val_loss: 0.5605 - val_accuracy: 0.8307\n","Epoch 29/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3858 - accuracy: 0.8884 - val_loss: 0.5418 - val_accuracy: 0.8530\n","Epoch 30/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3914 - accuracy: 0.8825 - val_loss: 0.6854 - val_accuracy: 0.7827\n","Epoch 31/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3769 - accuracy: 0.8889 - val_loss: 0.5045 - val_accuracy: 0.8466\n","Epoch 32/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3559 - accuracy: 0.9053 - val_loss: 0.5483 - val_accuracy: 0.8371\n","Epoch 33/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3673 - accuracy: 0.8871 - val_loss: 0.6222 - val_accuracy: 0.7764\n","Epoch 34/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3481 - accuracy: 0.8922 - val_loss: 0.6340 - val_accuracy: 0.8115\n","Epoch 35/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3710 - accuracy: 0.8911 - val_loss: 0.5511 - val_accuracy: 0.8371\n","\n","Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 36/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3035 - accuracy: 0.9146 - val_loss: 0.5051 - val_accuracy: 0.8243\n","Epoch 37/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2839 - accuracy: 0.9246 - val_loss: 0.4869 - val_accuracy: 0.8498\n","Epoch 38/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2873 - accuracy: 0.9136 - val_loss: 0.4915 - val_accuracy: 0.8403\n","Epoch 39/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2766 - accuracy: 0.9159 - val_loss: 0.4807 - val_accuracy: 0.8339\n","Epoch 40/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2912 - accuracy: 0.9075 - val_loss: 0.5154 - val_accuracy: 0.8339\n","Epoch 41/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2838 - accuracy: 0.9224 - val_loss: 0.4611 - val_accuracy: 0.8562\n","Epoch 42/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2864 - accuracy: 0.9209 - val_loss: 0.5057 - val_accuracy: 0.8115\n","Epoch 43/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2812 - accuracy: 0.9265 - val_loss: 0.5441 - val_accuracy: 0.8339\n","Epoch 44/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2675 - accuracy: 0.9212 - val_loss: 0.4607 - val_accuracy: 0.8754\n","Epoch 45/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2530 - accuracy: 0.9313 - val_loss: 0.4634 - val_accuracy: 0.8562\n","Epoch 46/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2612 - accuracy: 0.9211 - val_loss: 0.4469 - val_accuracy: 0.8466\n","Epoch 47/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2411 - accuracy: 0.9307 - val_loss: 0.4880 - val_accuracy: 0.8371\n","Epoch 48/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2616 - accuracy: 0.9253 - val_loss: 0.4468 - val_accuracy: 0.8722\n","Epoch 49/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2514 - accuracy: 0.9325 - val_loss: 0.4643 - val_accuracy: 0.8594\n","Epoch 50/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2444 - accuracy: 0.9300 - val_loss: 0.4673 - val_accuracy: 0.8498\n","Epoch 51/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2419 - accuracy: 0.9320 - val_loss: 0.4708 - val_accuracy: 0.8498\n","Epoch 52/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2735 - accuracy: 0.9193 - val_loss: 0.4440 - val_accuracy: 0.8562\n","Epoch 53/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2437 - accuracy: 0.9312 - val_loss: 0.4416 - val_accuracy: 0.8658\n","Epoch 54/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2511 - accuracy: 0.9311 - val_loss: 0.4671 - val_accuracy: 0.8371\n","Epoch 55/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2344 - accuracy: 0.9381 - val_loss: 0.4686 - val_accuracy: 0.8435\n","Epoch 56/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2422 - accuracy: 0.9357 - val_loss: 0.4475 - val_accuracy: 0.8498\n","Epoch 57/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2275 - accuracy: 0.9319 - val_loss: 0.4914 - val_accuracy: 0.8371\n","\n","Epoch 00057: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 58/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2114 - accuracy: 0.9442 - val_loss: 0.4538 - val_accuracy: 0.8530\n","Epoch 59/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2008 - accuracy: 0.9443 - val_loss: 0.4311 - val_accuracy: 0.8594\n","Epoch 60/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2157 - accuracy: 0.9423 - val_loss: 0.4303 - val_accuracy: 0.8690\n","Epoch 61/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1861 - accuracy: 0.9572 - val_loss: 0.4213 - val_accuracy: 0.8594\n","Epoch 62/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2137 - accuracy: 0.9410 - val_loss: 0.4368 - val_accuracy: 0.8562\n","Epoch 63/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2047 - accuracy: 0.9446 - val_loss: 0.4231 - val_accuracy: 0.8850\n","Epoch 64/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1721 - accuracy: 0.9535 - val_loss: 0.4327 - val_accuracy: 0.8658\n","Epoch 65/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2028 - accuracy: 0.9475 - val_loss: 0.4217 - val_accuracy: 0.8626\n","\n","Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 66/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1989 - accuracy: 0.9491 - val_loss: 0.4292 - val_accuracy: 0.8658\n","Epoch 67/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1793 - accuracy: 0.9477 - val_loss: 0.4295 - val_accuracy: 0.8562\n","Epoch 68/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1781 - accuracy: 0.9572 - val_loss: 0.4212 - val_accuracy: 0.8722\n","Epoch 69/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1677 - accuracy: 0.9539 - val_loss: 0.4266 - val_accuracy: 0.8722\n","\n","Epoch 00069: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","Epoch 70/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1831 - accuracy: 0.9538 - val_loss: 0.4144 - val_accuracy: 0.8786\n","Epoch 71/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1711 - accuracy: 0.9596 - val_loss: 0.4165 - val_accuracy: 0.8722\n","Epoch 72/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1741 - accuracy: 0.9537 - val_loss: 0.4138 - val_accuracy: 0.8786\n","Epoch 73/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1760 - accuracy: 0.9587 - val_loss: 0.4133 - val_accuracy: 0.8754\n","Epoch 74/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1774 - accuracy: 0.9612 - val_loss: 0.4154 - val_accuracy: 0.8626\n","Epoch 75/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1782 - accuracy: 0.9520 - val_loss: 0.4138 - val_accuracy: 0.8658\n","Epoch 76/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1685 - accuracy: 0.9596 - val_loss: 0.4179 - val_accuracy: 0.8690\n","Epoch 77/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1682 - accuracy: 0.9599 - val_loss: 0.4160 - val_accuracy: 0.8722\n","\n","Epoch 00077: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","Epoch 78/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1822 - accuracy: 0.9530 - val_loss: 0.4137 - val_accuracy: 0.8690\n","Epoch 79/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1587 - accuracy: 0.9692 - val_loss: 0.4119 - val_accuracy: 0.8690\n","Epoch 80/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1703 - accuracy: 0.9623 - val_loss: 0.4159 - val_accuracy: 0.8722\n","Epoch 81/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1813 - accuracy: 0.9545 - val_loss: 0.4153 - val_accuracy: 0.8690\n","Epoch 82/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1713 - accuracy: 0.9596 - val_loss: 0.4153 - val_accuracy: 0.8594\n","Epoch 83/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1711 - accuracy: 0.9561 - val_loss: 0.4160 - val_accuracy: 0.8658\n","\n","Epoch 00083: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n","Epoch 84/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1737 - accuracy: 0.9582 - val_loss: 0.4184 - val_accuracy: 0.8626\n","Epoch 85/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1682 - accuracy: 0.9561 - val_loss: 0.4158 - val_accuracy: 0.8690\n","Epoch 86/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1682 - accuracy: 0.9576 - val_loss: 0.4143 - val_accuracy: 0.8690\n","Epoch 87/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.1655 - accuracy: 0.9556 - val_loss: 0.4147 - val_accuracy: 0.8722\n","\n","Epoch 00087: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n","10/10 [==============================] - 0s 8ms/step - loss: 0.4119 - accuracy: 0.8690\n","10/10 [==============================] - 0s 8ms/step - loss: 0.4119 - accuracy: 0.8690\n","--------------------Fold_4--------------------\n","Epoch 1/100\n","44/44 [==============================] - 4s 59ms/step - loss: 3.4232 - accuracy: 0.3610 - val_loss: 3.2335 - val_accuracy: 0.2907\n","Epoch 2/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.9774 - accuracy: 0.5573 - val_loss: 2.0495 - val_accuracy: 0.5304\n","Epoch 3/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.7399 - accuracy: 0.5853 - val_loss: 1.9862 - val_accuracy: 0.5335\n","Epoch 4/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.5558 - accuracy: 0.6199 - val_loss: 2.0006 - val_accuracy: 0.5335\n","Epoch 5/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.3662 - accuracy: 0.6504 - val_loss: 1.7003 - val_accuracy: 0.5751\n","Epoch 6/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.2757 - accuracy: 0.6614 - val_loss: 1.4575 - val_accuracy: 0.6390\n","Epoch 7/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.1692 - accuracy: 0.6972 - val_loss: 1.3515 - val_accuracy: 0.6454\n","Epoch 8/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.1069 - accuracy: 0.7087 - val_loss: 1.4344 - val_accuracy: 0.6294\n","Epoch 9/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.9822 - accuracy: 0.7316 - val_loss: 1.1864 - val_accuracy: 0.6837\n","Epoch 10/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.9001 - accuracy: 0.7426 - val_loss: 1.1137 - val_accuracy: 0.7188\n","Epoch 11/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.8437 - accuracy: 0.7678 - val_loss: 1.0515 - val_accuracy: 0.7125\n","Epoch 12/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.7814 - accuracy: 0.7830 - val_loss: 1.0819 - val_accuracy: 0.7220\n","Epoch 13/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.7288 - accuracy: 0.7908 - val_loss: 1.0537 - val_accuracy: 0.7188\n","Epoch 14/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.6876 - accuracy: 0.7987 - val_loss: 1.0404 - val_accuracy: 0.7284\n","Epoch 15/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6317 - accuracy: 0.8246 - val_loss: 1.1145 - val_accuracy: 0.7284\n","Epoch 16/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6167 - accuracy: 0.8263 - val_loss: 0.8656 - val_accuracy: 0.7540\n","Epoch 17/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5861 - accuracy: 0.8290 - val_loss: 0.8310 - val_accuracy: 0.7572\n","Epoch 18/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5679 - accuracy: 0.8324 - val_loss: 0.8942 - val_accuracy: 0.7636\n","Epoch 19/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.5645 - accuracy: 0.8364 - val_loss: 0.8535 - val_accuracy: 0.7604\n","Epoch 20/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5357 - accuracy: 0.8443 - val_loss: 0.8152 - val_accuracy: 0.7827\n","Epoch 21/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5029 - accuracy: 0.8596 - val_loss: 0.7862 - val_accuracy: 0.7891\n","Epoch 22/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4636 - accuracy: 0.8685 - val_loss: 0.8786 - val_accuracy: 0.7604\n","Epoch 23/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5147 - accuracy: 0.8396 - val_loss: 0.7991 - val_accuracy: 0.7732\n","Epoch 24/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4471 - accuracy: 0.8724 - val_loss: 0.7644 - val_accuracy: 0.7859\n","Epoch 25/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4417 - accuracy: 0.8738 - val_loss: 0.7996 - val_accuracy: 0.7604\n","Epoch 26/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4071 - accuracy: 0.8886 - val_loss: 0.8443 - val_accuracy: 0.7540\n","Epoch 27/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4302 - accuracy: 0.8783 - val_loss: 0.8280 - val_accuracy: 0.7540\n","Epoch 28/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4139 - accuracy: 0.8734 - val_loss: 0.7505 - val_accuracy: 0.7764\n","Epoch 29/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3879 - accuracy: 0.8879 - val_loss: 0.8810 - val_accuracy: 0.7476\n","Epoch 30/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4007 - accuracy: 0.8834 - val_loss: 0.7918 - val_accuracy: 0.7987\n","Epoch 31/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3914 - accuracy: 0.8854 - val_loss: 0.7799 - val_accuracy: 0.7859\n","Epoch 32/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3689 - accuracy: 0.8960 - val_loss: 0.7443 - val_accuracy: 0.7796\n","Epoch 33/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3739 - accuracy: 0.8861 - val_loss: 0.7490 - val_accuracy: 0.7572\n","Epoch 34/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3433 - accuracy: 0.8994 - val_loss: 0.7715 - val_accuracy: 0.7891\n","Epoch 35/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3651 - accuracy: 0.8946 - val_loss: 0.6782 - val_accuracy: 0.8019\n","Epoch 36/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3162 - accuracy: 0.9075 - val_loss: 0.7006 - val_accuracy: 0.7859\n","Epoch 37/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3209 - accuracy: 0.9040 - val_loss: 0.7750 - val_accuracy: 0.7796\n","Epoch 38/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3029 - accuracy: 0.9201 - val_loss: 0.8985 - val_accuracy: 0.7508\n","Epoch 39/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2938 - accuracy: 0.9140 - val_loss: 0.6932 - val_accuracy: 0.8083\n","\n","Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 40/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2871 - accuracy: 0.9186 - val_loss: 0.6587 - val_accuracy: 0.8211\n","Epoch 41/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2490 - accuracy: 0.9318 - val_loss: 0.6462 - val_accuracy: 0.8115\n","Epoch 42/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2546 - accuracy: 0.9306 - val_loss: 0.6672 - val_accuracy: 0.7923\n","Epoch 43/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2663 - accuracy: 0.9291 - val_loss: 0.6514 - val_accuracy: 0.8051\n","Epoch 44/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2687 - accuracy: 0.9264 - val_loss: 0.6633 - val_accuracy: 0.8051\n","Epoch 45/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2406 - accuracy: 0.9271 - val_loss: 0.7041 - val_accuracy: 0.7859\n","\n","Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 46/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2326 - accuracy: 0.9356 - val_loss: 0.6615 - val_accuracy: 0.7955\n","Epoch 47/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2218 - accuracy: 0.9423 - val_loss: 0.6455 - val_accuracy: 0.8051\n","Epoch 48/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2281 - accuracy: 0.9394 - val_loss: 0.6664 - val_accuracy: 0.7987\n","Epoch 49/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2305 - accuracy: 0.9400 - val_loss: 0.6406 - val_accuracy: 0.8147\n","Epoch 50/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2206 - accuracy: 0.9393 - val_loss: 0.6680 - val_accuracy: 0.7891\n","Epoch 51/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2133 - accuracy: 0.9481 - val_loss: 0.6448 - val_accuracy: 0.8115\n","Epoch 52/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2116 - accuracy: 0.9475 - val_loss: 0.6554 - val_accuracy: 0.8051\n","Epoch 53/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2182 - accuracy: 0.9416 - val_loss: 0.6650 - val_accuracy: 0.7987\n","\n","Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 54/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1982 - accuracy: 0.9468 - val_loss: 0.6377 - val_accuracy: 0.8115\n","Epoch 55/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1997 - accuracy: 0.9541 - val_loss: 0.6309 - val_accuracy: 0.8147\n","Epoch 56/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1955 - accuracy: 0.9476 - val_loss: 0.6330 - val_accuracy: 0.8051\n","Epoch 57/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1944 - accuracy: 0.9513 - val_loss: 0.6421 - val_accuracy: 0.8083\n","Epoch 58/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1958 - accuracy: 0.9486 - val_loss: 0.6497 - val_accuracy: 0.8019\n","Epoch 59/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2038 - accuracy: 0.9443 - val_loss: 0.6506 - val_accuracy: 0.8051\n","\n","Epoch 00059: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","Epoch 60/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2043 - accuracy: 0.9513 - val_loss: 0.6320 - val_accuracy: 0.8147\n","Epoch 61/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2026 - accuracy: 0.9473 - val_loss: 0.6322 - val_accuracy: 0.8147\n","Epoch 62/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1887 - accuracy: 0.9532 - val_loss: 0.6497 - val_accuracy: 0.7987\n","Epoch 63/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2095 - accuracy: 0.9436 - val_loss: 0.6311 - val_accuracy: 0.8051\n","\n","Epoch 00063: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","10/10 [==============================] - 0s 8ms/step - loss: 0.6309 - accuracy: 0.8147\n","10/10 [==============================] - 0s 8ms/step - loss: 0.6309 - accuracy: 0.8147\n","--------------------Fold_5--------------------\n","Epoch 1/100\n","44/44 [==============================] - 4s 59ms/step - loss: 3.4747 - accuracy: 0.3518 - val_loss: 3.6607 - val_accuracy: 0.1565\n","Epoch 2/100\n","44/44 [==============================] - 2s 48ms/step - loss: 2.0759 - accuracy: 0.5332 - val_loss: 2.0282 - val_accuracy: 0.5304\n","Epoch 3/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.7434 - accuracy: 0.5907 - val_loss: 2.2032 - val_accuracy: 0.5176\n","Epoch 4/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.5838 - accuracy: 0.6216 - val_loss: 1.7128 - val_accuracy: 0.5623\n","Epoch 5/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.3903 - accuracy: 0.6482 - val_loss: 1.5758 - val_accuracy: 0.5942\n","Epoch 6/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.2734 - accuracy: 0.6804 - val_loss: 1.4211 - val_accuracy: 0.6198\n","Epoch 7/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.1404 - accuracy: 0.7156 - val_loss: 1.2961 - val_accuracy: 0.6550\n","Epoch 8/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.0909 - accuracy: 0.7036 - val_loss: 1.1861 - val_accuracy: 0.6869\n","Epoch 9/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.9748 - accuracy: 0.7465 - val_loss: 1.1135 - val_accuracy: 0.7188\n","Epoch 10/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.9371 - accuracy: 0.7447 - val_loss: 1.0400 - val_accuracy: 0.7220\n","Epoch 11/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.8628 - accuracy: 0.7718 - val_loss: 1.0082 - val_accuracy: 0.7125\n","Epoch 12/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.8131 - accuracy: 0.7755 - val_loss: 0.9027 - val_accuracy: 0.7444\n","Epoch 13/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.7363 - accuracy: 0.8005 - val_loss: 0.8208 - val_accuracy: 0.7604\n","Epoch 14/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.7017 - accuracy: 0.8011 - val_loss: 0.8073 - val_accuracy: 0.7604\n","Epoch 15/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6585 - accuracy: 0.8231 - val_loss: 0.7679 - val_accuracy: 0.7604\n","Epoch 16/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6502 - accuracy: 0.8054 - val_loss: 0.7178 - val_accuracy: 0.7891\n","Epoch 17/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5776 - accuracy: 0.8323 - val_loss: 0.7774 - val_accuracy: 0.7636\n","Epoch 18/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5664 - accuracy: 0.8292 - val_loss: 0.8057 - val_accuracy: 0.7923\n","Epoch 19/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.5481 - accuracy: 0.8420 - val_loss: 0.6939 - val_accuracy: 0.8051\n","Epoch 20/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5439 - accuracy: 0.8334 - val_loss: 0.6715 - val_accuracy: 0.8179\n","Epoch 21/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5010 - accuracy: 0.8506 - val_loss: 0.6509 - val_accuracy: 0.8051\n","Epoch 22/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5204 - accuracy: 0.8482 - val_loss: 0.7131 - val_accuracy: 0.8019\n","Epoch 23/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4786 - accuracy: 0.8487 - val_loss: 0.6481 - val_accuracy: 0.8019\n","Epoch 24/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4843 - accuracy: 0.8577 - val_loss: 0.6184 - val_accuracy: 0.7891\n","Epoch 25/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4662 - accuracy: 0.8704 - val_loss: 0.6268 - val_accuracy: 0.8051\n","Epoch 26/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4692 - accuracy: 0.8578 - val_loss: 0.6363 - val_accuracy: 0.7827\n","Epoch 27/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4217 - accuracy: 0.8720 - val_loss: 0.6216 - val_accuracy: 0.7987\n","Epoch 28/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4059 - accuracy: 0.8761 - val_loss: 0.6557 - val_accuracy: 0.7891\n","\n","Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 29/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3956 - accuracy: 0.8777 - val_loss: 0.5569 - val_accuracy: 0.8051\n","Epoch 30/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3512 - accuracy: 0.9031 - val_loss: 0.5973 - val_accuracy: 0.7923\n","Epoch 31/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3753 - accuracy: 0.8931 - val_loss: 0.5224 - val_accuracy: 0.8275\n","Epoch 32/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3646 - accuracy: 0.8998 - val_loss: 0.5259 - val_accuracy: 0.8275\n","Epoch 33/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3358 - accuracy: 0.9046 - val_loss: 0.5315 - val_accuracy: 0.8371\n","Epoch 34/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3316 - accuracy: 0.9060 - val_loss: 0.5649 - val_accuracy: 0.8243\n","Epoch 35/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3282 - accuracy: 0.9078 - val_loss: 0.6240 - val_accuracy: 0.7891\n","\n","Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 36/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3275 - accuracy: 0.9081 - val_loss: 0.5134 - val_accuracy: 0.8339\n","Epoch 37/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3079 - accuracy: 0.9177 - val_loss: 0.5288 - val_accuracy: 0.8307\n","Epoch 38/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.3174 - accuracy: 0.9101 - val_loss: 0.5236 - val_accuracy: 0.8371\n","Epoch 39/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3133 - accuracy: 0.9172 - val_loss: 0.5303 - val_accuracy: 0.8307\n","Epoch 40/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.3054 - accuracy: 0.9096 - val_loss: 0.5139 - val_accuracy: 0.8466\n","\n","Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 41/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2979 - accuracy: 0.9208 - val_loss: 0.5094 - val_accuracy: 0.8435\n","Epoch 42/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2764 - accuracy: 0.9288 - val_loss: 0.4940 - val_accuracy: 0.8466\n","Epoch 43/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2919 - accuracy: 0.9221 - val_loss: 0.4894 - val_accuracy: 0.8562\n","Epoch 44/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2804 - accuracy: 0.9234 - val_loss: 0.5039 - val_accuracy: 0.8371\n","Epoch 45/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2827 - accuracy: 0.9276 - val_loss: 0.4949 - val_accuracy: 0.8498\n","Epoch 46/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2813 - accuracy: 0.9257 - val_loss: 0.4885 - val_accuracy: 0.8435\n","Epoch 47/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2599 - accuracy: 0.9352 - val_loss: 0.4969 - val_accuracy: 0.8530\n","Epoch 48/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2676 - accuracy: 0.9234 - val_loss: 0.4984 - val_accuracy: 0.8466\n","Epoch 49/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2512 - accuracy: 0.9370 - val_loss: 0.4960 - val_accuracy: 0.8435\n","Epoch 50/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3016 - accuracy: 0.9226 - val_loss: 0.4916 - val_accuracy: 0.8498\n","\n","Epoch 00050: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","Epoch 51/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2640 - accuracy: 0.9292 - val_loss: 0.4872 - val_accuracy: 0.8530\n","Epoch 52/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2758 - accuracy: 0.9273 - val_loss: 0.4893 - val_accuracy: 0.8530\n","Epoch 53/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2769 - accuracy: 0.9252 - val_loss: 0.4731 - val_accuracy: 0.8530\n","Epoch 54/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2761 - accuracy: 0.9306 - val_loss: 0.4815 - val_accuracy: 0.8530\n","Epoch 55/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2579 - accuracy: 0.9347 - val_loss: 0.4799 - val_accuracy: 0.8594\n","Epoch 56/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2648 - accuracy: 0.9350 - val_loss: 0.4740 - val_accuracy: 0.8658\n","Epoch 57/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2615 - accuracy: 0.9308 - val_loss: 0.4724 - val_accuracy: 0.8626\n","Epoch 58/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2718 - accuracy: 0.9298 - val_loss: 0.4887 - val_accuracy: 0.8466\n","Epoch 59/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2494 - accuracy: 0.9311 - val_loss: 0.4894 - val_accuracy: 0.8435\n","Epoch 60/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2409 - accuracy: 0.9401 - val_loss: 0.4786 - val_accuracy: 0.8371\n","Epoch 61/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2616 - accuracy: 0.9317 - val_loss: 0.4720 - val_accuracy: 0.8530\n","Epoch 62/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2521 - accuracy: 0.9318 - val_loss: 0.4769 - val_accuracy: 0.8594\n","Epoch 63/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2542 - accuracy: 0.9319 - val_loss: 0.4877 - val_accuracy: 0.8530\n","Epoch 64/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2541 - accuracy: 0.9363 - val_loss: 0.4825 - val_accuracy: 0.8562\n","Epoch 65/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2528 - accuracy: 0.9333 - val_loss: 0.4862 - val_accuracy: 0.8498\n","\n","Epoch 00065: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","Epoch 66/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2411 - accuracy: 0.9395 - val_loss: 0.4897 - val_accuracy: 0.8562\n","Epoch 67/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2450 - accuracy: 0.9403 - val_loss: 0.4816 - val_accuracy: 0.8498\n","Epoch 68/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2606 - accuracy: 0.9335 - val_loss: 0.4788 - val_accuracy: 0.8466\n","Epoch 69/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2521 - accuracy: 0.9337 - val_loss: 0.4840 - val_accuracy: 0.8530\n","\n","Epoch 00069: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n","10/10 [==============================] - 0s 11ms/step - loss: 0.4720 - accuracy: 0.8530\n","10/10 [==============================] - 0s 10ms/step - loss: 0.4720 - accuracy: 0.8530\n","--------------------Fold_6--------------------\n","Epoch 1/100\n","44/44 [==============================] - 4s 64ms/step - loss: 3.4718 - accuracy: 0.3595 - val_loss: 3.4006 - val_accuracy: 0.1667\n","Epoch 2/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.9992 - accuracy: 0.5541 - val_loss: 2.0690 - val_accuracy: 0.5385\n","Epoch 3/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.8187 - accuracy: 0.5662 - val_loss: 2.0751 - val_accuracy: 0.5417\n","Epoch 4/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.5366 - accuracy: 0.6219 - val_loss: 1.9643 - val_accuracy: 0.5481\n","Epoch 5/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.4388 - accuracy: 0.6335 - val_loss: 1.6828 - val_accuracy: 0.5929\n","Epoch 6/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.2457 - accuracy: 0.6758 - val_loss: 1.6260 - val_accuracy: 0.5833\n","Epoch 7/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.1931 - accuracy: 0.6795 - val_loss: 1.4538 - val_accuracy: 0.6058\n","Epoch 8/100\n","44/44 [==============================] - 2s 50ms/step - loss: 1.0453 - accuracy: 0.7122 - val_loss: 1.2659 - val_accuracy: 0.6827\n","Epoch 9/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.9866 - accuracy: 0.7415 - val_loss: 1.2026 - val_accuracy: 0.6859\n","Epoch 10/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.9405 - accuracy: 0.7409 - val_loss: 1.1350 - val_accuracy: 0.7179\n","Epoch 11/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.8794 - accuracy: 0.7504 - val_loss: 1.1076 - val_accuracy: 0.7115\n","Epoch 12/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.8117 - accuracy: 0.7770 - val_loss: 1.0286 - val_accuracy: 0.7083\n","Epoch 13/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.7721 - accuracy: 0.7774 - val_loss: 1.0109 - val_accuracy: 0.7372\n","Epoch 14/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.7003 - accuracy: 0.8021 - val_loss: 0.9383 - val_accuracy: 0.7404\n","Epoch 15/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.6837 - accuracy: 0.7901 - val_loss: 0.8643 - val_accuracy: 0.7564\n","Epoch 16/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6576 - accuracy: 0.7965 - val_loss: 0.8664 - val_accuracy: 0.7532\n","Epoch 17/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5914 - accuracy: 0.8316 - val_loss: 0.8761 - val_accuracy: 0.7532\n","Epoch 18/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5488 - accuracy: 0.8376 - val_loss: 0.8216 - val_accuracy: 0.7596\n","Epoch 19/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5663 - accuracy: 0.8328 - val_loss: 0.7924 - val_accuracy: 0.7660\n","Epoch 20/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5418 - accuracy: 0.8403 - val_loss: 0.7404 - val_accuracy: 0.7821\n","Epoch 21/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5529 - accuracy: 0.8329 - val_loss: 0.8666 - val_accuracy: 0.7660\n","Epoch 22/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5221 - accuracy: 0.8384 - val_loss: 0.7188 - val_accuracy: 0.8109\n","Epoch 23/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4644 - accuracy: 0.8573 - val_loss: 0.7281 - val_accuracy: 0.7885\n","Epoch 24/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4515 - accuracy: 0.8609 - val_loss: 0.7196 - val_accuracy: 0.7885\n","Epoch 25/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4333 - accuracy: 0.8756 - val_loss: 0.7300 - val_accuracy: 0.7853\n","Epoch 26/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4402 - accuracy: 0.8647 - val_loss: 0.7152 - val_accuracy: 0.7949\n","Epoch 27/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4277 - accuracy: 0.8780 - val_loss: 0.7052 - val_accuracy: 0.7949\n","Epoch 28/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4244 - accuracy: 0.8658 - val_loss: 0.6745 - val_accuracy: 0.8173\n","Epoch 29/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4325 - accuracy: 0.8578 - val_loss: 0.6970 - val_accuracy: 0.7885\n","Epoch 30/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3783 - accuracy: 0.8858 - val_loss: 0.6722 - val_accuracy: 0.8109\n","Epoch 31/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3834 - accuracy: 0.8836 - val_loss: 0.6978 - val_accuracy: 0.7917\n","Epoch 32/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3643 - accuracy: 0.8900 - val_loss: 0.6869 - val_accuracy: 0.8077\n","Epoch 33/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3506 - accuracy: 0.8965 - val_loss: 0.6580 - val_accuracy: 0.8237\n","Epoch 34/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3407 - accuracy: 0.9001 - val_loss: 0.6366 - val_accuracy: 0.8173\n","Epoch 35/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3154 - accuracy: 0.9039 - val_loss: 0.7055 - val_accuracy: 0.8109\n","Epoch 36/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3343 - accuracy: 0.8954 - val_loss: 0.7316 - val_accuracy: 0.8141\n","Epoch 37/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3350 - accuracy: 0.9028 - val_loss: 0.7022 - val_accuracy: 0.8109\n","Epoch 38/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3242 - accuracy: 0.9087 - val_loss: 0.7338 - val_accuracy: 0.7981\n","\n","Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 39/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2784 - accuracy: 0.9298 - val_loss: 0.6599 - val_accuracy: 0.8141\n","Epoch 40/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2550 - accuracy: 0.9295 - val_loss: 0.6466 - val_accuracy: 0.8301\n","Epoch 41/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2654 - accuracy: 0.9218 - val_loss: 0.6888 - val_accuracy: 0.8141\n","Epoch 42/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2572 - accuracy: 0.9290 - val_loss: 0.6443 - val_accuracy: 0.8141\n","\n","Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","10/10 [==============================] - 0s 19ms/step - loss: 0.6366 - accuracy: 0.8173\n","10/10 [==============================] - 0s 9ms/step - loss: 0.6366 - accuracy: 0.8173\n","--------------------Fold_7--------------------\n","Epoch 1/100\n","44/44 [==============================] - 4s 59ms/step - loss: 3.4687 - accuracy: 0.3330 - val_loss: 3.7678 - val_accuracy: 0.1250\n","Epoch 2/100\n","44/44 [==============================] - 2s 49ms/step - loss: 2.0063 - accuracy: 0.5483 - val_loss: 1.9349 - val_accuracy: 0.5449\n","Epoch 3/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.7361 - accuracy: 0.5864 - val_loss: 2.0752 - val_accuracy: 0.5577\n","Epoch 4/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.5269 - accuracy: 0.6354 - val_loss: 1.8105 - val_accuracy: 0.5609\n","Epoch 5/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.4089 - accuracy: 0.6506 - val_loss: 1.6682 - val_accuracy: 0.5737\n","Epoch 6/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.3012 - accuracy: 0.6626 - val_loss: 1.4061 - val_accuracy: 0.6282\n","Epoch 7/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.1511 - accuracy: 0.6994 - val_loss: 1.2929 - val_accuracy: 0.6442\n","Epoch 8/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.1010 - accuracy: 0.7028 - val_loss: 1.2151 - val_accuracy: 0.6635\n","Epoch 9/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.0103 - accuracy: 0.7345 - val_loss: 1.1349 - val_accuracy: 0.6795\n","Epoch 10/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.9295 - accuracy: 0.7468 - val_loss: 1.0121 - val_accuracy: 0.7404\n","Epoch 11/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.8517 - accuracy: 0.7693 - val_loss: 1.0112 - val_accuracy: 0.6955\n","Epoch 12/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.7836 - accuracy: 0.7868 - val_loss: 0.9202 - val_accuracy: 0.7404\n","Epoch 13/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.7658 - accuracy: 0.7811 - val_loss: 0.8741 - val_accuracy: 0.7372\n","Epoch 14/100\n","44/44 [==============================] - 2s 52ms/step - loss: 0.7195 - accuracy: 0.8012 - val_loss: 0.8324 - val_accuracy: 0.7404\n","Epoch 15/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.7165 - accuracy: 0.7923 - val_loss: 0.8085 - val_accuracy: 0.7660\n","Epoch 16/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.6392 - accuracy: 0.8155 - val_loss: 0.7965 - val_accuracy: 0.7564\n","Epoch 17/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.6049 - accuracy: 0.8288 - val_loss: 0.7253 - val_accuracy: 0.7788\n","Epoch 18/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6214 - accuracy: 0.8143 - val_loss: 0.7208 - val_accuracy: 0.7756\n","Epoch 19/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5407 - accuracy: 0.8449 - val_loss: 0.7645 - val_accuracy: 0.7660\n","Epoch 20/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5478 - accuracy: 0.8376 - val_loss: 0.6675 - val_accuracy: 0.7885\n","Epoch 21/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5324 - accuracy: 0.8528 - val_loss: 0.6664 - val_accuracy: 0.8045\n","Epoch 22/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5162 - accuracy: 0.8414 - val_loss: 0.7437 - val_accuracy: 0.7853\n","Epoch 23/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5014 - accuracy: 0.8476 - val_loss: 0.6658 - val_accuracy: 0.7981\n","Epoch 24/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4811 - accuracy: 0.8609 - val_loss: 0.6884 - val_accuracy: 0.7756\n","Epoch 25/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4884 - accuracy: 0.8571 - val_loss: 0.7161 - val_accuracy: 0.7788\n","Epoch 26/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4139 - accuracy: 0.8799 - val_loss: 0.5854 - val_accuracy: 0.8269\n","Epoch 27/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.4196 - accuracy: 0.8676 - val_loss: 0.6261 - val_accuracy: 0.8141\n","Epoch 28/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.4081 - accuracy: 0.8838 - val_loss: 0.5731 - val_accuracy: 0.8141\n","Epoch 29/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4236 - accuracy: 0.8798 - val_loss: 0.5785 - val_accuracy: 0.8365\n","Epoch 30/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3991 - accuracy: 0.8821 - val_loss: 0.5966 - val_accuracy: 0.8205\n","Epoch 31/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3760 - accuracy: 0.8919 - val_loss: 0.5662 - val_accuracy: 0.8205\n","Epoch 32/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3723 - accuracy: 0.8901 - val_loss: 0.5733 - val_accuracy: 0.8269\n","Epoch 33/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3357 - accuracy: 0.9095 - val_loss: 0.6541 - val_accuracy: 0.8109\n","Epoch 34/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3646 - accuracy: 0.8841 - val_loss: 0.5455 - val_accuracy: 0.8333\n","Epoch 35/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3624 - accuracy: 0.8940 - val_loss: 0.6154 - val_accuracy: 0.8301\n","Epoch 36/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.3438 - accuracy: 0.8934 - val_loss: 0.5643 - val_accuracy: 0.8237\n","Epoch 37/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3529 - accuracy: 0.8956 - val_loss: 0.5441 - val_accuracy: 0.8269\n","Epoch 38/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.3603 - accuracy: 0.8924 - val_loss: 0.5515 - val_accuracy: 0.8269\n","Epoch 39/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.2946 - accuracy: 0.9182 - val_loss: 0.5613 - val_accuracy: 0.8045\n","Epoch 40/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3019 - accuracy: 0.9080 - val_loss: 0.5720 - val_accuracy: 0.8301\n","Epoch 41/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3075 - accuracy: 0.9027 - val_loss: 0.5795 - val_accuracy: 0.8237\n","\n","Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 42/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2532 - accuracy: 0.9344 - val_loss: 0.5163 - val_accuracy: 0.8397\n","Epoch 43/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2527 - accuracy: 0.9324 - val_loss: 0.5143 - val_accuracy: 0.8301\n","Epoch 44/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2530 - accuracy: 0.9341 - val_loss: 0.5269 - val_accuracy: 0.8173\n","Epoch 45/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2428 - accuracy: 0.9267 - val_loss: 0.5046 - val_accuracy: 0.8237\n","Epoch 46/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2550 - accuracy: 0.9219 - val_loss: 0.4844 - val_accuracy: 0.8494\n","Epoch 47/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2313 - accuracy: 0.9293 - val_loss: 0.4865 - val_accuracy: 0.8333\n","Epoch 48/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2358 - accuracy: 0.9393 - val_loss: 0.5534 - val_accuracy: 0.8237\n","Epoch 49/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2425 - accuracy: 0.9265 - val_loss: 0.5307 - val_accuracy: 0.8301\n","Epoch 50/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2167 - accuracy: 0.9360 - val_loss: 0.5083 - val_accuracy: 0.8429\n","\n","Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 51/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2066 - accuracy: 0.9456 - val_loss: 0.4979 - val_accuracy: 0.8462\n","Epoch 52/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2123 - accuracy: 0.9461 - val_loss: 0.4862 - val_accuracy: 0.8494\n","Epoch 53/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2189 - accuracy: 0.9401 - val_loss: 0.4726 - val_accuracy: 0.8462\n","Epoch 54/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2138 - accuracy: 0.9389 - val_loss: 0.4962 - val_accuracy: 0.8462\n","Epoch 55/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2144 - accuracy: 0.9410 - val_loss: 0.4782 - val_accuracy: 0.8494\n","Epoch 56/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2076 - accuracy: 0.9489 - val_loss: 0.4613 - val_accuracy: 0.8429\n","Epoch 57/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2122 - accuracy: 0.9406 - val_loss: 0.4681 - val_accuracy: 0.8429\n","Epoch 58/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.2126 - accuracy: 0.9388 - val_loss: 0.4516 - val_accuracy: 0.8526\n","Epoch 59/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2139 - accuracy: 0.9416 - val_loss: 0.4694 - val_accuracy: 0.8429\n","Epoch 60/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2027 - accuracy: 0.9465 - val_loss: 0.4527 - val_accuracy: 0.8526\n","Epoch 61/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.1836 - accuracy: 0.9557 - val_loss: 0.4679 - val_accuracy: 0.8365\n","Epoch 62/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2003 - accuracy: 0.9444 - val_loss: 0.4811 - val_accuracy: 0.8365\n","\n","Epoch 00062: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 63/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1852 - accuracy: 0.9499 - val_loss: 0.4714 - val_accuracy: 0.8365\n","Epoch 64/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1890 - accuracy: 0.9500 - val_loss: 0.4547 - val_accuracy: 0.8526\n","Epoch 65/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1737 - accuracy: 0.9621 - val_loss: 0.4746 - val_accuracy: 0.8429\n","Epoch 66/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1854 - accuracy: 0.9506 - val_loss: 0.4634 - val_accuracy: 0.8462\n","\n","Epoch 00066: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","10/10 [==============================] - 0s 8ms/step - loss: 0.4516 - accuracy: 0.8526\n","10/10 [==============================] - 0s 8ms/step - loss: 0.4516 - accuracy: 0.8526\n","--------------------Fold_8--------------------\n","Epoch 1/100\n","44/44 [==============================] - 4s 57ms/step - loss: 3.4460 - accuracy: 0.3547 - val_loss: 3.2661 - val_accuracy: 0.2596\n","Epoch 2/100\n","44/44 [==============================] - 2s 48ms/step - loss: 2.0290 - accuracy: 0.5404 - val_loss: 1.9139 - val_accuracy: 0.5353\n","Epoch 3/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.7287 - accuracy: 0.5825 - val_loss: 1.9264 - val_accuracy: 0.5641\n","Epoch 4/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.5139 - accuracy: 0.6334 - val_loss: 1.8696 - val_accuracy: 0.5577\n","Epoch 5/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.4501 - accuracy: 0.6308 - val_loss: 1.6223 - val_accuracy: 0.5897\n","Epoch 6/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.2803 - accuracy: 0.6637 - val_loss: 1.5139 - val_accuracy: 0.6282\n","Epoch 7/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.1362 - accuracy: 0.6992 - val_loss: 1.2581 - val_accuracy: 0.6410\n","Epoch 8/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.0451 - accuracy: 0.7173 - val_loss: 1.2193 - val_accuracy: 0.6667\n","Epoch 9/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.9934 - accuracy: 0.7372 - val_loss: 1.0878 - val_accuracy: 0.6955\n","Epoch 10/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.8770 - accuracy: 0.7697 - val_loss: 1.0365 - val_accuracy: 0.7115\n","Epoch 11/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.8410 - accuracy: 0.7616 - val_loss: 1.0035 - val_accuracy: 0.7083\n","Epoch 12/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.8091 - accuracy: 0.7903 - val_loss: 0.9097 - val_accuracy: 0.7308\n","Epoch 13/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.7112 - accuracy: 0.8039 - val_loss: 0.8382 - val_accuracy: 0.7179\n","Epoch 14/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.7164 - accuracy: 0.7960 - val_loss: 0.8808 - val_accuracy: 0.7532\n","Epoch 15/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6969 - accuracy: 0.7949 - val_loss: 0.8035 - val_accuracy: 0.7596\n","Epoch 16/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6222 - accuracy: 0.8191 - val_loss: 0.8250 - val_accuracy: 0.7596\n","Epoch 17/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.6233 - accuracy: 0.8164 - val_loss: 0.7409 - val_accuracy: 0.7660\n","Epoch 18/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5780 - accuracy: 0.8309 - val_loss: 0.6949 - val_accuracy: 0.7821\n","Epoch 19/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5618 - accuracy: 0.8275 - val_loss: 0.7141 - val_accuracy: 0.7596\n","Epoch 20/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5499 - accuracy: 0.8368 - val_loss: 0.6661 - val_accuracy: 0.8173\n","Epoch 21/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4895 - accuracy: 0.8541 - val_loss: 0.6197 - val_accuracy: 0.7853\n","Epoch 22/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4926 - accuracy: 0.8577 - val_loss: 0.6397 - val_accuracy: 0.7917\n","Epoch 23/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4738 - accuracy: 0.8639 - val_loss: 0.6650 - val_accuracy: 0.7564\n","Epoch 24/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4807 - accuracy: 0.8578 - val_loss: 0.6012 - val_accuracy: 0.8045\n","Epoch 25/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4271 - accuracy: 0.8816 - val_loss: 0.6225 - val_accuracy: 0.7949\n","Epoch 26/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4382 - accuracy: 0.8702 - val_loss: 0.6164 - val_accuracy: 0.7917\n","Epoch 27/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4292 - accuracy: 0.8802 - val_loss: 0.6331 - val_accuracy: 0.7853\n","Epoch 28/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4115 - accuracy: 0.8781 - val_loss: 0.5881 - val_accuracy: 0.8077\n","Epoch 29/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.3764 - accuracy: 0.8948 - val_loss: 0.5970 - val_accuracy: 0.8013\n","Epoch 30/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3873 - accuracy: 0.8917 - val_loss: 0.5960 - val_accuracy: 0.7949\n","Epoch 31/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3652 - accuracy: 0.8928 - val_loss: 0.6166 - val_accuracy: 0.8045\n","Epoch 32/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3933 - accuracy: 0.8798 - val_loss: 0.5981 - val_accuracy: 0.8141\n","\n","Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 33/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3370 - accuracy: 0.9088 - val_loss: 0.5411 - val_accuracy: 0.8237\n","Epoch 34/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3290 - accuracy: 0.9081 - val_loss: 0.5232 - val_accuracy: 0.8237\n","Epoch 35/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2981 - accuracy: 0.9202 - val_loss: 0.5843 - val_accuracy: 0.8045\n","Epoch 36/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2950 - accuracy: 0.9237 - val_loss: 0.5655 - val_accuracy: 0.8045\n","Epoch 37/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.3235 - accuracy: 0.9072 - val_loss: 0.5310 - val_accuracy: 0.8173\n","Epoch 38/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2909 - accuracy: 0.9186 - val_loss: 0.5264 - val_accuracy: 0.8205\n","\n","Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 39/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2861 - accuracy: 0.9220 - val_loss: 0.4676 - val_accuracy: 0.8462\n","Epoch 40/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2805 - accuracy: 0.9271 - val_loss: 0.4735 - val_accuracy: 0.8365\n","Epoch 41/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.2646 - accuracy: 0.9325 - val_loss: 0.4651 - val_accuracy: 0.8590\n","Epoch 42/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2599 - accuracy: 0.9329 - val_loss: 0.4555 - val_accuracy: 0.8558\n","Epoch 43/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2776 - accuracy: 0.9213 - val_loss: 0.4614 - val_accuracy: 0.8429\n","Epoch 44/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2566 - accuracy: 0.9374 - val_loss: 0.4558 - val_accuracy: 0.8462\n","Epoch 45/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2816 - accuracy: 0.9249 - val_loss: 0.4897 - val_accuracy: 0.8333\n","Epoch 46/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2581 - accuracy: 0.9328 - val_loss: 0.4632 - val_accuracy: 0.8558\n","\n","Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 47/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2504 - accuracy: 0.9352 - val_loss: 0.4489 - val_accuracy: 0.8590\n","Epoch 48/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2511 - accuracy: 0.9373 - val_loss: 0.4550 - val_accuracy: 0.8429\n","Epoch 49/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2392 - accuracy: 0.9377 - val_loss: 0.4458 - val_accuracy: 0.8462\n","Epoch 50/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2413 - accuracy: 0.9391 - val_loss: 0.4524 - val_accuracy: 0.8462\n","Epoch 51/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2335 - accuracy: 0.9438 - val_loss: 0.4571 - val_accuracy: 0.8365\n","Epoch 52/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2389 - accuracy: 0.9428 - val_loss: 0.4591 - val_accuracy: 0.8397\n","Epoch 53/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2644 - accuracy: 0.9283 - val_loss: 0.4467 - val_accuracy: 0.8429\n","\n","Epoch 00053: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","Epoch 54/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2336 - accuracy: 0.9413 - val_loss: 0.4520 - val_accuracy: 0.8462\n","Epoch 55/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2498 - accuracy: 0.9330 - val_loss: 0.4551 - val_accuracy: 0.8333\n","Epoch 56/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2629 - accuracy: 0.9297 - val_loss: 0.4484 - val_accuracy: 0.8429\n","Epoch 57/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2606 - accuracy: 0.9381 - val_loss: 0.4491 - val_accuracy: 0.8526\n","\n","Epoch 00057: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","10/10 [==============================] - 0s 11ms/step - loss: 0.4458 - accuracy: 0.8462\n","10/10 [==============================] - 0s 10ms/step - loss: 0.4458 - accuracy: 0.8462\n","--------------------Fold_9--------------------\n","Epoch 1/100\n","44/44 [==============================] - 4s 58ms/step - loss: 3.4284 - accuracy: 0.3672 - val_loss: 3.0021 - val_accuracy: 0.3974\n","Epoch 2/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.9968 - accuracy: 0.5559 - val_loss: 1.9842 - val_accuracy: 0.5417\n","Epoch 3/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.7820 - accuracy: 0.5767 - val_loss: 1.8464 - val_accuracy: 0.5545\n","Epoch 4/100\n","44/44 [==============================] - 2s 47ms/step - loss: 1.5501 - accuracy: 0.6193 - val_loss: 1.8023 - val_accuracy: 0.5577\n","Epoch 5/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.4028 - accuracy: 0.6379 - val_loss: 1.5869 - val_accuracy: 0.5865\n","Epoch 6/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.2709 - accuracy: 0.6715 - val_loss: 1.3486 - val_accuracy: 0.6506\n","Epoch 7/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.1625 - accuracy: 0.6924 - val_loss: 1.2522 - val_accuracy: 0.6442\n","Epoch 8/100\n","44/44 [==============================] - 2s 51ms/step - loss: 1.0628 - accuracy: 0.7219 - val_loss: 1.1590 - val_accuracy: 0.6731\n","Epoch 9/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.0127 - accuracy: 0.7217 - val_loss: 1.0328 - val_accuracy: 0.7212\n","Epoch 10/100\n","44/44 [==============================] - 2s 51ms/step - loss: 0.9088 - accuracy: 0.7528 - val_loss: 1.0161 - val_accuracy: 0.7019\n","Epoch 11/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.8763 - accuracy: 0.7522 - val_loss: 0.9711 - val_accuracy: 0.7019\n","Epoch 12/100\n","44/44 [==============================] - 2s 52ms/step - loss: 0.7572 - accuracy: 0.7942 - val_loss: 0.8887 - val_accuracy: 0.7436\n","Epoch 13/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.7746 - accuracy: 0.7838 - val_loss: 0.8433 - val_accuracy: 0.7532\n","Epoch 14/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.6799 - accuracy: 0.8126 - val_loss: 0.8835 - val_accuracy: 0.7147\n","Epoch 15/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.6710 - accuracy: 0.8073 - val_loss: 0.7611 - val_accuracy: 0.7949\n","Epoch 16/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.6064 - accuracy: 0.8265 - val_loss: 0.7365 - val_accuracy: 0.7660\n","Epoch 17/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.6166 - accuracy: 0.8200 - val_loss: 0.7779 - val_accuracy: 0.7468\n","Epoch 18/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5652 - accuracy: 0.8396 - val_loss: 0.6616 - val_accuracy: 0.7853\n","Epoch 19/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5350 - accuracy: 0.8393 - val_loss: 0.6702 - val_accuracy: 0.8269\n","Epoch 20/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5345 - accuracy: 0.8455 - val_loss: 0.7176 - val_accuracy: 0.7500\n","Epoch 21/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.4981 - accuracy: 0.8472 - val_loss: 0.7178 - val_accuracy: 0.7981\n","Epoch 22/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5067 - accuracy: 0.8483 - val_loss: 0.6280 - val_accuracy: 0.8013\n","Epoch 23/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4736 - accuracy: 0.8688 - val_loss: 0.6237 - val_accuracy: 0.8141\n","Epoch 24/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4467 - accuracy: 0.8676 - val_loss: 0.6074 - val_accuracy: 0.8013\n","Epoch 25/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4580 - accuracy: 0.8639 - val_loss: 0.5915 - val_accuracy: 0.8045\n","Epoch 26/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3987 - accuracy: 0.8839 - val_loss: 0.6174 - val_accuracy: 0.8109\n","Epoch 27/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4103 - accuracy: 0.8757 - val_loss: 0.5812 - val_accuracy: 0.8077\n","Epoch 28/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4528 - accuracy: 0.8590 - val_loss: 0.6952 - val_accuracy: 0.7564\n","Epoch 29/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4104 - accuracy: 0.8854 - val_loss: 0.6621 - val_accuracy: 0.7756\n","Epoch 30/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3967 - accuracy: 0.8856 - val_loss: 0.5349 - val_accuracy: 0.8013\n","Epoch 31/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3844 - accuracy: 0.8855 - val_loss: 0.5903 - val_accuracy: 0.8013\n","Epoch 32/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.3558 - accuracy: 0.8912 - val_loss: 0.5700 - val_accuracy: 0.8173\n","Epoch 33/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.3942 - accuracy: 0.8852 - val_loss: 0.5995 - val_accuracy: 0.7917\n","Epoch 34/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.3364 - accuracy: 0.9020 - val_loss: 0.5344 - val_accuracy: 0.8237\n","Epoch 35/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3500 - accuracy: 0.8934 - val_loss: 0.5399 - val_accuracy: 0.8237\n","Epoch 36/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3295 - accuracy: 0.8986 - val_loss: 0.5634 - val_accuracy: 0.8013\n","Epoch 37/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.3454 - accuracy: 0.8943 - val_loss: 0.5104 - val_accuracy: 0.8141\n","Epoch 38/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3242 - accuracy: 0.9036 - val_loss: 0.5092 - val_accuracy: 0.8205\n","Epoch 39/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3016 - accuracy: 0.9110 - val_loss: 0.5293 - val_accuracy: 0.7981\n","Epoch 40/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3207 - accuracy: 0.9046 - val_loss: 0.5677 - val_accuracy: 0.8109\n","Epoch 41/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2902 - accuracy: 0.9102 - val_loss: 0.4808 - val_accuracy: 0.8301\n","Epoch 42/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3174 - accuracy: 0.9028 - val_loss: 0.5502 - val_accuracy: 0.8173\n","Epoch 43/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.2940 - accuracy: 0.9087 - val_loss: 0.5480 - val_accuracy: 0.8205\n","Epoch 44/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2713 - accuracy: 0.9178 - val_loss: 0.5692 - val_accuracy: 0.8013\n","Epoch 45/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2803 - accuracy: 0.9172 - val_loss: 0.5341 - val_accuracy: 0.8205\n","\n","Epoch 00045: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 46/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2562 - accuracy: 0.9295 - val_loss: 0.5193 - val_accuracy: 0.8269\n","Epoch 47/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2311 - accuracy: 0.9330 - val_loss: 0.4545 - val_accuracy: 0.8558\n","Epoch 48/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2376 - accuracy: 0.9320 - val_loss: 0.4878 - val_accuracy: 0.8301\n","Epoch 49/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2324 - accuracy: 0.9350 - val_loss: 0.4870 - val_accuracy: 0.8237\n","Epoch 50/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2187 - accuracy: 0.9350 - val_loss: 0.4826 - val_accuracy: 0.8526\n","Epoch 51/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2034 - accuracy: 0.9451 - val_loss: 0.4857 - val_accuracy: 0.8429\n","\n","Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 52/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2256 - accuracy: 0.9391 - val_loss: 0.4417 - val_accuracy: 0.8558\n","Epoch 53/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2105 - accuracy: 0.9452 - val_loss: 0.4698 - val_accuracy: 0.8429\n","Epoch 54/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1985 - accuracy: 0.9439 - val_loss: 0.4530 - val_accuracy: 0.8429\n","Epoch 55/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1850 - accuracy: 0.9573 - val_loss: 0.4520 - val_accuracy: 0.8397\n","Epoch 56/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.1838 - accuracy: 0.9510 - val_loss: 0.4721 - val_accuracy: 0.8462\n","\n","Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 57/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1818 - accuracy: 0.9480 - val_loss: 0.4375 - val_accuracy: 0.8462\n","Epoch 58/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1914 - accuracy: 0.9522 - val_loss: 0.4419 - val_accuracy: 0.8462\n","Epoch 59/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1771 - accuracy: 0.9550 - val_loss: 0.4403 - val_accuracy: 0.8397\n","Epoch 60/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1824 - accuracy: 0.9536 - val_loss: 0.4516 - val_accuracy: 0.8494\n","Epoch 61/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1791 - accuracy: 0.9534 - val_loss: 0.4243 - val_accuracy: 0.8590\n","Epoch 62/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1804 - accuracy: 0.9530 - val_loss: 0.4553 - val_accuracy: 0.8429\n","Epoch 63/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1881 - accuracy: 0.9485 - val_loss: 0.4317 - val_accuracy: 0.8558\n","Epoch 64/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1814 - accuracy: 0.9538 - val_loss: 0.4485 - val_accuracy: 0.8590\n","Epoch 65/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1792 - accuracy: 0.9543 - val_loss: 0.4503 - val_accuracy: 0.8462\n","\n","Epoch 00065: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","Epoch 66/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1797 - accuracy: 0.9543 - val_loss: 0.4433 - val_accuracy: 0.8494\n","Epoch 67/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1690 - accuracy: 0.9535 - val_loss: 0.4453 - val_accuracy: 0.8429\n","Epoch 68/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.1829 - accuracy: 0.9558 - val_loss: 0.4381 - val_accuracy: 0.8462\n","Epoch 69/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1649 - accuracy: 0.9645 - val_loss: 0.4416 - val_accuracy: 0.8494\n","\n","Epoch 00069: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","10/10 [==============================] - 0s 12ms/step - loss: 0.4243 - accuracy: 0.8590\n","10/10 [==============================] - 0s 11ms/step - loss: 0.4243 - accuracy: 0.8590\n","--------------------Fold_10--------------------\n","Epoch 1/100\n","44/44 [==============================] - 4s 58ms/step - loss: 3.4454 - accuracy: 0.3432 - val_loss: 3.2154 - val_accuracy: 0.2500\n","Epoch 2/100\n","44/44 [==============================] - 2s 48ms/step - loss: 2.0168 - accuracy: 0.5465 - val_loss: 1.9623 - val_accuracy: 0.5481\n","Epoch 3/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.7013 - accuracy: 0.5855 - val_loss: 1.9081 - val_accuracy: 0.5417\n","Epoch 4/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.5305 - accuracy: 0.6234 - val_loss: 1.7268 - val_accuracy: 0.5769\n","Epoch 5/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.3816 - accuracy: 0.6475 - val_loss: 1.5196 - val_accuracy: 0.6026\n","Epoch 6/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.3006 - accuracy: 0.6511 - val_loss: 1.4627 - val_accuracy: 0.6218\n","Epoch 7/100\n","44/44 [==============================] - 2s 48ms/step - loss: 1.1666 - accuracy: 0.6983 - val_loss: 1.3258 - val_accuracy: 0.6603\n","Epoch 8/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.0652 - accuracy: 0.7144 - val_loss: 1.1168 - val_accuracy: 0.7212\n","Epoch 9/100\n","44/44 [==============================] - 2s 49ms/step - loss: 1.0161 - accuracy: 0.7127 - val_loss: 1.0232 - val_accuracy: 0.7147\n","Epoch 10/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.8906 - accuracy: 0.7654 - val_loss: 1.0106 - val_accuracy: 0.7276\n","Epoch 11/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.8487 - accuracy: 0.7568 - val_loss: 0.8837 - val_accuracy: 0.7404\n","Epoch 12/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.7769 - accuracy: 0.7770 - val_loss: 0.9042 - val_accuracy: 0.7532\n","Epoch 13/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.7784 - accuracy: 0.7852 - val_loss: 0.9049 - val_accuracy: 0.7404\n","Epoch 14/100\n","44/44 [==============================] - 2s 51ms/step - loss: 0.7115 - accuracy: 0.8038 - val_loss: 0.7958 - val_accuracy: 0.7885\n","Epoch 15/100\n","44/44 [==============================] - 2s 51ms/step - loss: 0.6782 - accuracy: 0.8144 - val_loss: 0.7470 - val_accuracy: 0.7853\n","Epoch 16/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.6632 - accuracy: 0.8190 - val_loss: 0.7161 - val_accuracy: 0.8045\n","Epoch 17/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.6095 - accuracy: 0.8157 - val_loss: 0.6972 - val_accuracy: 0.8205\n","Epoch 18/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5554 - accuracy: 0.8417 - val_loss: 0.6496 - val_accuracy: 0.8173\n","Epoch 19/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5460 - accuracy: 0.8389 - val_loss: 0.6485 - val_accuracy: 0.8077\n","Epoch 20/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5369 - accuracy: 0.8514 - val_loss: 0.6414 - val_accuracy: 0.8237\n","Epoch 21/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.5431 - accuracy: 0.8391 - val_loss: 0.6888 - val_accuracy: 0.7917\n","Epoch 22/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.5222 - accuracy: 0.8427 - val_loss: 0.6048 - val_accuracy: 0.8269\n","Epoch 23/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.5188 - accuracy: 0.8500 - val_loss: 0.6254 - val_accuracy: 0.8205\n","Epoch 24/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.4550 - accuracy: 0.8676 - val_loss: 0.6578 - val_accuracy: 0.8077\n","Epoch 25/100\n","44/44 [==============================] - 2s 51ms/step - loss: 0.4634 - accuracy: 0.8599 - val_loss: 0.5612 - val_accuracy: 0.8109\n","Epoch 26/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.4102 - accuracy: 0.8819 - val_loss: 0.6126 - val_accuracy: 0.8141\n","Epoch 27/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4504 - accuracy: 0.8719 - val_loss: 0.5715 - val_accuracy: 0.8205\n","Epoch 28/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.4306 - accuracy: 0.8712 - val_loss: 0.5883 - val_accuracy: 0.8109\n","Epoch 29/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3953 - accuracy: 0.8795 - val_loss: 0.5453 - val_accuracy: 0.8269\n","Epoch 30/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3892 - accuracy: 0.8838 - val_loss: 0.5890 - val_accuracy: 0.8205\n","Epoch 31/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3705 - accuracy: 0.8860 - val_loss: 0.5574 - val_accuracy: 0.8205\n","Epoch 32/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.3451 - accuracy: 0.9027 - val_loss: 0.5358 - val_accuracy: 0.8397\n","Epoch 33/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3431 - accuracy: 0.8975 - val_loss: 0.5065 - val_accuracy: 0.8429\n","Epoch 34/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3566 - accuracy: 0.8979 - val_loss: 0.5246 - val_accuracy: 0.8269\n","Epoch 35/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.3216 - accuracy: 0.9193 - val_loss: 0.5128 - val_accuracy: 0.8590\n","Epoch 36/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.3142 - accuracy: 0.9086 - val_loss: 0.5584 - val_accuracy: 0.8301\n","Epoch 37/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.3417 - accuracy: 0.9011 - val_loss: 0.6086 - val_accuracy: 0.7981\n","\n","Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n","Epoch 38/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.3084 - accuracy: 0.9185 - val_loss: 0.5079 - val_accuracy: 0.8269\n","Epoch 39/100\n","44/44 [==============================] - 2s 51ms/step - loss: 0.2819 - accuracy: 0.9192 - val_loss: 0.5165 - val_accuracy: 0.8429\n","Epoch 40/100\n","44/44 [==============================] - 2s 47ms/step - loss: 0.3030 - accuracy: 0.9159 - val_loss: 0.4750 - val_accuracy: 0.8462\n","Epoch 41/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2610 - accuracy: 0.9332 - val_loss: 0.4691 - val_accuracy: 0.8526\n","Epoch 42/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.2689 - accuracy: 0.9260 - val_loss: 0.4800 - val_accuracy: 0.8397\n","Epoch 43/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2680 - accuracy: 0.9253 - val_loss: 0.4769 - val_accuracy: 0.8526\n","Epoch 44/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.2785 - accuracy: 0.9242 - val_loss: 0.4846 - val_accuracy: 0.8462\n","Epoch 45/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2543 - accuracy: 0.9315 - val_loss: 0.4616 - val_accuracy: 0.8494\n","Epoch 46/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.2639 - accuracy: 0.9244 - val_loss: 0.5196 - val_accuracy: 0.8269\n","Epoch 47/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2614 - accuracy: 0.9268 - val_loss: 0.4719 - val_accuracy: 0.8429\n","Epoch 48/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.2538 - accuracy: 0.9287 - val_loss: 0.4825 - val_accuracy: 0.8462\n","Epoch 49/100\n","44/44 [==============================] - 2s 51ms/step - loss: 0.2384 - accuracy: 0.9372 - val_loss: 0.4641 - val_accuracy: 0.8526\n","\n","Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n","Epoch 50/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2283 - accuracy: 0.9388 - val_loss: 0.4576 - val_accuracy: 0.8429\n","Epoch 51/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2296 - accuracy: 0.9403 - val_loss: 0.4515 - val_accuracy: 0.8494\n","Epoch 52/100\n","44/44 [==============================] - 2s 51ms/step - loss: 0.2253 - accuracy: 0.9532 - val_loss: 0.4431 - val_accuracy: 0.8462\n","Epoch 53/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.2146 - accuracy: 0.9455 - val_loss: 0.4448 - val_accuracy: 0.8622\n","Epoch 54/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2221 - accuracy: 0.9438 - val_loss: 0.4417 - val_accuracy: 0.8494\n","Epoch 55/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2093 - accuracy: 0.9503 - val_loss: 0.4510 - val_accuracy: 0.8526\n","Epoch 56/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.2120 - accuracy: 0.9444 - val_loss: 0.4390 - val_accuracy: 0.8462\n","Epoch 57/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.1970 - accuracy: 0.9497 - val_loss: 0.4460 - val_accuracy: 0.8462\n","Epoch 58/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2227 - accuracy: 0.9431 - val_loss: 0.4447 - val_accuracy: 0.8462\n","Epoch 59/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.2003 - accuracy: 0.9502 - val_loss: 0.4457 - val_accuracy: 0.8462\n","Epoch 60/100\n","44/44 [==============================] - 2s 51ms/step - loss: 0.2091 - accuracy: 0.9418 - val_loss: 0.4396 - val_accuracy: 0.8462\n","\n","Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n","Epoch 61/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2185 - accuracy: 0.9413 - val_loss: 0.4290 - val_accuracy: 0.8590\n","Epoch 62/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1953 - accuracy: 0.9513 - val_loss: 0.4284 - val_accuracy: 0.8526\n","Epoch 63/100\n","44/44 [==============================] - 2s 50ms/step - loss: 0.2085 - accuracy: 0.9493 - val_loss: 0.4337 - val_accuracy: 0.8526\n","Epoch 64/100\n","44/44 [==============================] - 2s 51ms/step - loss: 0.2005 - accuracy: 0.9468 - val_loss: 0.4335 - val_accuracy: 0.8622\n","Epoch 65/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.2074 - accuracy: 0.9459 - val_loss: 0.4477 - val_accuracy: 0.8526\n","Epoch 66/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1943 - accuracy: 0.9451 - val_loss: 0.4333 - val_accuracy: 0.8558\n","\n","Epoch 00066: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n","Epoch 67/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1938 - accuracy: 0.9464 - val_loss: 0.4261 - val_accuracy: 0.8558\n","Epoch 68/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1871 - accuracy: 0.9582 - val_loss: 0.4267 - val_accuracy: 0.8654\n","Epoch 69/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1639 - accuracy: 0.9625 - val_loss: 0.4237 - val_accuracy: 0.8590\n","Epoch 70/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1794 - accuracy: 0.9529 - val_loss: 0.4208 - val_accuracy: 0.8622\n","Epoch 71/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.2007 - accuracy: 0.9475 - val_loss: 0.4236 - val_accuracy: 0.8590\n","Epoch 72/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1798 - accuracy: 0.9580 - val_loss: 0.4286 - val_accuracy: 0.8622\n","Epoch 73/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1843 - accuracy: 0.9573 - val_loss: 0.4298 - val_accuracy: 0.8590\n","Epoch 74/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1968 - accuracy: 0.9491 - val_loss: 0.4285 - val_accuracy: 0.8590\n","\n","Epoch 00074: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n","Epoch 75/100\n","44/44 [==============================] - 2s 48ms/step - loss: 0.1863 - accuracy: 0.9498 - val_loss: 0.4259 - val_accuracy: 0.8590\n","Epoch 76/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1794 - accuracy: 0.9593 - val_loss: 0.4223 - val_accuracy: 0.8590\n","Epoch 77/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1908 - accuracy: 0.9584 - val_loss: 0.4258 - val_accuracy: 0.8622\n","Epoch 78/100\n","44/44 [==============================] - 2s 49ms/step - loss: 0.1804 - accuracy: 0.9570 - val_loss: 0.4232 - val_accuracy: 0.8590\n","\n","Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n","10/10 [==============================] - 0s 11ms/step - loss: 0.4208 - accuracy: 0.8622\n","10/10 [==============================] - 0s 11ms/step - loss: 0.4208 - accuracy: 0.8622\n","\n","K-fold cross validation Auc: ['0.8403', '0.8435', '0.8690', '0.8147', '0.8530', '0.8173', '0.8526', '0.8462', '0.8590', '0.8622']\n","\n","K-fold cross validation loss: ['0.5497', '0.4770', '0.4119', '0.6309', '0.4720', '0.6366', '0.4516', '0.4458', '0.4243', '0.4208']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G6Yp7y9DRIj-"},"source":["##### 성능 확인 및 제출"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITlrPaW3ugnj","executionInfo":{"status":"ok","timestamp":1623820434005,"user_tz":-540,"elapsed":306,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"43b86b2a-32b6-41f1-ae86-decdbae2be11"},"source":["print(accuracy)\n","print()\n","print(losss)"],"execution_count":39,"outputs":[{"output_type":"stream","text":["['0.8403', '0.8435', '0.8690', '0.8147', '0.8530', '0.8173', '0.8526', '0.8462', '0.8590', '0.8622']\n","\n","['0.5497', '0.4770', '0.4119', '0.6309', '0.4720', '0.6366', '0.4516', '0.4458', '0.4243', '0.4208']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AwYvUlwaRIj-","executionInfo":{"status":"ok","timestamp":1623820434985,"user_tz":-540,"elapsed":3,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"0d73f0b1-88de-408f-804c-03f2cc13ab84"},"source":["print(sum([float(i) for i in accuracy])/10)\n","print()\n","print(sum([float(i) for i in losss])/10)"],"execution_count":40,"outputs":[{"output_type":"stream","text":["0.8457800000000001\n","\n","0.49205999999999994\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-b_mTFALRIj-","executionInfo":{"status":"ok","timestamp":1623820436053,"user_tz":-540,"elapsed":4,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"e2ebf1e8-f8ad-4dd2-c238-7d760836a17c"},"source":["test_X=np.array(test_sc.iloc[:,2:]).reshape(76, 600, -1)\n","test_X.shape"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(76, 600, 18)"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mdWapct-RIj_","executionInfo":{"status":"ok","timestamp":1623820437976,"user_tz":-540,"elapsed":752,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"55e6550f-a623-4ff6-bd81-95bbbc68ae7a"},"source":["preds = []\n","for model in models:\n","    pred = model.predict(test_X)\n","    preds.append(pred)\n","pred = np.mean(preds, axis=0)\n","pred"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[3.3137949e-05, 2.1677962e-01, 1.2704056e-06, ..., 1.1569775e-05,\n","        6.1953080e-01, 1.8673344e-07],\n","       [9.0124767e-04, 2.3786895e-01, 2.0319039e-05, ..., 3.2262757e-05,\n","        1.5562007e-01, 1.2238205e-06],\n","       [1.9967064e-04, 8.8423297e-02, 2.0058437e-06, ..., 3.2195945e-05,\n","        5.8947718e-01, 2.0121711e-06],\n","       ...,\n","       [9.4937645e-03, 5.9643318e-04, 3.5271264e-04, ..., 2.5471675e-04,\n","        1.0540507e-04, 1.0774012e-01],\n","       [2.4439324e-02, 2.7237949e-03, 8.7398061e-05, ..., 1.3677953e-03,\n","        8.9700734e-05, 1.7851952e-01],\n","       [5.2897923e-04, 1.1170584e-04, 1.1840065e-05, ..., 1.2710578e-04,\n","        7.3052339e-05, 4.0960473e-01]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":605},"id":"8OALM4SXRIj_","executionInfo":{"status":"ok","timestamp":1623820448590,"user_tz":-540,"elapsed":303,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"55e737f0-c9f4-4e8c-9af6-db3edacb5f49"},"source":["submission=pd.read_csv('./sample_submission_0616Lee.csv')\n","submission.iloc[:,1:]=pred\n","submission"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>...</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>40</th>\n","      <th>41</th>\n","      <th>42</th>\n","      <th>43</th>\n","      <th>44</th>\n","      <th>45</th>\n","      <th>46</th>\n","      <th>47</th>\n","      <th>48</th>\n","      <th>49</th>\n","      <th>50</th>\n","      <th>51</th>\n","      <th>52</th>\n","      <th>53</th>\n","      <th>54</th>\n","      <th>55</th>\n","      <th>56</th>\n","      <th>57</th>\n","      <th>58</th>\n","      <th>59</th>\n","      <th>60</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.000033</td>\n","      <td>0.216780</td>\n","      <td>0.000001</td>\n","      <td>2.304875e-07</td>\n","      <td>0.005119</td>\n","      <td>0.000073</td>\n","      <td>0.006855</td>\n","      <td>0.000006</td>\n","      <td>0.000002</td>\n","      <td>0.000012</td>\n","      <td>0.000634</td>\n","      <td>0.000060</td>\n","      <td>8.701382e-09</td>\n","      <td>0.000178</td>\n","      <td>0.000034</td>\n","      <td>0.000044</td>\n","      <td>1.761358e-03</td>\n","      <td>0.002399</td>\n","      <td>0.000090</td>\n","      <td>0.000105</td>\n","      <td>0.000477</td>\n","      <td>0.000158</td>\n","      <td>8.523672e-09</td>\n","      <td>0.000021</td>\n","      <td>...</td>\n","      <td>0.000123</td>\n","      <td>0.013445</td>\n","      <td>0.000123</td>\n","      <td>0.000252</td>\n","      <td>0.002353</td>\n","      <td>0.000464</td>\n","      <td>0.000046</td>\n","      <td>0.000127</td>\n","      <td>0.000562</td>\n","      <td>0.001032</td>\n","      <td>0.002458</td>\n","      <td>0.000294</td>\n","      <td>0.000169</td>\n","      <td>0.000237</td>\n","      <td>0.000126</td>\n","      <td>2.731252e-07</td>\n","      <td>5.536824e-08</td>\n","      <td>1.653439e-07</td>\n","      <td>0.000048</td>\n","      <td>1.862531e-07</td>\n","      <td>0.000053</td>\n","      <td>3.193653e-10</td>\n","      <td>0.000012</td>\n","      <td>0.619531</td>\n","      <td>1.867334e-07</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.000901</td>\n","      <td>0.237869</td>\n","      <td>0.000020</td>\n","      <td>1.075717e-06</td>\n","      <td>0.019341</td>\n","      <td>0.000552</td>\n","      <td>0.034963</td>\n","      <td>0.001162</td>\n","      <td>0.000034</td>\n","      <td>0.000289</td>\n","      <td>0.002232</td>\n","      <td>0.000707</td>\n","      <td>3.663784e-07</td>\n","      <td>0.000967</td>\n","      <td>0.000025</td>\n","      <td>0.001595</td>\n","      <td>6.480485e-03</td>\n","      <td>0.011268</td>\n","      <td>0.000736</td>\n","      <td>0.000765</td>\n","      <td>0.001681</td>\n","      <td>0.001307</td>\n","      <td>1.266715e-06</td>\n","      <td>0.000187</td>\n","      <td>...</td>\n","      <td>0.000627</td>\n","      <td>0.082675</td>\n","      <td>0.000925</td>\n","      <td>0.001401</td>\n","      <td>0.022617</td>\n","      <td>0.000699</td>\n","      <td>0.000217</td>\n","      <td>0.000303</td>\n","      <td>0.011345</td>\n","      <td>0.006809</td>\n","      <td>0.007643</td>\n","      <td>0.001229</td>\n","      <td>0.003017</td>\n","      <td>0.001176</td>\n","      <td>0.003434</td>\n","      <td>1.191768e-05</td>\n","      <td>1.618906e-06</td>\n","      <td>6.451095e-06</td>\n","      <td>0.000448</td>\n","      <td>3.590443e-06</td>\n","      <td>0.000599</td>\n","      <td>1.343261e-08</td>\n","      <td>0.000032</td>\n","      <td>0.155620</td>\n","      <td>1.223821e-06</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.000200</td>\n","      <td>0.088423</td>\n","      <td>0.000002</td>\n","      <td>1.600786e-06</td>\n","      <td>0.045726</td>\n","      <td>0.000392</td>\n","      <td>0.003061</td>\n","      <td>0.000026</td>\n","      <td>0.000013</td>\n","      <td>0.000102</td>\n","      <td>0.001133</td>\n","      <td>0.000111</td>\n","      <td>5.830112e-08</td>\n","      <td>0.000197</td>\n","      <td>0.000079</td>\n","      <td>0.000193</td>\n","      <td>1.606288e-03</td>\n","      <td>0.004511</td>\n","      <td>0.000558</td>\n","      <td>0.000292</td>\n","      <td>0.000800</td>\n","      <td>0.000172</td>\n","      <td>2.283222e-07</td>\n","      <td>0.000052</td>\n","      <td>...</td>\n","      <td>0.000181</td>\n","      <td>0.003308</td>\n","      <td>0.000478</td>\n","      <td>0.000274</td>\n","      <td>0.003770</td>\n","      <td>0.001397</td>\n","      <td>0.000110</td>\n","      <td>0.000150</td>\n","      <td>0.000533</td>\n","      <td>0.000364</td>\n","      <td>0.004485</td>\n","      <td>0.000225</td>\n","      <td>0.000776</td>\n","      <td>0.001460</td>\n","      <td>0.000953</td>\n","      <td>1.114920e-06</td>\n","      <td>2.855551e-07</td>\n","      <td>1.426241e-06</td>\n","      <td>0.000157</td>\n","      <td>1.206894e-06</td>\n","      <td>0.000198</td>\n","      <td>1.130814e-08</td>\n","      <td>0.000032</td>\n","      <td>0.589477</td>\n","      <td>2.012171e-06</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.004559</td>\n","      <td>0.101666</td>\n","      <td>0.000070</td>\n","      <td>1.996433e-04</td>\n","      <td>0.007421</td>\n","      <td>0.006646</td>\n","      <td>0.019953</td>\n","      <td>0.004027</td>\n","      <td>0.000023</td>\n","      <td>0.000145</td>\n","      <td>0.000745</td>\n","      <td>0.000010</td>\n","      <td>4.907999e-07</td>\n","      <td>0.000035</td>\n","      <td>0.000078</td>\n","      <td>0.002877</td>\n","      <td>1.314297e-03</td>\n","      <td>0.010105</td>\n","      <td>0.000484</td>\n","      <td>0.001106</td>\n","      <td>0.000210</td>\n","      <td>0.002055</td>\n","      <td>7.698406e-05</td>\n","      <td>0.000167</td>\n","      <td>...</td>\n","      <td>0.000800</td>\n","      <td>0.003985</td>\n","      <td>0.001817</td>\n","      <td>0.000340</td>\n","      <td>0.059454</td>\n","      <td>0.021787</td>\n","      <td>0.000191</td>\n","      <td>0.000121</td>\n","      <td>0.001126</td>\n","      <td>0.000269</td>\n","      <td>0.002702</td>\n","      <td>0.003927</td>\n","      <td>0.015111</td>\n","      <td>0.026035</td>\n","      <td>0.135528</td>\n","      <td>5.902705e-06</td>\n","      <td>1.354985e-06</td>\n","      <td>4.168314e-05</td>\n","      <td>0.008514</td>\n","      <td>1.982548e-05</td>\n","      <td>0.002052</td>\n","      <td>8.629020e-07</td>\n","      <td>0.000129</td>\n","      <td>0.148905</td>\n","      <td>1.615707e-05</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.095148</td>\n","      <td>0.221749</td>\n","      <td>0.000108</td>\n","      <td>1.437838e-04</td>\n","      <td>0.000188</td>\n","      <td>0.000151</td>\n","      <td>0.146353</td>\n","      <td>0.002076</td>\n","      <td>0.000176</td>\n","      <td>0.004950</td>\n","      <td>0.000205</td>\n","      <td>0.000372</td>\n","      <td>1.059002e-04</td>\n","      <td>0.000118</td>\n","      <td>0.000408</td>\n","      <td>0.003985</td>\n","      <td>4.725608e-03</td>\n","      <td>0.000518</td>\n","      <td>0.000079</td>\n","      <td>0.000480</td>\n","      <td>0.000031</td>\n","      <td>0.002248</td>\n","      <td>2.609558e-05</td>\n","      <td>0.000050</td>\n","      <td>...</td>\n","      <td>0.001638</td>\n","      <td>0.031113</td>\n","      <td>0.007858</td>\n","      <td>0.004650</td>\n","      <td>0.000627</td>\n","      <td>0.000271</td>\n","      <td>0.001782</td>\n","      <td>0.001308</td>\n","      <td>0.000464</td>\n","      <td>0.003011</td>\n","      <td>0.000253</td>\n","      <td>0.235857</td>\n","      <td>0.035470</td>\n","      <td>0.009145</td>\n","      <td>0.000198</td>\n","      <td>1.566867e-04</td>\n","      <td>6.160420e-05</td>\n","      <td>3.025192e-05</td>\n","      <td>0.010477</td>\n","      <td>9.790144e-06</td>\n","      <td>0.001494</td>\n","      <td>4.047729e-06</td>\n","      <td>0.000102</td>\n","      <td>0.001427</td>\n","      <td>3.471247e-03</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>71</th>\n","      <td>71</td>\n","      <td>0.128587</td>\n","      <td>0.092072</td>\n","      <td>0.000518</td>\n","      <td>1.891940e-03</td>\n","      <td>0.000127</td>\n","      <td>0.000317</td>\n","      <td>0.000457</td>\n","      <td>0.002397</td>\n","      <td>0.001709</td>\n","      <td>0.000016</td>\n","      <td>0.000042</td>\n","      <td>0.000096</td>\n","      <td>2.521380e-06</td>\n","      <td>0.000054</td>\n","      <td>0.000034</td>\n","      <td>0.447334</td>\n","      <td>5.323219e-03</td>\n","      <td>0.052556</td>\n","      <td>0.009053</td>\n","      <td>0.008714</td>\n","      <td>0.000208</td>\n","      <td>0.002699</td>\n","      <td>3.524705e-03</td>\n","      <td>0.001047</td>\n","      <td>...</td>\n","      <td>0.000396</td>\n","      <td>0.002071</td>\n","      <td>0.000097</td>\n","      <td>0.000107</td>\n","      <td>0.001583</td>\n","      <td>0.000590</td>\n","      <td>0.013130</td>\n","      <td>0.004190</td>\n","      <td>0.000077</td>\n","      <td>0.000302</td>\n","      <td>0.003471</td>\n","      <td>0.000252</td>\n","      <td>0.003415</td>\n","      <td>0.003843</td>\n","      <td>0.001328</td>\n","      <td>2.950664e-04</td>\n","      <td>6.880833e-07</td>\n","      <td>2.672819e-04</td>\n","      <td>0.000093</td>\n","      <td>9.241825e-05</td>\n","      <td>0.002421</td>\n","      <td>9.153256e-03</td>\n","      <td>0.000022</td>\n","      <td>0.000798</td>\n","      <td>2.973501e-03</td>\n","    </tr>\n","    <tr>\n","      <th>72</th>\n","      <td>72</td>\n","      <td>0.170183</td>\n","      <td>0.009281</td>\n","      <td>0.001028</td>\n","      <td>5.411435e-03</td>\n","      <td>0.000059</td>\n","      <td>0.001127</td>\n","      <td>0.000123</td>\n","      <td>0.002980</td>\n","      <td>0.002122</td>\n","      <td>0.000017</td>\n","      <td>0.000028</td>\n","      <td>0.000014</td>\n","      <td>6.822567e-07</td>\n","      <td>0.000008</td>\n","      <td>0.000024</td>\n","      <td>0.080970</td>\n","      <td>4.238651e-04</td>\n","      <td>0.001889</td>\n","      <td>0.000649</td>\n","      <td>0.000524</td>\n","      <td>0.000068</td>\n","      <td>0.008454</td>\n","      <td>1.440839e-02</td>\n","      <td>0.000307</td>\n","      <td>...</td>\n","      <td>0.001327</td>\n","      <td>0.000360</td>\n","      <td>0.000016</td>\n","      <td>0.000019</td>\n","      <td>0.000968</td>\n","      <td>0.000465</td>\n","      <td>0.001771</td>\n","      <td>0.001131</td>\n","      <td>0.000021</td>\n","      <td>0.000061</td>\n","      <td>0.000340</td>\n","      <td>0.000646</td>\n","      <td>0.005047</td>\n","      <td>0.015650</td>\n","      <td>0.005307</td>\n","      <td>1.445090e-04</td>\n","      <td>3.027336e-07</td>\n","      <td>3.825970e-04</td>\n","      <td>0.000572</td>\n","      <td>5.153039e-04</td>\n","      <td>0.000463</td>\n","      <td>1.244407e-02</td>\n","      <td>0.000011</td>\n","      <td>0.000133</td>\n","      <td>4.145924e-03</td>\n","    </tr>\n","    <tr>\n","      <th>73</th>\n","      <td>73</td>\n","      <td>0.009494</td>\n","      <td>0.000596</td>\n","      <td>0.000353</td>\n","      <td>2.172152e-03</td>\n","      <td>0.000032</td>\n","      <td>0.000274</td>\n","      <td>0.002884</td>\n","      <td>0.003924</td>\n","      <td>0.011947</td>\n","      <td>0.001534</td>\n","      <td>0.002128</td>\n","      <td>0.005968</td>\n","      <td>1.357916e-03</td>\n","      <td>0.000083</td>\n","      <td>0.001140</td>\n","      <td>0.011879</td>\n","      <td>2.010970e-05</td>\n","      <td>0.000204</td>\n","      <td>0.000076</td>\n","      <td>0.000432</td>\n","      <td>0.000016</td>\n","      <td>0.002016</td>\n","      <td>1.455235e-04</td>\n","      <td>0.001247</td>\n","      <td>...</td>\n","      <td>0.012574</td>\n","      <td>0.000246</td>\n","      <td>0.010567</td>\n","      <td>0.000488</td>\n","      <td>0.000234</td>\n","      <td>0.000161</td>\n","      <td>0.000270</td>\n","      <td>0.000561</td>\n","      <td>0.001352</td>\n","      <td>0.004634</td>\n","      <td>0.000196</td>\n","      <td>0.018899</td>\n","      <td>0.131330</td>\n","      <td>0.080850</td>\n","      <td>0.000801</td>\n","      <td>1.077751e-02</td>\n","      <td>1.597015e-03</td>\n","      <td>1.533933e-04</td>\n","      <td>0.009429</td>\n","      <td>1.532576e-04</td>\n","      <td>0.000313</td>\n","      <td>2.831365e-05</td>\n","      <td>0.000255</td>\n","      <td>0.000105</td>\n","      <td>1.077401e-01</td>\n","    </tr>\n","    <tr>\n","      <th>74</th>\n","      <td>74</td>\n","      <td>0.024439</td>\n","      <td>0.002724</td>\n","      <td>0.000087</td>\n","      <td>2.212586e-03</td>\n","      <td>0.000026</td>\n","      <td>0.000058</td>\n","      <td>0.024150</td>\n","      <td>0.001432</td>\n","      <td>0.000706</td>\n","      <td>0.008373</td>\n","      <td>0.002559</td>\n","      <td>0.009885</td>\n","      <td>2.164593e-02</td>\n","      <td>0.000161</td>\n","      <td>0.004116</td>\n","      <td>0.007174</td>\n","      <td>1.140379e-04</td>\n","      <td>0.000245</td>\n","      <td>0.000084</td>\n","      <td>0.000360</td>\n","      <td>0.000004</td>\n","      <td>0.000326</td>\n","      <td>4.859712e-05</td>\n","      <td>0.000490</td>\n","      <td>...</td>\n","      <td>0.002869</td>\n","      <td>0.000575</td>\n","      <td>0.102243</td>\n","      <td>0.004593</td>\n","      <td>0.000044</td>\n","      <td>0.000120</td>\n","      <td>0.001695</td>\n","      <td>0.001307</td>\n","      <td>0.000533</td>\n","      <td>0.008791</td>\n","      <td>0.000128</td>\n","      <td>0.015540</td>\n","      <td>0.104182</td>\n","      <td>0.020923</td>\n","      <td>0.000287</td>\n","      <td>5.935887e-03</td>\n","      <td>1.066511e-02</td>\n","      <td>6.323193e-05</td>\n","      <td>0.004966</td>\n","      <td>9.100653e-06</td>\n","      <td>0.000476</td>\n","      <td>1.993952e-05</td>\n","      <td>0.001368</td>\n","      <td>0.000090</td>\n","      <td>1.785195e-01</td>\n","    </tr>\n","    <tr>\n","      <th>75</th>\n","      <td>75</td>\n","      <td>0.000529</td>\n","      <td>0.000112</td>\n","      <td>0.000012</td>\n","      <td>1.532508e-04</td>\n","      <td>0.000013</td>\n","      <td>0.000059</td>\n","      <td>0.000173</td>\n","      <td>0.000291</td>\n","      <td>0.004023</td>\n","      <td>0.000121</td>\n","      <td>0.000784</td>\n","      <td>0.003439</td>\n","      <td>1.641499e-03</td>\n","      <td>0.000009</td>\n","      <td>0.002631</td>\n","      <td>0.003453</td>\n","      <td>6.087416e-07</td>\n","      <td>0.000029</td>\n","      <td>0.000021</td>\n","      <td>0.000249</td>\n","      <td>0.000003</td>\n","      <td>0.000282</td>\n","      <td>2.974230e-06</td>\n","      <td>0.000170</td>\n","      <td>...</td>\n","      <td>0.001544</td>\n","      <td>0.000009</td>\n","      <td>0.015737</td>\n","      <td>0.000514</td>\n","      <td>0.000018</td>\n","      <td>0.000035</td>\n","      <td>0.000084</td>\n","      <td>0.000458</td>\n","      <td>0.000499</td>\n","      <td>0.001351</td>\n","      <td>0.000124</td>\n","      <td>0.002685</td>\n","      <td>0.152167</td>\n","      <td>0.073116</td>\n","      <td>0.000115</td>\n","      <td>2.530185e-03</td>\n","      <td>2.382848e-03</td>\n","      <td>9.330537e-06</td>\n","      <td>0.001617</td>\n","      <td>6.710043e-06</td>\n","      <td>0.000066</td>\n","      <td>1.195099e-06</td>\n","      <td>0.000127</td>\n","      <td>0.000073</td>\n","      <td>4.096047e-01</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>76 rows × 62 columns</p>\n","</div>"],"text/plain":["    id         0         1         2             3         4         5  \\\n","0    0  0.000033  0.216780  0.000001  2.304875e-07  0.005119  0.000073   \n","1    1  0.000901  0.237869  0.000020  1.075717e-06  0.019341  0.000552   \n","2    2  0.000200  0.088423  0.000002  1.600786e-06  0.045726  0.000392   \n","3    3  0.004559  0.101666  0.000070  1.996433e-04  0.007421  0.006646   \n","4    4  0.095148  0.221749  0.000108  1.437838e-04  0.000188  0.000151   \n","..  ..       ...       ...       ...           ...       ...       ...   \n","71  71  0.128587  0.092072  0.000518  1.891940e-03  0.000127  0.000317   \n","72  72  0.170183  0.009281  0.001028  5.411435e-03  0.000059  0.001127   \n","73  73  0.009494  0.000596  0.000353  2.172152e-03  0.000032  0.000274   \n","74  74  0.024439  0.002724  0.000087  2.212586e-03  0.000026  0.000058   \n","75  75  0.000529  0.000112  0.000012  1.532508e-04  0.000013  0.000059   \n","\n","           6         7         8         9        10        11            12  \\\n","0   0.006855  0.000006  0.000002  0.000012  0.000634  0.000060  8.701382e-09   \n","1   0.034963  0.001162  0.000034  0.000289  0.002232  0.000707  3.663784e-07   \n","2   0.003061  0.000026  0.000013  0.000102  0.001133  0.000111  5.830112e-08   \n","3   0.019953  0.004027  0.000023  0.000145  0.000745  0.000010  4.907999e-07   \n","4   0.146353  0.002076  0.000176  0.004950  0.000205  0.000372  1.059002e-04   \n","..       ...       ...       ...       ...       ...       ...           ...   \n","71  0.000457  0.002397  0.001709  0.000016  0.000042  0.000096  2.521380e-06   \n","72  0.000123  0.002980  0.002122  0.000017  0.000028  0.000014  6.822567e-07   \n","73  0.002884  0.003924  0.011947  0.001534  0.002128  0.005968  1.357916e-03   \n","74  0.024150  0.001432  0.000706  0.008373  0.002559  0.009885  2.164593e-02   \n","75  0.000173  0.000291  0.004023  0.000121  0.000784  0.003439  1.641499e-03   \n","\n","          13        14        15            16        17        18        19  \\\n","0   0.000178  0.000034  0.000044  1.761358e-03  0.002399  0.000090  0.000105   \n","1   0.000967  0.000025  0.001595  6.480485e-03  0.011268  0.000736  0.000765   \n","2   0.000197  0.000079  0.000193  1.606288e-03  0.004511  0.000558  0.000292   \n","3   0.000035  0.000078  0.002877  1.314297e-03  0.010105  0.000484  0.001106   \n","4   0.000118  0.000408  0.003985  4.725608e-03  0.000518  0.000079  0.000480   \n","..       ...       ...       ...           ...       ...       ...       ...   \n","71  0.000054  0.000034  0.447334  5.323219e-03  0.052556  0.009053  0.008714   \n","72  0.000008  0.000024  0.080970  4.238651e-04  0.001889  0.000649  0.000524   \n","73  0.000083  0.001140  0.011879  2.010970e-05  0.000204  0.000076  0.000432   \n","74  0.000161  0.004116  0.007174  1.140379e-04  0.000245  0.000084  0.000360   \n","75  0.000009  0.002631  0.003453  6.087416e-07  0.000029  0.000021  0.000249   \n","\n","          20        21            22        23  ...        36        37  \\\n","0   0.000477  0.000158  8.523672e-09  0.000021  ...  0.000123  0.013445   \n","1   0.001681  0.001307  1.266715e-06  0.000187  ...  0.000627  0.082675   \n","2   0.000800  0.000172  2.283222e-07  0.000052  ...  0.000181  0.003308   \n","3   0.000210  0.002055  7.698406e-05  0.000167  ...  0.000800  0.003985   \n","4   0.000031  0.002248  2.609558e-05  0.000050  ...  0.001638  0.031113   \n","..       ...       ...           ...       ...  ...       ...       ...   \n","71  0.000208  0.002699  3.524705e-03  0.001047  ...  0.000396  0.002071   \n","72  0.000068  0.008454  1.440839e-02  0.000307  ...  0.001327  0.000360   \n","73  0.000016  0.002016  1.455235e-04  0.001247  ...  0.012574  0.000246   \n","74  0.000004  0.000326  4.859712e-05  0.000490  ...  0.002869  0.000575   \n","75  0.000003  0.000282  2.974230e-06  0.000170  ...  0.001544  0.000009   \n","\n","          38        39        40        41        42        43        44  \\\n","0   0.000123  0.000252  0.002353  0.000464  0.000046  0.000127  0.000562   \n","1   0.000925  0.001401  0.022617  0.000699  0.000217  0.000303  0.011345   \n","2   0.000478  0.000274  0.003770  0.001397  0.000110  0.000150  0.000533   \n","3   0.001817  0.000340  0.059454  0.021787  0.000191  0.000121  0.001126   \n","4   0.007858  0.004650  0.000627  0.000271  0.001782  0.001308  0.000464   \n","..       ...       ...       ...       ...       ...       ...       ...   \n","71  0.000097  0.000107  0.001583  0.000590  0.013130  0.004190  0.000077   \n","72  0.000016  0.000019  0.000968  0.000465  0.001771  0.001131  0.000021   \n","73  0.010567  0.000488  0.000234  0.000161  0.000270  0.000561  0.001352   \n","74  0.102243  0.004593  0.000044  0.000120  0.001695  0.001307  0.000533   \n","75  0.015737  0.000514  0.000018  0.000035  0.000084  0.000458  0.000499   \n","\n","          45        46        47        48        49        50            51  \\\n","0   0.001032  0.002458  0.000294  0.000169  0.000237  0.000126  2.731252e-07   \n","1   0.006809  0.007643  0.001229  0.003017  0.001176  0.003434  1.191768e-05   \n","2   0.000364  0.004485  0.000225  0.000776  0.001460  0.000953  1.114920e-06   \n","3   0.000269  0.002702  0.003927  0.015111  0.026035  0.135528  5.902705e-06   \n","4   0.003011  0.000253  0.235857  0.035470  0.009145  0.000198  1.566867e-04   \n","..       ...       ...       ...       ...       ...       ...           ...   \n","71  0.000302  0.003471  0.000252  0.003415  0.003843  0.001328  2.950664e-04   \n","72  0.000061  0.000340  0.000646  0.005047  0.015650  0.005307  1.445090e-04   \n","73  0.004634  0.000196  0.018899  0.131330  0.080850  0.000801  1.077751e-02   \n","74  0.008791  0.000128  0.015540  0.104182  0.020923  0.000287  5.935887e-03   \n","75  0.001351  0.000124  0.002685  0.152167  0.073116  0.000115  2.530185e-03   \n","\n","              52            53        54            55        56  \\\n","0   5.536824e-08  1.653439e-07  0.000048  1.862531e-07  0.000053   \n","1   1.618906e-06  6.451095e-06  0.000448  3.590443e-06  0.000599   \n","2   2.855551e-07  1.426241e-06  0.000157  1.206894e-06  0.000198   \n","3   1.354985e-06  4.168314e-05  0.008514  1.982548e-05  0.002052   \n","4   6.160420e-05  3.025192e-05  0.010477  9.790144e-06  0.001494   \n","..           ...           ...       ...           ...       ...   \n","71  6.880833e-07  2.672819e-04  0.000093  9.241825e-05  0.002421   \n","72  3.027336e-07  3.825970e-04  0.000572  5.153039e-04  0.000463   \n","73  1.597015e-03  1.533933e-04  0.009429  1.532576e-04  0.000313   \n","74  1.066511e-02  6.323193e-05  0.004966  9.100653e-06  0.000476   \n","75  2.382848e-03  9.330537e-06  0.001617  6.710043e-06  0.000066   \n","\n","              57        58        59            60  \n","0   3.193653e-10  0.000012  0.619531  1.867334e-07  \n","1   1.343261e-08  0.000032  0.155620  1.223821e-06  \n","2   1.130814e-08  0.000032  0.589477  2.012171e-06  \n","3   8.629020e-07  0.000129  0.148905  1.615707e-05  \n","4   4.047729e-06  0.000102  0.001427  3.471247e-03  \n","..           ...       ...       ...           ...  \n","71  9.153256e-03  0.000022  0.000798  2.973501e-03  \n","72  1.244407e-02  0.000011  0.000133  4.145924e-03  \n","73  2.831365e-05  0.000255  0.000105  1.077401e-01  \n","74  1.993952e-05  0.001368  0.000090  1.785195e-01  \n","75  1.195099e-06  0.000127  0.000073  4.096047e-01  \n","\n","[76 rows x 62 columns]"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"9TpxR538RIj_","executionInfo":{"status":"ok","timestamp":1623820466410,"user_tz":-540,"elapsed":289,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["submission.to_csv('./result/0616Leeresult.csv',index=False)"],"execution_count":45,"outputs":[]},{"cell_type":"code","metadata":{"id":"um5jH_DhRIkA","executionInfo":{"status":"ok","timestamp":1623820493433,"user_tz":-540,"elapsed":291,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["test = pd.read_csv('./result/0616Leeresult.csv')"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"O0akVGo9XCPj","executionInfo":{"status":"ok","timestamp":1623807240021,"user_tz":-540,"elapsed":288,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":[""],"execution_count":90,"outputs":[]},{"cell_type":"code","metadata":{"id":"EH0zU40wcLR_","executionInfo":{"status":"ok","timestamp":1623820507722,"user_tz":-540,"elapsed":290,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["df = pd.DataFrame(index=range(0,76), columns=['label', 'max'])"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Od7IimXXCpj","executionInfo":{"status":"ok","timestamp":1623820508900,"user_tz":-540,"elapsed":2,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["label_list = []\n","max_list = []\n","for index,row in test.iterrows(): \n","    label_list.append(list(row[1:]).index(max(row[1:])))\n","    max_list.append(max(row[1:]))\n","df['label']=label_list\n","df['max']=max_list"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"YwbkE3JrhOHr","executionInfo":{"status":"ok","timestamp":1623820531050,"user_tz":-540,"elapsed":291,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["exercise = pd.read_csv('./0616Lee.csv')\n","exercise=exercise.drop('Time',axis=1)\n","exercise=exercise.drop('acc_x',axis=1)\n","exercise=exercise.drop('acc_y',axis=1)\n","exercise=exercise.drop('acc_z',axis=1)\n","exercise=exercise.drop('gy_x',axis=1)\n","exercise=exercise.drop('gy_y',axis=1)\n","exercise=exercise.drop('gy_z',axis=1)\n","exercise = exercise.drop_duplicates()"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"X9bFPBI8XEwU","executionInfo":{"status":"ok","timestamp":1623820537336,"user_tz":-540,"elapsed":286,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["label = pd.read_csv('./train_labels.csv')\n","label = label.drop('id',axis=1)\n","label = label.drop_duplicates()\n","label = label.sort_values('label')\n","merge = pd.merge(df,label,on=['label'])\n","merge.insert( 0, 'id', None)\n","merge['id'] = list(range(76))"],"execution_count":53,"outputs":[]},{"cell_type":"code","metadata":{"id":"2yNuA3AAi8iB","executionInfo":{"status":"ok","timestamp":1623820539279,"user_tz":-540,"elapsed":290,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["merge = pd.merge(merge,exercise,on=['id'])\n","merge.rename(columns = {'label_desc' : '결과'}, inplace = True)\n","merge.rename(columns = {'exercise' : '실제 운동'}, inplace = True)"],"execution_count":54,"outputs":[]},{"cell_type":"code","metadata":{"id":"2i4HYpOMPySL","executionInfo":{"status":"ok","timestamp":1623820546795,"user_tz":-540,"elapsed":470,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}},"outputId":"9c8f5e49-5758-4f00-ac94-057c79d6dfd6","colab":{"base_uri":"https://localhost:8080/","height":417}},"source":["merge"],"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>max</th>\n","      <th>결과</th>\n","      <th>실제 운동</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>59</td>\n","      <td>0.619531</td>\n","      <td>Wall Ball</td>\n","      <td>Burpee</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>59</td>\n","      <td>0.589477</td>\n","      <td>Wall Ball</td>\n","      <td>Burpee</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>25</td>\n","      <td>0.260089</td>\n","      <td>Medicine Ball Slam</td>\n","      <td>Burpee</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>33</td>\n","      <td>0.332503</td>\n","      <td>Rowing machine</td>\n","      <td>Burpee</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>33</td>\n","      <td>0.276555</td>\n","      <td>Rowing machine</td>\n","      <td>Burpee</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>71</th>\n","      <td>71</td>\n","      <td>29</td>\n","      <td>0.222826</td>\n","      <td>Power Boat pose</td>\n","      <td>Walk</td>\n","    </tr>\n","    <tr>\n","      <th>72</th>\n","      <td>72</td>\n","      <td>29</td>\n","      <td>0.222101</td>\n","      <td>Power Boat pose</td>\n","      <td>Walk</td>\n","    </tr>\n","    <tr>\n","      <th>73</th>\n","      <td>73</td>\n","      <td>15</td>\n","      <td>0.387852</td>\n","      <td>Dynamic Stretch (at your own pace)</td>\n","      <td>Walk</td>\n","    </tr>\n","    <tr>\n","      <th>74</th>\n","      <td>74</td>\n","      <td>15</td>\n","      <td>0.391979</td>\n","      <td>Dynamic Stretch (at your own pace)</td>\n","      <td>Walk</td>\n","    </tr>\n","    <tr>\n","      <th>75</th>\n","      <td>75</td>\n","      <td>15</td>\n","      <td>0.447334</td>\n","      <td>Dynamic Stretch (at your own pace)</td>\n","      <td>Walk</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>76 rows × 5 columns</p>\n","</div>"],"text/plain":["    id  label       max                                  결과   실제 운동\n","0    0     59  0.619531                           Wall Ball  Burpee\n","1    1     59  0.589477                           Wall Ball  Burpee\n","2    2     25  0.260089                  Medicine Ball Slam  Burpee\n","3    3     33  0.332503                      Rowing machine  Burpee\n","4    4     33  0.276555                      Rowing machine  Burpee\n","..  ..    ...       ...                                 ...     ...\n","71  71     29  0.222826                     Power Boat pose    Walk\n","72  72     29  0.222101                     Power Boat pose    Walk\n","73  73     15  0.387852  Dynamic Stretch (at your own pace)    Walk\n","74  74     15  0.391979  Dynamic Stretch (at your own pace)    Walk\n","75  75     15  0.447334  Dynamic Stretch (at your own pace)    Walk\n","\n","[76 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"5WMIJc_-ck0z","executionInfo":{"status":"ok","timestamp":1623820559307,"user_tz":-540,"elapsed":292,"user":{"displayName":"정의석","photoUrl":"","userId":"17136621062166100362"}}},"source":["merge.to_csv('./result/0616Leeresult_mapping.csv',encoding='utf-8',index=False)"],"execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yb6uIk5UiNVO"},"source":[""],"execution_count":null,"outputs":[]}]}